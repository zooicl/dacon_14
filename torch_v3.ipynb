{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-02T09:31:40.243505Z",
     "start_time": "2020-01-02T09:31:38.965699Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1590,
     "status": "ok",
     "timestamp": 1576286906380,
     "user": {
      "displayName": "주이클",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCWVh4nvn9788qUddhoWTl5qQoYt0bzVMDlJWUumTg=s64",
      "userId": "02708070532256873610"
     },
     "user_tz": -540
    },
    "id": "U3xmRNtgpZwi",
    "outputId": "f6e95e03-5913-4a53-9ff3-1b6cde1b5a82"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiden/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.2\n",
      "1.3.1\n",
      "GeForce RTX 2070 SUPER\n",
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.externals import joblib \n",
    "import os\n",
    "from konlpy.tag import Mecab\n",
    "import lightgbm as lgb\n",
    "print(lgb.__version__)\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.externals import joblib \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import gc\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "from tools import EarlyStopping, eval_summary\n",
    "\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-02T09:31:40.251368Z",
     "start_time": "2020-01-02T09:31:40.244618Z"
    }
   },
   "outputs": [],
   "source": [
    "class RDModel(torch.nn.Module):\n",
    "    def __init__(self, input_size, vocab_size, embed_size, hidden_size, num_layers):\n",
    "        super(RDModel,self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.dropout = torch.nn.Dropout(p=0.25)\n",
    "        \n",
    "#         self.embed = torch.nn.Embedding(vocab_size, embed_size, sparse=True)\n",
    "        self.embed = torch.nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = torch.nn.LSTM(embed_size,\n",
    "                    hidden_size,\n",
    "                    num_layers,\n",
    "                    batch_first=True,\n",
    "                    bidirectional=False, \n",
    "                     dropout=0.3)\n",
    "        \n",
    "#         self.fc = torch.nn.Sequential(\n",
    "#             torch.nn.Linear(input_size, 2048), self.relu, torch.nn.BatchNorm1d(2048), self.dropout,\n",
    "#             torch.nn.Linear(2048, 1024), self.relu, torch.nn.BatchNorm1d(1024), self.dropout,\n",
    "# #             torch.nn.Linear(1024, 512), self.relu, torch.nn.BatchNorm1d(512), self.dropout,\n",
    "# #             torch.nn.Linear(512, 512), self.relu, torch.nn.BatchNorm1d(512), self.dropout,\n",
    "# #             torch.nn.Linear(512, 256), self.relu, torch.nn.BatchNorm1d(256), self.dropout,\n",
    "# #             torch.nn.Linear(256, 128), self.relu, torch.nn.BatchNorm1d(128), self.dropout,\n",
    "            \n",
    "#             torch.nn.Linear(1024, 128), self.relu, torch.nn.BatchNorm1d(128), self.dropout,\n",
    "# #             torch.nn.Linear(128, 2), \n",
    "#         )\n",
    "#         self.output = torch.nn.Linear(hidden_size + 128, 2)\n",
    "        \n",
    "        self.fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_size, 64), self.relu, torch.nn.BatchNorm1d(64), self.dropout,\n",
    "            torch.nn.Linear(64, 64), self.relu, torch.nn.BatchNorm1d(64), self.dropout,\n",
    "        )\n",
    "        \n",
    "        self.output = torch.nn.Linear(hidden_size + 64, 2)\n",
    "        \n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # (num_layers * num_directions, input_size, hidden_size)\n",
    "        hidden = Variable(torch.zeros(self.num_layers, batch_size, self.hidden_size)).to(device)\n",
    "        cell = Variable(torch.zeros(self.num_layers, batch_size, self.hidden_size)).to(device)\n",
    "        return hidden, cell\n",
    "\n",
    "        \n",
    "    def forward(self, x, seq):   \n",
    "        embed = self.embed(seq)\n",
    "        hidden, cell = self.init_hidden(embed.size(0)) # initial hidden,cell\n",
    "        output, (hidden, cell) = self.lstm(embed, (hidden, cell))\n",
    "        \n",
    "        hidden = hidden[-1:]\n",
    "        hidden = torch.cat([h for h in hidden] + [self.fc(x)], 1)\n",
    "        \n",
    "        return self.output(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-02T09:31:40.267465Z",
     "start_time": "2020-01-02T09:31:40.253271Z"
    }
   },
   "outputs": [],
   "source": [
    "class RDDataset(Dataset):\n",
    "    def __init__(self, df, y_col, seq_col='text_idx'):\n",
    "        self.seq_col = seq_col\n",
    "        self.cols = [c for c in df.columns if c not in [y_col, seq_col]]\n",
    "        \n",
    "        print(seq_col, len(self.cols), y_col)\n",
    "        \n",
    "        self.X = df[self.cols].values\n",
    "        self.y = pd.get_dummies(df[y_col].astype(int), prefix=y_col).values\n",
    "        \n",
    "        self.seq_X = np.stack(df[self.seq_col].values)        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        return self.cols + [self.seq_col]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = self.X[idx].astype(np.float32)\n",
    "        X_seq = self.seq_X[idx].astype(np.int64)\n",
    "        y = self.y[idx].astype(np.float32)\n",
    "        \n",
    "#         print(X.shape, X_seq.shape, y.shape)\n",
    "        return X, X_seq, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train/test_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-02T09:31:40.279882Z",
     "start_time": "2020-01-02T09:31:40.268624Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_torch(dataset, step=100, num_workers=3):\n",
    "    model.train()\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    data_loader = DataLoader(dataset=dataset,\n",
    "#                           batch_size=100000,\n",
    "#                         batch_size=int(train_size * 0.7),\n",
    "                          batch_size=len(dataset) // step,\n",
    "#                           batch_size=10000,\n",
    "                          shuffle=True,\n",
    "                          num_workers=num_workers,\n",
    "                         drop_last=True\n",
    "                         )\n",
    "    for i, data in enumerate(data_loader):\n",
    "#     for i, data in tqdm_notebook(enumerate(train_loader), total=len(train_loader), desc = 'epoch{}_batch'.format(e)):\n",
    "        X_batch, X_seq_batch, y_batch = data\n",
    "        \n",
    "        X_batch = X_batch.to(device)\n",
    "        X_seq_batch = X_seq_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        \n",
    "#         print(X_batch.size())\n",
    "        \n",
    "        y_pred = model(X_batch, X_seq_batch)\n",
    "        \n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        \n",
    "        loss += loss.item()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        acc += (y_pred.argmax(1) == y_batch.argmax(1)).sum().item()\n",
    "        \n",
    "        del X_batch, y_batch, y_pred\n",
    "        gc.collect()\n",
    "\n",
    "    return loss / len(dataset), acc / len(dataset)\n",
    "\n",
    "\n",
    "def test_torch(dataset, step=100, num_workers=3):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    \n",
    "    y_true_list = []\n",
    "    y_score_list = []\n",
    "    \n",
    "    data_loader = DataLoader(dataset=dataset,\n",
    "                          batch_size=len(dataset) // step,\n",
    "                          shuffle=False,\n",
    "                          num_workers=num_workers,\n",
    "                          drop_last=True\n",
    "                         ) \n",
    "    \n",
    "    for i, data in enumerate(data_loader):\n",
    "        X_batch, X_seq_batch, y_batch = data\n",
    "        y_true = y_batch\n",
    "        \n",
    "        X_batch = X_batch.to(device)\n",
    "        X_seq_batch = X_seq_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        y_true_list.append(y_true[:, 1].cpu().detach().numpy())\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            y_pred = model(X_batch, X_seq_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            loss += loss.item()\n",
    "            acc += (y_pred.argmax(1) == y_batch.argmax(1)).sum().item()\n",
    "            \n",
    "            y_pred = torch.sigmoid(y_pred)\n",
    "            y_score_list.append(y_pred[:, 1].cpu().detach().numpy())\n",
    "            \n",
    "#              del X_batch, y_batch, y_true, y_pred\n",
    "            \n",
    "    return loss / len(dataset), acc / len(dataset), np.concatenate(y_true_list, axis=0), np.concatenate(y_score_list, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-02T09:31:40.288188Z",
     "start_time": "2020-01-02T09:31:40.280617Z"
    }
   },
   "outputs": [],
   "source": [
    "fc_cols = ['tfidf_pos_word_22_0028',\n",
    " 'tfidf_pos_char_11_0000',\n",
    " 'tfidf_word_11_1263',\n",
    " 'tfidf_word_11_1516',\n",
    " 'tfidf_word_11_0552',\n",
    " 'cnt_0583',\n",
    " 'tfidf_word_22_0130',\n",
    " 'tfidf_word_11_0177',\n",
    " 'tfidf_word_11_0307',\n",
    " 'tfidf_word_22_0132',\n",
    " 'tfidf_word_11_0928',\n",
    " 'tfidf_word_11_0186',\n",
    " 'cnt_0492',\n",
    " 'tfidf_pos_word_11_0129',\n",
    " 'tfidf_char_11_0264',\n",
    " 'tfidf_pos_char_11_0650',\n",
    " 'tfidf_pos_char_11_0242',\n",
    " 'tfidf_char_11_0731',\n",
    " 'tfidf_word_11_0916',\n",
    " 'tfidf_pos_char_11_0213',\n",
    " 'tfidf_pos_word_22_0021',\n",
    " 'tfidf_char_11_0230',\n",
    " 'tfidf_pos_word_11_0391',\n",
    " 'cnt_0041',\n",
    " 'cnt_0042',\n",
    " 'tfidf_char_11_0796',\n",
    " 'tfidf_word_22_0095',\n",
    " 'tfidf_word_11_0011',\n",
    " 'tfidf_word_11_0736',\n",
    " 'tfidf_pos_char_11_0005',\n",
    " 'tfidf_pos_word_11_0077',\n",
    " 'fea__noun',\n",
    " 'cnt_0126',\n",
    " 'cnt_0223',\n",
    " 'tfidf_word_11_1439',\n",
    " 'tfidf_pos_char_11_0003',\n",
    " 'tfidf_word_11_0854',\n",
    " 'tfidf_word_11_1660',\n",
    " 'tfidf_char_11_0359',\n",
    " 'tfidf_pos_char_11_0589',\n",
    " 'cnt_0715',\n",
    " 'tfidf_pos_char_11_0415',\n",
    " 'tfidf_pos_word_11_0235',\n",
    " 'tfidf_char_11_0702',\n",
    " 'tfidf_char_11_0464',\n",
    " 'tfidf_pos_char_11_0017',\n",
    " 'tfidf_word_11_0319',\n",
    " 'tfidf_pos_char_11_0626',\n",
    " 'tfidf_pos_word_11_0420',\n",
    " 'tfidf_char_11_0657',\n",
    " 'tfidf_word_22_0091',\n",
    " 'cnt_0796',\n",
    " 'tfidf_char_11_0126',\n",
    " 'tfidf_word_11_0166',\n",
    " 'tfidf_word_33_0026',\n",
    " 'fea__text_len',\n",
    " 'tfidf_char_11_0130',\n",
    " 'tfidf_word_22_0134',\n",
    " 'cnt_0007',\n",
    " 'tfidf_pos_word_11_0044',\n",
    " 'tfidf_pos_char_11_0007',\n",
    " 'tfidf_pos_char_11_0324']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-02T09:31:49.931139Z",
     "start_time": "2020-01-02T09:31:40.289156Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_set\n",
      " 0    277242\n",
      "1     18703\n",
      "Name: smishing, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# merged_ts = '20191230T014439_8180'\n",
    "# merged_ts = '20191229T155539'\n",
    "# merged_ts = '20191231T113708_5499'\n",
    "# merged_ts = '20191231T165424_6099'\n",
    "# merged_ts = '20191231T162533_2022'\n",
    "# merged_ts = '20200101T212111_5854_100_24161'\n",
    "merged_ts = '20200102T015155_8438_128_49980'\n",
    "\n",
    "merged_ts = '20200102T113923_8438_744_49980'\n",
    "\n",
    "merged_ts = '20200102T160226_8438_275_997'\n",
    "\n",
    "\n",
    "\n",
    "train_path = 'data/df_merged_{}_train.pkl'.format(merged_ts)\n",
    "test_path = 'data/df_merged_{}_test.pkl'.format(merged_ts)\n",
    "\n",
    "df_model = joblib.load(train_path)  \n",
    "df_model = df_model.reset_index()\n",
    "print('model_set\\n', df_model['smishing'].value_counts())\n",
    "df_test = joblib.load(test_path) \n",
    "\n",
    "idx_cols = ['smishing', 'id', 'index']\n",
    "\n",
    "fea_cols = [c for c in df_model.columns if c not in idx_cols]#[:500]\n",
    "\n",
    "fea_cols = [c for c in fea_cols if c in fc_cols]\n",
    "fea_cols = fea_cols + [c for c in df_model.columns if 'fea__' in c]\n",
    "fea_cols = list(set(fea_cols))\n",
    "\n",
    "seq_col = [c for c in df_model.columns if '_idx' in c][0]\n",
    "# fea_cols.remove(seq_col)\n",
    "input_size = len(fea_cols)\n",
    "\n",
    "vocab_size = int(merged_ts.split('_')[-1])\n",
    "# vocab_size = 24161\n",
    "\n",
    "x_test = torch.Tensor(df_test[fea_cols].values).to(device)\n",
    "x_seq_test = torch.Tensor(np.stack(df_test[seq_col].values)).long().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-02T09:31:49.936954Z",
     "start_time": "2020-01-02T09:31:49.932624Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(107, 107, 'morphs_275_idx', 997)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size, len(fea_cols), seq_col, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-02T09:31:49.973774Z",
     "start_time": "2020-01-02T09:31:49.939675Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(295945, 107)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model[fea_cols].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-01-02T09:31:39.053Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 2\n",
      "morphs_275_idx 107 smishing\n",
      "morphs_275_idx 107 smishing\n",
      "236755 59190\n",
      "\n",
      "CV 0\n",
      "RDModel(\n",
      "  (relu): ReLU()\n",
      "  (dropout): Dropout(p=0.25, inplace=False)\n",
      "  (embed): Embedding(997, 128)\n",
      "  (lstm): LSTM(128, 64, num_layers=2, batch_first=True, dropout=0.3)\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=107, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Dropout(p=0.25, inplace=False)\n",
      "    (4): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): Dropout(p=0.25, inplace=False)\n",
      "  )\n",
      "  (output): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n",
      "20200102T183154\n",
      "Epoch: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13489147292546449e49a809737365f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='CV 0 Epoch', style=ProgressStyle(description_width='initial')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV 0 Epoch 1\n",
      "\tTrain loss: 0.0010678513208404183\tValid loss: 0.014871115796267986\t0.07180707156658173\n",
      "\t {'auc': 0.9991263715735177, 'confusion_matrix': array([[55335,   114],\n",
      "       [   62,  3625]]), 'precision': 0.9695105643220112, 'recall': 0.9831841605641443}\n",
      "Validation loss decreased (inf --> -0.999126).  Saving model ...\n",
      "CV 0 Epoch 2\n",
      "\tTrain loss: 0.0008797846967354417\tValid loss: 0.011380963027477264\t0.07730318605899811\n",
      "\t {'auc': 0.9996125742485723, 'confusion_matrix': array([[55362,    87],\n",
      "       [   39,  3648]]), 'precision': 0.976706827309237, 'recall': 0.9894222945484134}\n",
      "Validation loss decreased (-0.999126 --> -0.999613).  Saving model ...\n",
      "CV 0 Epoch 3\n",
      "\tTrain loss: 0.001104887342080474\tValid loss: 0.009846928529441357\t0.11220629513263702\n",
      "\t {'auc': 0.9993083536501285, 'confusion_matrix': array([[54828,   621],\n",
      "       [   20,  3667]]), 'precision': 0.8551772388059702, 'recall': 0.994575535665853}\n",
      "EarlyStopping counter: 1 out of 15\n",
      "CV 0 Epoch 4\n",
      "\tTrain loss: 0.001230676076374948\tValid loss: 0.010633460246026516\t0.11573617905378342\n",
      "\t {'auc': 0.9995887457953957, 'confusion_matrix': array([[55251,   198],\n",
      "       [   20,  3667]]), 'precision': 0.9487710219922381, 'recall': 0.994575535665853}\n",
      "EarlyStopping counter: 2 out of 15\n",
      "CV 0 Epoch 5\n",
      "\tTrain loss: 0.0003353688516654074\tValid loss: 0.014121605083346367\t0.02374863624572754\n",
      "\t {'auc': 0.9998823887421934, 'confusion_matrix': array([[55423,    26],\n",
      "       [   30,  3657]]), 'precision': 0.9929405376052132, 'recall': 0.9918633034987795}\n",
      "Validation loss decreased (-0.999613 --> -0.999882).  Saving model ...\n",
      "CV 0 Epoch 6\n",
      "\tTrain loss: 0.00153081095777452\tValid loss: 0.014418045058846474\t0.10617326945066452\n",
      "\t {'auc': 0.9999435728141548, 'confusion_matrix': array([[55429,    20],\n",
      "       [   33,  3654]]), 'precision': 0.994556341861731, 'recall': 0.9910496338486574}\n",
      "Validation loss decreased (-0.999882 --> -0.999944).  Saving model ...\n",
      "CV 0 Epoch 7\n",
      "\tTrain loss: 0.0006350571056827903\tValid loss: 0.005946883000433445\t0.10678823292255402\n",
      "\t {'auc': 0.9992038537889635, 'confusion_matrix': array([[51971,  3478],\n",
      "       [   12,  3675]]), 'precision': 0.5137704459667273, 'recall': 0.9967453213995118}\n",
      "EarlyStopping counter: 1 out of 15\n",
      "CV 0 Epoch 8\n",
      "\tTrain loss: 0.0005872674519196153\tValid loss: 0.014303777366876602\t0.041056808084249496\n",
      "\t {'auc': 0.9998886913105847, 'confusion_matrix': array([[55436,    13],\n",
      "       [   51,  3636]]), 'precision': 0.9964373801041381, 'recall': 0.9861676159479251}\n",
      "EarlyStopping counter: 2 out of 15\n",
      "CV 0 Epoch 9\n",
      "\tTrain loss: 0.000506300013512373\tValid loss: 0.006217636168003082\t0.08142966032028198\n",
      "\t {'auc': 0.999863314729433, 'confusion_matrix': array([[55177,   272],\n",
      "       [   17,  3670]]), 'precision': 0.9309994926433283, 'recall': 0.9953892053159751}\n",
      "EarlyStopping counter: 3 out of 15\n",
      "CV 0 Epoch 10\n",
      "\tTrain loss: 0.00031678713276050985\tValid loss: 0.009769411757588387\t0.03242642804980278\n",
      "\t {'auc': 0.9999332666351866, 'confusion_matrix': array([[55436,    13],\n",
      "       [   33,  3654]]), 'precision': 0.9964548677392965, 'recall': 0.9910496338486574}\n",
      "EarlyStopping counter: 4 out of 15\n",
      "CV 0 Epoch 11\n",
      "\tTrain loss: 0.0007837731973268092\tValid loss: 0.012788234278559685\t0.06128861755132675\n",
      "\t {'auc': 0.9999690985829943, 'confusion_matrix': array([[55442,     7],\n",
      "       [   38,  3649]]), 'precision': 0.9980853391684902, 'recall': 0.9896935177651207}\n",
      "Validation loss decreased (-0.999944 --> -0.999969).  Saving model ...\n",
      "CV 0 Epoch 12\n",
      "\tTrain loss: 6.441953883040696e-05\tValid loss: 0.01682272180914879\t0.0038293171674013138\n",
      "\t {'auc': 0.9999431276967906, 'confusion_matrix': array([[55445,     4],\n",
      "       [   60,  3627]]), 'precision': 0.9988983751032773, 'recall': 0.983726606997559}\n",
      "EarlyStopping counter: 1 out of 15\n",
      "CV 0 Epoch 13\n",
      "\tTrain loss: 0.0002778795314952731\tValid loss: 0.011744818650186062\t0.023659754544496536\n",
      "\t {'auc': 0.9999702456162017, 'confusion_matrix': array([[55419,    30],\n",
      "       [   28,  3659]]), 'precision': 0.9918677148278666, 'recall': 0.9924057499321942}\n",
      "Validation loss decreased (-0.999969 --> -0.999970).  Saving model ...\n",
      "CV 0 Epoch 14\n",
      "\tTrain loss: 0.00015294492186512798\tValid loss: 0.006670183502137661\t0.022929642349481583\n",
      "\t {'auc': 0.9999618715400777, 'confusion_matrix': array([[55430,    19],\n",
      "       [   18,  3669]]), 'precision': 0.9948481561822126, 'recall': 0.9951179820992677}\n",
      "EarlyStopping counter: 1 out of 15\n",
      "CV 0 Epoch 15\n",
      "\tTrain loss: 0.00017857573402579874\tValid loss: 0.006352434866130352\t0.028111383318901062\n",
      "\t {'auc': 0.9999736060077304, 'confusion_matrix': array([[55414,    35],\n",
      "       [   15,  3672]]), 'precision': 0.990558403021311, 'recall': 0.9959316517493898}\n",
      "Validation loss decreased (-0.999970 --> -0.999974).  Saving model ...\n",
      "CV 0 Epoch 16\n",
      "\tTrain loss: 0.00033528576022945344\tValid loss: 0.004618193954229355\t0.07260105758905411\n",
      "\t {'auc': 0.9999647085518486, 'confusion_matrix': array([[55408,    41],\n",
      "       [   12,  3675]]), 'precision': 0.9889666307857912, 'recall': 0.9967453213995118}\n",
      "EarlyStopping counter: 1 out of 15\n",
      "CV 0 Epoch 17\n",
      "\tTrain loss: 0.0004130169400013983\tValid loss: 0.00793839618563652\t0.05202775448560715\n",
      "\t {'auc': 0.9999788495881072, 'confusion_matrix': array([[55432,    17],\n",
      "       [   16,  3671]]), 'precision': 0.9953904555314533, 'recall': 0.9956604285326824}\n",
      "Validation loss decreased (-0.999974 --> -0.999979).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "# Model = RDModel\n",
    "# Dataset = RDDataset\n",
    "\n",
    "step = 64\n",
    "num_workers = 2\n",
    "\n",
    "print(step, num_workers)\n",
    "\n",
    "[df_test.drop(c, axis=1, inplace=True) for c in df_test.columns if 'smishing_' in c]\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, random_state=8405)\n",
    "\n",
    "for cv, index in enumerate(skf.split(df_model[fea_cols], df_model['smishing'])):\n",
    "    train_index, valid_index = index\n",
    "    train_set = RDDataset(df_model.loc[train_index, fea_cols + [seq_col, 'smishing']],\n",
    "                          'smishing', seq_col)\n",
    "    valid_set = RDDataset(df_model.loc[valid_index, fea_cols + [seq_col, 'smishing']],\n",
    "                          'smishing', seq_col)\n",
    "    \n",
    "    print(len(train_index), len(valid_index))\n",
    "    print('\\nCV', cv)\n",
    "    model = RDModel(input_size=input_size, \n",
    "                     vocab_size=vocab_size,\n",
    "                     embed_size=128, \n",
    "                     hidden_size=64, #hidden_size=128, \n",
    "                     num_layers=2).to(device)\n",
    "    # initialize the early_stopping object\n",
    "    early_stopping = EarlyStopping(patience=15, verbose=True)\n",
    "\n",
    "    epoch = 1\n",
    "    if cv == 0:\n",
    "#         print(summary(model, (input_size, )))\n",
    "        print(model.train())\n",
    "    \n",
    "    pos_weight = torch.Tensor([1., 10.,])\n",
    "#     pos_weight = torch.Tensor([1., 1.,])\n",
    "    criterion = torch.nn.BCEWithLogitsLoss(reduction='sum', pos_weight=pos_weight).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\n",
    "#     optimizer = torch.optim.SparseAdam(model.parameters(), lr = 0.0025)\n",
    "\n",
    "    model_ts = datetime.now().strftime('%Y%m%dT%H%M%S')\n",
    "    print(model_ts)\n",
    "    print('Epoch:', epoch)\n",
    "\n",
    "    N_EPOCHS = 100\n",
    "    for e in tqdm_notebook(range(epoch, epoch + N_EPOCHS), total=N_EPOCHS, desc = 'CV {} Epoch'.format(cv)):\n",
    "        train_loss, train_acc = train_torch(train_set, step, num_workers)\n",
    "        valid_loss, valid_acc, y_true, y_score = test_torch(valid_set, step, num_workers)\n",
    "        print('CV {} Epoch {}\\n\\tTrain loss: {}\\tValid loss: {}\\t{}'.format(cv, e, train_loss, valid_loss, train_loss / valid_loss))\n",
    "        \n",
    "        eval_dict = eval_summary(y_true, y_score, cut_off=0.5)\n",
    "        print('\\t', eval_dict)\n",
    "        \n",
    "        early_stopping(-eval_dict['auc'], model)\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"\\tEarly stopping epoch {}, valid loss {}\".format(e, valid_loss))\n",
    "            break\n",
    "    \n",
    "        epoch = e + 1\n",
    "    \n",
    "    # load the last checkpoint with the best model\n",
    "    model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "    torch.save(model.state_dict(), 'model/{}_{}_{}.model'.format(model_ts, cv, epoch-1))\n",
    "    \n",
    "    valid_loss, valid_acc, y_true, y_score = test_torch(valid_set)\n",
    "    print('END<valid> CV {} eval summary (train)\\n'.format(cv), eval_summary(y_true, y_score, cut_off=0.5))\n",
    "    train_loss, train_acc, y_true, y_score = test_torch(train_set)\n",
    "    print('END<train> CV {} eval summary (train)\\n'.format(cv), eval_summary(y_true, y_score, cut_off=0.5))\n",
    "\n",
    "    \n",
    "    model.eval()\n",
    "    pred_col = 'smishing_{}'.format(cv)\n",
    "    df_test[pred_col] = torch.sigmoid(model(x_test, x_seq_test))[:, 1].cpu().detach().numpy()\n",
    "    df_test[[pred_col]].to_csv('submit/{}_{}_nn.csv'.format(model_ts, pred_col), index=True)\n",
    "    \n",
    "    del train_set, valid_set\n",
    "    \n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-01-02T09:31:39.055Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.Series(y_score)\n",
    "df.hist(bins=100, figsize=(20, 5))\n",
    "(df * 10).astype(int).value_counts(sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-01-02T09:31:39.056Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_model[(y_score <= 0.5) & (y_true == 1)]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-01-02T09:31:39.058Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_model[(y_score > 0.5) & (y_true == 0)]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-01-02T09:31:39.059Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_cols = [c for c in df_test.columns if 'smishing_' in c]\n",
    "print(len(pred_cols))\n",
    "df_test['pred_max'] = df_test[pred_cols].max(axis=1)\n",
    "df_test['pred_min'] = df_test[pred_cols].min(axis=1)\n",
    "df_test['pred_mean'] = df_test[pred_cols].mean(axis=1)\n",
    "df_test['pred_std'] = df_test[pred_cols].std(axis=1)\n",
    "\n",
    "print(df_test['pred_std'].max(), df_test['pred_std'].min(), df_test['pred_std'].mean())\n",
    "\n",
    "df_test['smishing'] = df_test['pred_mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-01-02T09:31:39.060Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test['smishing'].hist(bins=100, figsize=(20, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-01-02T09:31:39.062Z"
    }
   },
   "outputs": [],
   "source": [
    "for c in pred_cols:\n",
    "    print(c)\n",
    "    display((df_test[c] * 10).astype(int).value_counts(sort=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-01-02T09:31:39.063Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 0     1504\n",
    "# 1       11\n",
    "# 2        6\n",
    "# 3        6\n",
    "# 4        2\n",
    "# 5        3\n",
    "# 6        2\n",
    "# 9       39\n",
    "# 10      53\n",
    "(df_test['smishing'] * 10).astype(int).value_counts(sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-01-02T09:31:39.064Z"
    }
   },
   "outputs": [],
   "source": [
    "model_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-01-02T09:31:39.065Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test[['smishing']].to_csv('submit/{}_nn.csv'.format(model_ts), index=True)\n",
    "# df_test[['id', 'smishing', 'text']].sort_values('smishing', ascending=False).to_csv('{}_text.csv'.format(model_ts), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dtoNNPnG75fT"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "mecab_tfidf.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
