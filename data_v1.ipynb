{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-02T06:40:02.789409Z",
     "start_time": "2020-01-02T06:40:01.830149Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1590,
     "status": "ok",
     "timestamp": 1576286906380,
     "user": {
      "displayName": "주이클",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCWVh4nvn9788qUddhoWTl5qQoYt0bzVMDlJWUumTg=s64",
      "userId": "02708070532256873610"
     },
     "user_tz": -540
    },
    "id": "U3xmRNtgpZwi",
    "outputId": "f6e95e03-5913-4a53-9ff3-1b6cde1b5a82"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiden/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.2\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.externals import joblib \n",
    "import os\n",
    "from konlpy.tag import Mecab\n",
    "import lightgbm as lgb\n",
    "print(lgb.__version__)\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.externals import joblib \n",
    "\n",
    "import gc\n",
    "from tqdm import tqdm_notebook\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-02T06:40:03.951722Z",
     "start_time": "2020-01-02T06:40:02.790730Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aiden/src/dacon_14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(297571, 3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "\n",
    "base_path = '.'\n",
    "\n",
    "df_train = pd.read_csv(os.path.join(base_path , 'input/train.csv'), index_col=0)\n",
    "df_test = pd.read_csv(os.path.join(base_path , 'input/public_test.csv'), index_col=0)\n",
    "df_test['smishing'] = -1\n",
    "\n",
    "df_fea = pd.concat([df_train, df_test])\n",
    "df_fea.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mecab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-02T06:44:30.926515Z",
     "start_time": "2020-01-02T06:40:03.952742Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "uquxguHUpZwt"
   },
   "outputs": [],
   "source": [
    "mecab = Mecab()\n",
    "df_fea['morphs'] = df_fea['text'].apply(lambda x: mecab.morphs(x))\n",
    "df_fea['morphs_str'] = df_fea['morphs'].apply(lambda x: ' '.join(x))\n",
    "df_fea['nouns'] = df_fea['text'].apply(lambda x: mecab.nouns(x))\n",
    "df_fea['nouns_str'] = df_fea['nouns'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "def pos(row):\n",
    "    x = row['text']\n",
    "    pos_dict = {c:0 for c in pos_cols}\n",
    "    \n",
    "    for _, p in mecab.pos(x):\n",
    "        for v in p.split('+'):\n",
    "            pos_dict[v] += 1\n",
    "        \n",
    "    return [pos_dict[k] for k in sorted(pos_dict.keys())]\n",
    "\n",
    "pos_cols = [\n",
    "    'EC',\n",
    "    'EF',\n",
    "    'EP',\n",
    "    'ETM',\n",
    "    'ETN',\n",
    "    'IC',\n",
    "    'JC',\n",
    "    'JKB',\n",
    "    'JKC',\n",
    "    'JKG',\n",
    "    'JKO',\n",
    "    'JKQ',\n",
    "    'JKS',\n",
    "    'JKV',\n",
    "    'JX',\n",
    "    'MAG',\n",
    "    'MAJ',\n",
    "    'MM',\n",
    "    'NA',\n",
    "    'NNB',\n",
    "    'NNBC',\n",
    "    'NNG',\n",
    "    'NNP',\n",
    "    'NP',\n",
    "    'NR',\n",
    "    'SC',\n",
    "    'SF',\n",
    "    'SL',\n",
    "    'SN',\n",
    "    'SSC',\n",
    "    'SSO',\n",
    "    'SY',\n",
    "    'UNA',\n",
    "    'UNKNOWN',\n",
    "    'VA',\n",
    "    'VCN',\n",
    "    'VCP',\n",
    "    'VV',\n",
    "    'VX',\n",
    "    'XPN',\n",
    "    'XR',\n",
    "    'XSA',\n",
    "    'XSN',\n",
    "    'XSV',\n",
    "]\n",
    "df_fea[pos_cols] = df_fea.apply(pos, axis=1, result_type='expand')\n",
    "df_fea[pos_cols] = df_fea[pos_cols].astype(np.int16)\n",
    "df_fea.rename(columns={c:'fea__'+c for c in pos_cols}, inplace=True)\n",
    "df_fea.to_pickle('data/df_fea_1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-02T06:44:40.128835Z",
     "start_time": "2020-01-02T06:44:30.927828Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year_month</th>\n",
       "      <th>text</th>\n",
       "      <th>smishing</th>\n",
       "      <th>morphs</th>\n",
       "      <th>morphs_str</th>\n",
       "      <th>nouns</th>\n",
       "      <th>nouns_str</th>\n",
       "      <th>fea__EC</th>\n",
       "      <th>fea__EF</th>\n",
       "      <th>fea__EP</th>\n",
       "      <th>...</th>\n",
       "      <th>fea__VA</th>\n",
       "      <th>fea__VCN</th>\n",
       "      <th>fea__VCP</th>\n",
       "      <th>fea__VV</th>\n",
       "      <th>fea__VX</th>\n",
       "      <th>fea__XPN</th>\n",
       "      <th>fea__XR</th>\n",
       "      <th>fea__XSA</th>\n",
       "      <th>fea__XSN</th>\n",
       "      <th>fea__XSV</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2017-01</td>\n",
       "      <td>XXX은행성산XXX팀장입니다.행복한주말되세요</td>\n",
       "      <td>0</td>\n",
       "      <td>[XXX, 은, 행성, 산, XXX, 팀장, 입니다, ., 행복, 한, 주말, 되,...</td>\n",
       "      <td>XXX 은 행성 산 XXX 팀장 입니다 . 행복 한 주말 되 세요</td>\n",
       "      <td>[행성, 산, 팀장, 행복, 주말]</td>\n",
       "      <td>행성 산 팀장 행복 주말</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2017-01</td>\n",
       "      <td>오늘도많이웃으시는하루시작하세요XXX은행 진월동VIP라운지 XXX올림</td>\n",
       "      <td>0</td>\n",
       "      <td>[오늘, 도, 많이, 웃, 으시, 는, 하루, 시작, 하, 세요, XXX, 은행, ...</td>\n",
       "      <td>오늘 도 많이 웃 으시 는 하루 시작 하 세요 XXX 은행 진월동 VIP 라운지 X...</td>\n",
       "      <td>[오늘, 하루, 시작, 은행, 진월동, 라운지]</td>\n",
       "      <td>오늘 하루 시작 은행 진월동 라운지</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2017-01</td>\n",
       "      <td>안녕하십니까 고객님. XXX은행입니다.금일 납부하셔야 할 금액은 153600원 입니...</td>\n",
       "      <td>0</td>\n",
       "      <td>[안녕, 하, 십니까, 고객, 님, ., XXX, 은행, 입니다, ., 금일, 납부...</td>\n",
       "      <td>안녕 하 십니까 고객 님 . XXX 은행 입니다 . 금일 납부 하 셔야 할 금액 은...</td>\n",
       "      <td>[안녕, 고객, 은행, 금일, 납부, 금액, 원, 감사, 새해, 복, 은행, 옥포]</td>\n",
       "      <td>안녕 고객 은행 금일 납부 금액 원 감사 새해 복 은행 옥포</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2017-01</td>\n",
       "      <td>XXX 고객님안녕하세요XXX은행 XXX지점입니다지난 한 해 동안 저희 XXX지점에 ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[XXX, 고객, 님, 안녕, 하, 세요, XXX, 은행, XXX, 지점, 입니다,...</td>\n",
       "      <td>XXX 고객 님 안녕 하 세요 XXX 은행 XXX 지점 입니다 지난 한 해 동안 저...</td>\n",
       "      <td>[고객, 안녕, 은행, 지점, 해, 동안, 저희, 지점, 성원, 감사, 시작, 년,...</td>\n",
       "      <td>고객 안녕 은행 지점 해 동안 저희 지점 성원 감사 시작 년 소망 일 고객 가정 건...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2017-01</td>\n",
       "      <td>1월은 새로움이 가득XXX입니다.올 한해 더 많이행복한 한해되시길바랍니다</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 월, 은, 새로움, 이, 가득, XXX, 입니다, ., 올, 한, 해, 더,...</td>\n",
       "      <td>1 월 은 새로움 이 가득 XXX 입니다 . 올 한 해 더 많이 행복 한 한 해 되...</td>\n",
       "      <td>[월, 한, 행복, 해]</td>\n",
       "      <td>월 한 행복 해</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2017-01</td>\n",
       "      <td>행복한주말보내세요XXX용현남전담직원대리 XXX올림</td>\n",
       "      <td>0</td>\n",
       "      <td>[행복, 한, 주말, 보내, 세요, XXX, 용, 현남, 전담, 직원, 대리, XX...</td>\n",
       "      <td>행복 한 주말 보내 세요 XXX 용 현남 전담 직원 대리 XXX 올림</td>\n",
       "      <td>[행복, 주말, 현남, 전담, 직원, 대리]</td>\n",
       "      <td>행복 주말 현남 전담 직원 대리</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2017-01</td>\n",
       "      <td>XXX 고객님 안녕하세요XXX은행 무교지점 XXX과장입니다 오늘 아침에 눈을 뜨니 ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[XXX, 고객, 님, 안녕, 하, 세요, XXX, 은행, 무교, 지점, XXX, ...</td>\n",
       "      <td>XXX 고객 님 안녕 하 세요 XXX 은행 무교 지점 XXX 과장 입니다 오늘 아침...</td>\n",
       "      <td>[고객, 안녕, 은행, 무교, 지점, 과장, 아침, 눈, 눈, 세상, 적, 눈, 눈...</td>\n",
       "      <td>고객 안녕 은행 무교 지점 과장 아침 눈 눈 세상 적 눈 눈 순간 출근 걱정 어른 ...</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2017-01</td>\n",
       "      <td>XXX 고객님지난 한해 베풀어 주신 은혜 진심으로 감사 드립니다.가슴 깊이 간직 하...</td>\n",
       "      <td>0</td>\n",
       "      <td>[XXX, 고객, 님, 지난, 한, 해, 베풀, 어, 주, 신, 은혜, 진심, 으로...</td>\n",
       "      <td>XXX 고객 님 지난 한 해 베풀 어 주 신 은혜 진심 으로 감사 드립니다 . 가슴...</td>\n",
       "      <td>[고객, 한, 은혜, 진심, 감사, 가슴, 간직, 정유, 년, 새해, 가족, 행복,...</td>\n",
       "      <td>고객 한 은혜 진심 감사 가슴 간직 정유 년 새해 가족 행복 뜻 바 진심 소망 은행...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2017-01</td>\n",
       "      <td>설연휴 가족들과 훈훈한 정 나누시고 정겨운추억 많이 만드세요XXX오XXX올림</td>\n",
       "      <td>0</td>\n",
       "      <td>[설, 연휴, 가족, 들, 과, 훈훈, 한, 정, 나누, 시, 고, 정겨운, 추억,...</td>\n",
       "      <td>설 연휴 가족 들 과 훈훈 한 정 나누 시 고 정겨운 추억 많이 만드세요 XXX 오...</td>\n",
       "      <td>[설, 연휴, 가족, 정, 추억, 오]</td>\n",
       "      <td>설 연휴 가족 정 추억 오</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2017-01</td>\n",
       "      <td>(광고)XXXBaXXX고객님들 뒤엔XXX 언제나 XXX새로운 마음가짐으로 새롭게 준...</td>\n",
       "      <td>1</td>\n",
       "      <td>[(, 광고, ), XXXBaXXX, 고객, 님, 들, 뒤, 엔, XXX, 언제나,...</td>\n",
       "      <td>( 광고 ) XXXBaXXX 고객 님 들 뒤 엔 XXX 언제나 XXX 새로운 마음가...</td>\n",
       "      <td>[광고, 고객, 뒤, 마음가짐, 준비, 당, 행상, 품, 자격, 기준, 심사, 기준...</td>\n",
       "      <td>광고 고객 뒤 마음가짐 준비 당 행상 품 자격 기준 심사 기준 완화 상품 상품 정보...</td>\n",
       "      <td>29</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   year_month                                               text  smishing  \\\n",
       "id                                                                           \n",
       "0     2017-01                           XXX은행성산XXX팀장입니다.행복한주말되세요         0   \n",
       "1     2017-01              오늘도많이웃으시는하루시작하세요XXX은행 진월동VIP라운지 XXX올림         0   \n",
       "2     2017-01  안녕하십니까 고객님. XXX은행입니다.금일 납부하셔야 할 금액은 153600원 입니...         0   \n",
       "4     2017-01  XXX 고객님안녕하세요XXX은행 XXX지점입니다지난 한 해 동안 저희 XXX지점에 ...         0   \n",
       "5     2017-01           1월은 새로움이 가득XXX입니다.올 한해 더 많이행복한 한해되시길바랍니다         0   \n",
       "6     2017-01                        행복한주말보내세요XXX용현남전담직원대리 XXX올림         0   \n",
       "7     2017-01  XXX 고객님 안녕하세요XXX은행 무교지점 XXX과장입니다 오늘 아침에 눈을 뜨니 ...         0   \n",
       "8     2017-01  XXX 고객님지난 한해 베풀어 주신 은혜 진심으로 감사 드립니다.가슴 깊이 간직 하...         0   \n",
       "9     2017-01         설연휴 가족들과 훈훈한 정 나누시고 정겨운추억 많이 만드세요XXX오XXX올림         0   \n",
       "10    2017-01  (광고)XXXBaXXX고객님들 뒤엔XXX 언제나 XXX새로운 마음가짐으로 새롭게 준...         1   \n",
       "\n",
       "                                               morphs  \\\n",
       "id                                                      \n",
       "0   [XXX, 은, 행성, 산, XXX, 팀장, 입니다, ., 행복, 한, 주말, 되,...   \n",
       "1   [오늘, 도, 많이, 웃, 으시, 는, 하루, 시작, 하, 세요, XXX, 은행, ...   \n",
       "2   [안녕, 하, 십니까, 고객, 님, ., XXX, 은행, 입니다, ., 금일, 납부...   \n",
       "4   [XXX, 고객, 님, 안녕, 하, 세요, XXX, 은행, XXX, 지점, 입니다,...   \n",
       "5   [1, 월, 은, 새로움, 이, 가득, XXX, 입니다, ., 올, 한, 해, 더,...   \n",
       "6   [행복, 한, 주말, 보내, 세요, XXX, 용, 현남, 전담, 직원, 대리, XX...   \n",
       "7   [XXX, 고객, 님, 안녕, 하, 세요, XXX, 은행, 무교, 지점, XXX, ...   \n",
       "8   [XXX, 고객, 님, 지난, 한, 해, 베풀, 어, 주, 신, 은혜, 진심, 으로...   \n",
       "9   [설, 연휴, 가족, 들, 과, 훈훈, 한, 정, 나누, 시, 고, 정겨운, 추억,...   \n",
       "10  [(, 광고, ), XXXBaXXX, 고객, 님, 들, 뒤, 엔, XXX, 언제나,...   \n",
       "\n",
       "                                           morphs_str  \\\n",
       "id                                                      \n",
       "0                XXX 은 행성 산 XXX 팀장 입니다 . 행복 한 주말 되 세요   \n",
       "1   오늘 도 많이 웃 으시 는 하루 시작 하 세요 XXX 은행 진월동 VIP 라운지 X...   \n",
       "2   안녕 하 십니까 고객 님 . XXX 은행 입니다 . 금일 납부 하 셔야 할 금액 은...   \n",
       "4   XXX 고객 님 안녕 하 세요 XXX 은행 XXX 지점 입니다 지난 한 해 동안 저...   \n",
       "5   1 월 은 새로움 이 가득 XXX 입니다 . 올 한 해 더 많이 행복 한 한 해 되...   \n",
       "6              행복 한 주말 보내 세요 XXX 용 현남 전담 직원 대리 XXX 올림   \n",
       "7   XXX 고객 님 안녕 하 세요 XXX 은행 무교 지점 XXX 과장 입니다 오늘 아침...   \n",
       "8   XXX 고객 님 지난 한 해 베풀 어 주 신 은혜 진심 으로 감사 드립니다 . 가슴...   \n",
       "9   설 연휴 가족 들 과 훈훈 한 정 나누 시 고 정겨운 추억 많이 만드세요 XXX 오...   \n",
       "10  ( 광고 ) XXXBaXXX 고객 님 들 뒤 엔 XXX 언제나 XXX 새로운 마음가...   \n",
       "\n",
       "                                                nouns  \\\n",
       "id                                                      \n",
       "0                                 [행성, 산, 팀장, 행복, 주말]   \n",
       "1                          [오늘, 하루, 시작, 은행, 진월동, 라운지]   \n",
       "2      [안녕, 고객, 은행, 금일, 납부, 금액, 원, 감사, 새해, 복, 은행, 옥포]   \n",
       "4   [고객, 안녕, 은행, 지점, 해, 동안, 저희, 지점, 성원, 감사, 시작, 년,...   \n",
       "5                                       [월, 한, 행복, 해]   \n",
       "6                            [행복, 주말, 현남, 전담, 직원, 대리]   \n",
       "7   [고객, 안녕, 은행, 무교, 지점, 과장, 아침, 눈, 눈, 세상, 적, 눈, 눈...   \n",
       "8   [고객, 한, 은혜, 진심, 감사, 가슴, 간직, 정유, 년, 새해, 가족, 행복,...   \n",
       "9                               [설, 연휴, 가족, 정, 추억, 오]   \n",
       "10  [광고, 고객, 뒤, 마음가짐, 준비, 당, 행상, 품, 자격, 기준, 심사, 기준...   \n",
       "\n",
       "                                            nouns_str  fea__EC  fea__EF  \\\n",
       "id                                                                        \n",
       "0                                       행성 산 팀장 행복 주말        0        2   \n",
       "1                                 오늘 하루 시작 은행 진월동 라운지        1        1   \n",
       "2                   안녕 고객 은행 금일 납부 금액 원 감사 새해 복 은행 옥포        2        5   \n",
       "4   고객 안녕 은행 지점 해 동안 저희 지점 성원 감사 시작 년 소망 일 고객 가정 건...        1        5   \n",
       "5                                            월 한 행복 해        2        1   \n",
       "6                                   행복 주말 현남 전담 직원 대리        1        1   \n",
       "7   고객 안녕 은행 무교 지점 과장 아침 눈 눈 세상 적 눈 눈 순간 출근 걱정 어른 ...       15        8   \n",
       "8   고객 한 은혜 진심 감사 가슴 간직 정유 년 새해 가족 행복 뜻 바 진심 소망 은행...        3        3   \n",
       "9                                      설 연휴 가족 정 추억 오        3        0   \n",
       "10  광고 고객 뒤 마음가짐 준비 당 행상 품 자격 기준 심사 기준 완화 상품 상품 정보...       29        7   \n",
       "\n",
       "    fea__EP  ...  fea__VA  fea__VCN  fea__VCP  fea__VV  fea__VX  fea__XPN  \\\n",
       "id           ...                                                            \n",
       "0         1  ...        0         0         1        0        0         0   \n",
       "1         2  ...        0         0         0        2        0         0   \n",
       "2         4  ...        0         0         2        2        1         0   \n",
       "4         5  ...        0         0         1        7        1         0   \n",
       "5         1  ...        1         0         1        1        0         0   \n",
       "6         1  ...        0         0         0        2        0         0   \n",
       "7        10  ...        5         0         2       22        2         0   \n",
       "8         4  ...        0         0         0        5        1         0   \n",
       "9         2  ...        1         0         0        3        0         0   \n",
       "10       15  ...       10         0         4       18        6         1   \n",
       "\n",
       "    fea__XR  fea__XSA  fea__XSN  fea__XSV  \n",
       "id                                         \n",
       "0         0         1         0         1  \n",
       "1         0         0         0         1  \n",
       "2         0         0         1         3  \n",
       "4         0         2         2         4  \n",
       "5         0         1         0         2  \n",
       "6         0         1         1         0  \n",
       "7         2         2         2         4  \n",
       "8         0         2         1         2  \n",
       "9         1         1         1         0  \n",
       "10        2         8        12         5  \n",
       "\n",
       "[10 rows x 51 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fea = pd.read_pickle('data/df_fea_1.pkl')\n",
    "df_fea.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-02T06:44:40.131779Z",
     "start_time": "2020-01-02T06:44:40.129576Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['year_month', 'text', 'smishing', 'morphs', 'morphs_str', 'nouns',\n",
       "       'nouns_str', 'fea__EC', 'fea__EF', 'fea__EP', 'fea__ETM', 'fea__ETN',\n",
       "       'fea__IC', 'fea__JC', 'fea__JKB', 'fea__JKC', 'fea__JKG', 'fea__JKO',\n",
       "       'fea__JKQ', 'fea__JKS', 'fea__JKV', 'fea__JX', 'fea__MAG', 'fea__MAJ',\n",
       "       'fea__MM', 'fea__NA', 'fea__NNB', 'fea__NNBC', 'fea__NNG', 'fea__NNP',\n",
       "       'fea__NP', 'fea__NR', 'fea__SC', 'fea__SF', 'fea__SL', 'fea__SN',\n",
       "       'fea__SSC', 'fea__SSO', 'fea__SY', 'fea__UNA', 'fea__UNKNOWN',\n",
       "       'fea__VA', 'fea__VCN', 'fea__VCP', 'fea__VV', 'fea__VX', 'fea__XPN',\n",
       "       'fea__XR', 'fea__XSA', 'fea__XSN', 'fea__XSV'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fea.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-02T06:58:09.998161Z",
     "start_time": "2020-01-02T06:58:09.988836Z"
    }
   },
   "outputs": [],
   "source": [
    "dfs = []\n",
    "fea_dict = {}\n",
    "\n",
    "# vocab = None\n",
    "# stop_words = joblib.load('model/vocab_20191231T111937.pkl')\n",
    "\n",
    "vocab = joblib.load('model/vocab_20191231T111937.pkl')\n",
    "stop_words = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq Fea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAX_COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-02T06:58:13.264896Z",
     "start_time": "2020-01-02T06:58:13.257578Z"
    }
   },
   "outputs": [],
   "source": [
    "def _toidx(x):\n",
    "    return [int(vocab_dict[x[i]]) if i < len(x) else 0 for i in range(max_len)]\n",
    "\n",
    "def toidx(src_col, max_len):\n",
    "    idx_col_nm = '{}_{}_idx'.format(src_col, max_len)\n",
    "\n",
    "    vocab_set = set()\n",
    "    _ = df_fea[src_col].apply(lambda x: [vocab_set.add(c) for c in x])\n",
    "    \n",
    "    vocab_dict = {v: i+1 for i, v in enumerate(vocab_set)}\n",
    "    vocab_dim = len(vocab_dict.keys()) + 1\n",
    "\n",
    "    def toidx(x):\n",
    "        return [int(vocab_dict[x[i]]) if i < len(x) else 0 for i in range(max_len)]\n",
    "\n",
    "#     print(vocab_dim, max_len)\n",
    "    \n",
    "    return idx_col_nm, df_fea[src_col].apply(toidx), vocab_dim, max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-02T06:58:21.606757Z",
     "start_time": "2020-01-02T06:58:13.434058Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=1000)\n",
    "vectorizer = vectorizer.fit(df_fea['morphs_str'].values)\n",
    "\n",
    "seq_vocab = set(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-02T06:58:33.641994Z",
     "start_time": "2020-01-02T06:58:21.607816Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "744 to 275\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('morphs_275_idx', 997, 275)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_col = 'morphs'\n",
    "# max_len = 128\n",
    "# max_len = org_max_len\n",
    "\n",
    "org_max_len = df_fea[src_col].str.len().max()\n",
    "df_fea[src_col] = df_fea[src_col].apply(lambda x: [m for m in x if m in seq_vocab])\n",
    "max_len = df_fea[src_col].str.len().max()\n",
    "print(org_max_len, 'to', max_len)\n",
    "\n",
    "c, d, vocab_dim, max_len = toidx(src_col, max_len)\n",
    "\n",
    "\n",
    "df_fea[c] = d\n",
    "c, vocab_dim, max_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Fea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-02T06:58:33.956321Z",
     "start_time": "2020-01-02T06:58:33.643541Z"
    }
   },
   "outputs": [],
   "source": [
    "df_fea['fea__text_len'] = df_fea['text'].str.len().fillna(0).astype(np.float16)\n",
    "df_fea['fea__morphs_cnt'] = df_fea['morphs'].apply(lambda x: len(x)).fillna(0).astype(np.float16)\n",
    "df_fea['fea__noun_cnt'] = df_fea['nouns'].apply(lambda x: len(x)).fillna(0).astype(np.float16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-02T06:59:09.027110Z",
     "start_time": "2020-01-02T06:58:33.957584Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(297571, 999)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer='char',\n",
    "#                              vocabulary=vocab,\n",
    "                             stop_words=stop_words, \n",
    "                             max_df=1.0,\n",
    "                             min_df=100)\n",
    "\n",
    "# vectorizer = vectorizer.fit(df_fea[df_fea['smishing']==1]['nouns_str'].values)\n",
    "vectorizer = vectorizer.fit(df_fea['text'].values)\n",
    "cnt_vec = vectorizer.transform(df_fea['text'].values).toarray()\n",
    "\n",
    "cnt_dict = {'cnt_{0:04d}'.format(i):'cnt_{0:04}_{1}'.format(i, c) for i, c in enumerate(vectorizer.get_feature_names())}\n",
    "fea_dict.update(cnt_dict)\n",
    "cnt_cols = sorted(cnt_dict.keys())\n",
    "\n",
    "df_cnt_vec = pd.DataFrame(data=cnt_vec, index=df_fea.index, columns=cnt_cols, dtype=np.float16)\n",
    "df_cnt_vec = df_cnt_vec.loc[:, (df_cnt_vec != 0).any(axis=0)]\n",
    "dfs.append(df_cnt_vec)\n",
    "df_cnt_vec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-02T06:59:09.032751Z",
     "start_time": "2020-01-02T06:59:09.027871Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "l2Dgkdt6pZw_"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def tfidf(data, params, tfidf_tag):\n",
    "    vectorizer = TfidfVectorizer(**params)\n",
    "    vectorizer = vectorizer.fit(data)\n",
    "\n",
    "    d = {'{0}_{1:04d}'.format(tfidf_tag, v):'{0}_{1:04d}_{2}'.format(tfidf_tag, v, k) for k, v in sorted(vectorizer.vocabulary_.items(), key=lambda item: item[1])}\n",
    "    c = sorted(d.keys())\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "#         data=vectorizer.transform(df_fea['nouns_str'].values).toarray(),\n",
    "        data=vectorizer.transform(df_fea['morphs_str'].values).toarray(),\n",
    "        columns=c, \n",
    "        index=df_fea.index,\n",
    "        dtype=np.float16)\n",
    "    \n",
    "    # Remove all zeros column\n",
    "    df = df.loc[:, (df != 0).any(axis=0)]\n",
    "    d = {k:v for k, v in d.items() if k in df.columns}\n",
    "    \n",
    "    print(tfidf_tag, df.shape)\n",
    "    \n",
    "    return df, d, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-02T06:59:09.041560Z",
     "start_time": "2020-01-02T06:59:09.033505Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_src_col = 'morphs_str'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1, 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-02T06:59:49.271778Z",
     "start_time": "2020-01-02T06:59:09.042205Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf_char_11 (297571, 999)\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'analyzer' : 'char',\n",
    "    'max_features':None, \n",
    "#     'vocabulary': vocab,\n",
    "    'stop_words':stop_words, \n",
    "#     'max_df':0.1, \n",
    "    'min_df':100, \n",
    "    'ngram_range':(1, 1), \n",
    "}\n",
    "df, d, v = tfidf(df_fea[tfidf_src_col].values, params, 'tfidf_char_11')\n",
    "dfs.append(df)\n",
    "fea_dict.update(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-02T07:00:15.248706Z",
     "start_time": "2020-01-02T06:59:49.273044Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf_word_11 (297571, 2000)\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'max_features':2000, \n",
    "#     'vocabulary': vocab,\n",
    "    'stop_words':stop_words, \n",
    "    'max_df':1.0, \n",
    "    'min_df':200, \n",
    "    'ngram_range':(1, 1), \n",
    "}\n",
    "df, d, v = tfidf(df_fea[tfidf_src_col].values, params, 'tfidf_word_11')\n",
    "dfs.append(df)\n",
    "fea_dict.update(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-02T07:00:49.237644Z",
     "start_time": "2020-01-02T07:00:15.249607Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "l2Dgkdt6pZw_",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf_word_22 (297571, 2000)\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'max_features':2000, \n",
    "#     'vocabulary': vocab,\n",
    "    'stop_words':stop_words, \n",
    "    'max_df':1.0, \n",
    "    'min_df':500, \n",
    "    'ngram_range':(2, 2), \n",
    "}\n",
    "df, d, v = tfidf(df_fea[tfidf_src_col].values, params, 'tfidf_word_22')\n",
    "dfs.append(df)\n",
    "fea_dict.update(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-02T07:01:20.434912Z",
     "start_time": "2020-01-02T07:00:49.238452Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf_word_33 (297571, 500)\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'max_features':500, \n",
    "#     'vocabulary': vocab,\n",
    "    'stop_words':stop_words, \n",
    "    'max_df':1.0, \n",
    "    'min_df':500, \n",
    "    'ngram_range':(3, 3), \n",
    "}\n",
    "df, d, v = tfidf(df_fea[tfidf_src_col].values, params, 'tfidf_word_33')\n",
    "dfs.append(df)\n",
    "fea_dict.update(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### smishing 1, 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-01T09:36:52.987190Z",
     "start_time": "2020-01-01T09:36:52.984965Z"
    }
   },
   "source": [
    "##### char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-02T07:01:42.842356Z",
     "start_time": "2020-01-02T07:01:20.435707Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf_pos_char_11 (297571, 892)\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'analyzer' : 'char',\n",
    "#     'max_features':500, \n",
    "#     'vocabulary': vocab,\n",
    "    'stop_words':stop_words, \n",
    "#     'max_df':1.0, \n",
    "#     'min_df':100, \n",
    "    'ngram_range':(1, 1), \n",
    "}\n",
    "df, d, v = tfidf(df_fea[df_fea['smishing']==1][tfidf_src_col].values, params, 'tfidf_pos_char_11')\n",
    "dfs.append(df)\n",
    "fea_dict.update(d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-02T07:01:55.749821Z",
     "start_time": "2020-01-02T07:01:42.843252Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf_pos_word_11 (297571, 500)\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'max_features':500, \n",
    "#     'vocabulary': vocab,\n",
    "    'stop_words':stop_words, \n",
    "    'max_df':1.0, \n",
    "    'min_df':100, \n",
    "    'ngram_range':(1, 1), \n",
    "}\n",
    "df, d, v = tfidf(df_fea[df_fea['smishing']==1][tfidf_src_col].values, params, 'tfidf_pos_word_11')\n",
    "dfs.append(df)\n",
    "fea_dict.update(d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### smishing 2, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-02T07:02:12.494022Z",
     "start_time": "2020-01-02T07:01:55.750607Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf_pos_word_22 (297571, 500)\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'max_features':500, \n",
    "#     'vocabulary': vocab,\n",
    "    'stop_words':stop_words, \n",
    "    'max_df':1.0, \n",
    "    'min_df':100, \n",
    "    'ngram_range':(2, 2), \n",
    "}\n",
    "df, d, v = tfidf(df_fea[df_fea['smishing']==1][tfidf_src_col].values, params, 'tfidf_pos_word_22')\n",
    "dfs.append(df)\n",
    "fea_dict.update(d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-02T07:02:12.496778Z",
     "start_time": "2020-01-02T07:02:12.494799Z"
    }
   },
   "outputs": [],
   "source": [
    "# ts = '20191214T055747'\n",
    "# file_name = 'model/gensim_{}'.format(ts)\n",
    "\n",
    "# w2v_model = joblib.load(os.path.join(base_path, '{}.pkl'.format(file_name)))\n",
    "# w2v_size = w2v_model.wv.vectors.shape[1]\n",
    "\n",
    "# def mean_w2v(row):\n",
    "#     nouns = row['nouns']\n",
    "#     w2v = np.zeros(w2v_size)\n",
    "\n",
    "    \n",
    "#     for n in nouns:\n",
    "#         if n in w2v_model.wv.vocab.keys():\n",
    "#             w2v = np.add(w2v, w2v_model.wv[n])\n",
    "            \n",
    "#     return w2v if len(nouns) == 0 else np.true_divide(w2v, len(nouns))\n",
    "\n",
    "# w2v_cols = ['w2v_{}'.format(i) for i in range(w2v_size)]\n",
    "\n",
    "# df_fea[w2v_cols] = df_fea.apply(mean_w2v, axis=1, result_type='expand')\n",
    "\n",
    "# for c in w2v_cols:\n",
    "#     df_fea[c] = df_fea[c].astype(np.float16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-02T07:02:25.785754Z",
     "start_time": "2020-01-02T07:02:12.497530Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 185503,
     "status": "ok",
     "timestamp": 1576287090605,
     "user": {
      "displayName": "주이클",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCWVh4nvn9788qUddhoWTl5qQoYt0bzVMDlJWUumTg=s64",
      "userId": "02708070532256873610"
     },
     "user_tz": -540
    },
    "id": "4f9oWotspZxF",
    "outputId": "6e0f2830-c6ef-4e00-9b0b-19ea6751dad0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(297571, 55)\n",
      "(297571, 999)\n",
      "(297571, 999)\n",
      "(297571, 2000)\n",
      "(297571, 2000)\n",
      "(297571, 500)\n",
      "(297571, 892)\n",
      "(297571, 500)\n",
      "(297571, 500)\n",
      "df_merged (297571, 8445)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 297571 entries, 0 to 341625\n",
      "Columns: 8445 entries, year_month to tfidf_pos_word_22_0499\n",
      "dtypes: float16(8393), int16(44), int64(1), object(7)\n",
      "memory usage: 4.7+ GB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df_fea.shape)\n",
    "for df in dfs:\n",
    "    print(df.shape)\n",
    "\n",
    "df_merged = pd.concat([df_fea] + dfs, axis=1)\n",
    "print('df_merged', df_merged.shape)\n",
    "print(df_merged.info())\n",
    "# df_merged.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-02T07:02:25.797060Z",
     "start_time": "2020-01-02T07:02:25.787254Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 185838,
     "status": "ok",
     "timestamp": 1576287091046,
     "user": {
      "displayName": "주이클",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCWVh4nvn9788qUddhoWTl5qQoYt0bzVMDlJWUumTg=s64",
      "userId": "02708070532256873610"
     },
     "user_tz": -540
    },
    "id": "QOi9dYzqpZxV",
    "outputId": "9ceb7e20-059e-4946-934e-ab85c98dca56",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8438, 8438, 997, 275)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_cols = ['id', 'year_month', 'text', 'smishing', 'nouns', 'nouns_str', 'morphs', 'morphs_str']\n",
    "\n",
    "cat_cols = []\n",
    "fea_cols = [c for c in df_merged.columns if c not in idx_cols]\n",
    "\n",
    "for c in fea_cols:\n",
    "    if c not in fea_dict.keys():\n",
    "        fea_dict[c] = c\n",
    "\n",
    "len(fea_cols), len(fea_dict.keys()), vocab_dim, max_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-02T07:02:26.733224Z",
     "start_time": "2020-01-02T07:02:25.798176Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "dtoNNPnG75fT"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df_fea, df_train, df_test\n",
    "for df in dfs:\n",
    "    del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-02T07:02:26.746044Z",
     "start_time": "2020-01-02T07:02:26.734169Z"
    }
   },
   "outputs": [],
   "source": [
    "# base_dict = (df_merged[fea_cols].max() - df_merged[fea_cols].min()).to_dict()\n",
    "\n",
    "# for c in fea_cols:\n",
    "#     df_merged[c] = df_merged[c] / base_dict[c]\n",
    "\n",
    "# print(df_merged[fea_cols].info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-02T07:02:36.910492Z",
     "start_time": "2020-01-02T07:02:26.747202Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20200102T160226_8438_275_997\n"
     ]
    }
   ],
   "source": [
    "# merged_ts = datetime.now().strftime('%Y%m%dT%H%M%S') + '_' + str(len(fea_cols))\n",
    "merged_ts = '{}_{}_{}_{}'.format(datetime.now().strftime('%Y%m%dT%H%M%S'), \n",
    "                                 str(len(fea_cols)), \n",
    "                                 str(max_len), \n",
    "                                 str(vocab_dim))\n",
    "print(merged_ts)\n",
    "for c in df_merged:\n",
    "    if c not in fea_cols + ['smishing']:\n",
    "        df_merged.drop(c, axis=1, inplace=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-02T07:02:37.208232Z",
     "start_time": "2020-01-02T07:02:36.911832Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 297571 entries, 0 to 341625\n",
      "Columns: 8439 entries, smishing to tfidf_pos_word_22_0499\n",
      "dtypes: float16(8393), int16(44), int64(1), object(1)\n",
      "memory usage: 4.7+ GB\n"
     ]
    }
   ],
   "source": [
    "df_merged.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-02T07:02:37.260826Z",
     "start_time": "2020-01-02T07:02:37.209336Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/df_merged_20200102T160226_8438_275_997_fea_dict.pkl']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(fea_dict, 'data/df_merged_{}_fea_dict.pkl'.format(merged_ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-02T07:02:48.214225Z",
     "start_time": "2020-01-02T07:02:37.261640Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/df_merged_20200102T160226_8438_275_997_train.pkl']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(df_merged.loc[df_merged['smishing'] != -1,:], 'data/df_merged_{}_train.pkl'.format(merged_ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-02T07:02:49.055864Z",
     "start_time": "2020-01-02T07:02:48.253992Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/df_merged_20200102T160226_8438_275_997_train_pos.pkl']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(df_merged.loc[df_merged['smishing'] == 1,:], 'data/df_merged_{}_train_pos.pkl'.format(merged_ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-02T07:02:49.122866Z",
     "start_time": "2020-01-02T07:02:49.056674Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/df_merged_20200102T160226_8438_275_997_test.pkl']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(df_merged.loc[df_merged['smishing'] == -1,:], 'data/df_merged_{}_test.pkl'.format(merged_ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-02T07:02:49.153359Z",
     "start_time": "2020-01-02T07:02:49.123602Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "dtoNNPnG75fT"
   },
   "outputs": [],
   "source": [
    "# del df_merged, df_fea, df_train, df_test\n",
    "# for df in dfs:\n",
    "#     del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-02T07:02:50.023041Z",
     "start_time": "2020-01-02T07:02:49.154012Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-02T07:02:50.025064Z",
     "start_time": "2020-01-02T07:02:50.023722Z"
    }
   },
   "outputs": [],
   "source": [
    "exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "mecab_tfidf.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
