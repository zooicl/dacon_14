{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T11:46:09.415394Z",
     "start_time": "2020-01-05T11:46:08.319839Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1590,
     "status": "ok",
     "timestamp": 1576286906380,
     "user": {
      "displayName": "주이클",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCWVh4nvn9788qUddhoWTl5qQoYt0bzVMDlJWUumTg=s64",
      "userId": "02708070532256873610"
     },
     "user_tz": -540
    },
    "id": "U3xmRNtgpZwi",
    "outputId": "f6e95e03-5913-4a53-9ff3-1b6cde1b5a82"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiden/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.2\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.externals import joblib \n",
    "import os\n",
    "from konlpy.tag import Mecab\n",
    "import lightgbm as lgb\n",
    "print(lgb.__version__)\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.externals import joblib \n",
    "\n",
    "import gc\n",
    "from tqdm import tqdm_notebook\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T11:46:11.243727Z",
     "start_time": "2020-01-05T11:46:09.416469Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aiden/src/dacon_14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(297571, 3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "\n",
    "base_path = '.'\n",
    "\n",
    "df_train = pd.read_csv(os.path.join(base_path , 'input/train.csv'), index_col=0)\n",
    "df_test = pd.read_csv(os.path.join(base_path , 'input/public_test.csv'), index_col=0)\n",
    "df_test['smishing'] = -1\n",
    "\n",
    "df_fea = pd.concat([df_train, df_test])\n",
    "df_fea.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mecab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T11:46:11.247274Z",
     "start_time": "2020-01-05T11:46:11.244814Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "uquxguHUpZwt"
   },
   "outputs": [],
   "source": [
    "# mecab = Mecab()\n",
    "# df_fea['morphs'] = df_fea['text'].apply(lambda x: mecab.morphs(x))\n",
    "# df_fea['morphs_str'] = df_fea['morphs'].apply(lambda x: ' '.join(x))\n",
    "# df_fea['nouns'] = df_fea['text'].apply(lambda x: mecab.nouns(x))\n",
    "# df_fea['nouns_str'] = df_fea['nouns'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# def pos(row):\n",
    "#     x = row['text']\n",
    "#     pos_dict = {c:0 for c in pos_cols}\n",
    "    \n",
    "#     for _, p in mecab.pos(x):\n",
    "#         for v in p.split('+'):\n",
    "#             pos_dict[v] += 1\n",
    "        \n",
    "#     return [pos_dict[k] for k in sorted(pos_dict.keys())]\n",
    "\n",
    "# pos_cols = [\n",
    "#     'EC',\n",
    "#     'EF',\n",
    "#     'EP',\n",
    "#     'ETM',\n",
    "#     'ETN',\n",
    "#     'IC',\n",
    "#     'JC',\n",
    "#     'JKB',\n",
    "#     'JKC',\n",
    "#     'JKG',\n",
    "#     'JKO',\n",
    "#     'JKQ',\n",
    "#     'JKS',\n",
    "#     'JKV',\n",
    "#     'JX',\n",
    "#     'MAG',\n",
    "#     'MAJ',\n",
    "#     'MM',\n",
    "#     'NA',\n",
    "#     'NNB',\n",
    "#     'NNBC',\n",
    "#     'NNG',\n",
    "#     'NNP',\n",
    "#     'NP',\n",
    "#     'NR',\n",
    "#     'SC',\n",
    "#     'SF',\n",
    "#     'SL',\n",
    "#     'SN',\n",
    "#     'SSC',\n",
    "#     'SSO',\n",
    "#     'SY',\n",
    "#     'UNA',\n",
    "#     'UNKNOWN',\n",
    "#     'VA',\n",
    "#     'VCN',\n",
    "#     'VCP',\n",
    "#     'VV',\n",
    "#     'VX',\n",
    "#     'XPN',\n",
    "#     'XR',\n",
    "#     'XSA',\n",
    "#     'XSN',\n",
    "#     'XSV',\n",
    "# ]\n",
    "# df_fea[pos_cols] = df_fea.apply(pos, axis=1, result_type='expand')\n",
    "# df_fea[pos_cols] = df_fea[pos_cols].astype(np.int16)\n",
    "# df_fea.rename(columns={c:'fea__'+c for c in pos_cols}, inplace=True)\n",
    "# df_fea.to_pickle('data/df_fea_1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T11:46:25.542263Z",
     "start_time": "2020-01-05T11:46:12.951591Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year_month</th>\n",
       "      <th>text</th>\n",
       "      <th>smishing</th>\n",
       "      <th>morphs</th>\n",
       "      <th>morphs_str</th>\n",
       "      <th>nouns</th>\n",
       "      <th>nouns_str</th>\n",
       "      <th>fea__EC</th>\n",
       "      <th>fea__EF</th>\n",
       "      <th>fea__EP</th>\n",
       "      <th>...</th>\n",
       "      <th>fea__VA</th>\n",
       "      <th>fea__VCN</th>\n",
       "      <th>fea__VCP</th>\n",
       "      <th>fea__VV</th>\n",
       "      <th>fea__VX</th>\n",
       "      <th>fea__XPN</th>\n",
       "      <th>fea__XR</th>\n",
       "      <th>fea__XSA</th>\n",
       "      <th>fea__XSN</th>\n",
       "      <th>fea__XSV</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2017-01</td>\n",
       "      <td>XXX은행성산XXX팀장입니다.행복한주말되세요</td>\n",
       "      <td>0</td>\n",
       "      <td>[XXX, 은, 행성, 산, XXX, 팀장, 입니다, ., 행복, 한, 주말, 되,...</td>\n",
       "      <td>XXX 은 행성 산 XXX 팀장 입니다 . 행복 한 주말 되 세요</td>\n",
       "      <td>[행성, 산, 팀장, 행복, 주말]</td>\n",
       "      <td>행성 산 팀장 행복 주말</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2017-01</td>\n",
       "      <td>오늘도많이웃으시는하루시작하세요XXX은행 진월동VIP라운지 XXX올림</td>\n",
       "      <td>0</td>\n",
       "      <td>[오늘, 도, 많이, 웃, 으시, 는, 하루, 시작, 하, 세요, XXX, 은행, ...</td>\n",
       "      <td>오늘 도 많이 웃 으시 는 하루 시작 하 세요 XXX 은행 진월동 VIP 라운지 X...</td>\n",
       "      <td>[오늘, 하루, 시작, 은행, 진월동, 라운지]</td>\n",
       "      <td>오늘 하루 시작 은행 진월동 라운지</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2017-01</td>\n",
       "      <td>안녕하십니까 고객님. XXX은행입니다.금일 납부하셔야 할 금액은 153600원 입니...</td>\n",
       "      <td>0</td>\n",
       "      <td>[안녕, 하, 십니까, 고객, 님, ., XXX, 은행, 입니다, ., 금일, 납부...</td>\n",
       "      <td>안녕 하 십니까 고객 님 . XXX 은행 입니다 . 금일 납부 하 셔야 할 금액 은...</td>\n",
       "      <td>[안녕, 고객, 은행, 금일, 납부, 금액, 원, 감사, 새해, 복, 은행, 옥포]</td>\n",
       "      <td>안녕 고객 은행 금일 납부 금액 원 감사 새해 복 은행 옥포</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2017-01</td>\n",
       "      <td>XXX 고객님안녕하세요XXX은행 XXX지점입니다지난 한 해 동안 저희 XXX지점에 ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[XXX, 고객, 님, 안녕, 하, 세요, XXX, 은행, XXX, 지점, 입니다,...</td>\n",
       "      <td>XXX 고객 님 안녕 하 세요 XXX 은행 XXX 지점 입니다 지난 한 해 동안 저...</td>\n",
       "      <td>[고객, 안녕, 은행, 지점, 해, 동안, 저희, 지점, 성원, 감사, 시작, 년,...</td>\n",
       "      <td>고객 안녕 은행 지점 해 동안 저희 지점 성원 감사 시작 년 소망 일 고객 가정 건...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2017-01</td>\n",
       "      <td>1월은 새로움이 가득XXX입니다.올 한해 더 많이행복한 한해되시길바랍니다</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 월, 은, 새로움, 이, 가득, XXX, 입니다, ., 올, 한, 해, 더,...</td>\n",
       "      <td>1 월 은 새로움 이 가득 XXX 입니다 . 올 한 해 더 많이 행복 한 한 해 되...</td>\n",
       "      <td>[월, 한, 행복, 해]</td>\n",
       "      <td>월 한 행복 해</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2017-01</td>\n",
       "      <td>행복한주말보내세요XXX용현남전담직원대리 XXX올림</td>\n",
       "      <td>0</td>\n",
       "      <td>[행복, 한, 주말, 보내, 세요, XXX, 용, 현남, 전담, 직원, 대리, XX...</td>\n",
       "      <td>행복 한 주말 보내 세요 XXX 용 현남 전담 직원 대리 XXX 올림</td>\n",
       "      <td>[행복, 주말, 현남, 전담, 직원, 대리]</td>\n",
       "      <td>행복 주말 현남 전담 직원 대리</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2017-01</td>\n",
       "      <td>XXX 고객님 안녕하세요XXX은행 무교지점 XXX과장입니다 오늘 아침에 눈을 뜨니 ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[XXX, 고객, 님, 안녕, 하, 세요, XXX, 은행, 무교, 지점, XXX, ...</td>\n",
       "      <td>XXX 고객 님 안녕 하 세요 XXX 은행 무교 지점 XXX 과장 입니다 오늘 아침...</td>\n",
       "      <td>[고객, 안녕, 은행, 무교, 지점, 과장, 아침, 눈, 눈, 세상, 적, 눈, 눈...</td>\n",
       "      <td>고객 안녕 은행 무교 지점 과장 아침 눈 눈 세상 적 눈 눈 순간 출근 걱정 어른 ...</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2017-01</td>\n",
       "      <td>XXX 고객님지난 한해 베풀어 주신 은혜 진심으로 감사 드립니다.가슴 깊이 간직 하...</td>\n",
       "      <td>0</td>\n",
       "      <td>[XXX, 고객, 님, 지난, 한, 해, 베풀, 어, 주, 신, 은혜, 진심, 으로...</td>\n",
       "      <td>XXX 고객 님 지난 한 해 베풀 어 주 신 은혜 진심 으로 감사 드립니다 . 가슴...</td>\n",
       "      <td>[고객, 한, 은혜, 진심, 감사, 가슴, 간직, 정유, 년, 새해, 가족, 행복,...</td>\n",
       "      <td>고객 한 은혜 진심 감사 가슴 간직 정유 년 새해 가족 행복 뜻 바 진심 소망 은행...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2017-01</td>\n",
       "      <td>설연휴 가족들과 훈훈한 정 나누시고 정겨운추억 많이 만드세요XXX오XXX올림</td>\n",
       "      <td>0</td>\n",
       "      <td>[설, 연휴, 가족, 들, 과, 훈훈, 한, 정, 나누, 시, 고, 정겨운, 추억,...</td>\n",
       "      <td>설 연휴 가족 들 과 훈훈 한 정 나누 시 고 정겨운 추억 많이 만드세요 XXX 오...</td>\n",
       "      <td>[설, 연휴, 가족, 정, 추억, 오]</td>\n",
       "      <td>설 연휴 가족 정 추억 오</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2017-01</td>\n",
       "      <td>(광고)XXXBaXXX고객님들 뒤엔XXX 언제나 XXX새로운 마음가짐으로 새롭게 준...</td>\n",
       "      <td>1</td>\n",
       "      <td>[(, 광고, ), XXXBaXXX, 고객, 님, 들, 뒤, 엔, XXX, 언제나,...</td>\n",
       "      <td>( 광고 ) XXXBaXXX 고객 님 들 뒤 엔 XXX 언제나 XXX 새로운 마음가...</td>\n",
       "      <td>[광고, 고객, 뒤, 마음가짐, 준비, 당, 행상, 품, 자격, 기준, 심사, 기준...</td>\n",
       "      <td>광고 고객 뒤 마음가짐 준비 당 행상 품 자격 기준 심사 기준 완화 상품 상품 정보...</td>\n",
       "      <td>29</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   year_month                                               text  smishing  \\\n",
       "id                                                                           \n",
       "0     2017-01                           XXX은행성산XXX팀장입니다.행복한주말되세요         0   \n",
       "1     2017-01              오늘도많이웃으시는하루시작하세요XXX은행 진월동VIP라운지 XXX올림         0   \n",
       "2     2017-01  안녕하십니까 고객님. XXX은행입니다.금일 납부하셔야 할 금액은 153600원 입니...         0   \n",
       "4     2017-01  XXX 고객님안녕하세요XXX은행 XXX지점입니다지난 한 해 동안 저희 XXX지점에 ...         0   \n",
       "5     2017-01           1월은 새로움이 가득XXX입니다.올 한해 더 많이행복한 한해되시길바랍니다         0   \n",
       "6     2017-01                        행복한주말보내세요XXX용현남전담직원대리 XXX올림         0   \n",
       "7     2017-01  XXX 고객님 안녕하세요XXX은행 무교지점 XXX과장입니다 오늘 아침에 눈을 뜨니 ...         0   \n",
       "8     2017-01  XXX 고객님지난 한해 베풀어 주신 은혜 진심으로 감사 드립니다.가슴 깊이 간직 하...         0   \n",
       "9     2017-01         설연휴 가족들과 훈훈한 정 나누시고 정겨운추억 많이 만드세요XXX오XXX올림         0   \n",
       "10    2017-01  (광고)XXXBaXXX고객님들 뒤엔XXX 언제나 XXX새로운 마음가짐으로 새롭게 준...         1   \n",
       "\n",
       "                                               morphs  \\\n",
       "id                                                      \n",
       "0   [XXX, 은, 행성, 산, XXX, 팀장, 입니다, ., 행복, 한, 주말, 되,...   \n",
       "1   [오늘, 도, 많이, 웃, 으시, 는, 하루, 시작, 하, 세요, XXX, 은행, ...   \n",
       "2   [안녕, 하, 십니까, 고객, 님, ., XXX, 은행, 입니다, ., 금일, 납부...   \n",
       "4   [XXX, 고객, 님, 안녕, 하, 세요, XXX, 은행, XXX, 지점, 입니다,...   \n",
       "5   [1, 월, 은, 새로움, 이, 가득, XXX, 입니다, ., 올, 한, 해, 더,...   \n",
       "6   [행복, 한, 주말, 보내, 세요, XXX, 용, 현남, 전담, 직원, 대리, XX...   \n",
       "7   [XXX, 고객, 님, 안녕, 하, 세요, XXX, 은행, 무교, 지점, XXX, ...   \n",
       "8   [XXX, 고객, 님, 지난, 한, 해, 베풀, 어, 주, 신, 은혜, 진심, 으로...   \n",
       "9   [설, 연휴, 가족, 들, 과, 훈훈, 한, 정, 나누, 시, 고, 정겨운, 추억,...   \n",
       "10  [(, 광고, ), XXXBaXXX, 고객, 님, 들, 뒤, 엔, XXX, 언제나,...   \n",
       "\n",
       "                                           morphs_str  \\\n",
       "id                                                      \n",
       "0                XXX 은 행성 산 XXX 팀장 입니다 . 행복 한 주말 되 세요   \n",
       "1   오늘 도 많이 웃 으시 는 하루 시작 하 세요 XXX 은행 진월동 VIP 라운지 X...   \n",
       "2   안녕 하 십니까 고객 님 . XXX 은행 입니다 . 금일 납부 하 셔야 할 금액 은...   \n",
       "4   XXX 고객 님 안녕 하 세요 XXX 은행 XXX 지점 입니다 지난 한 해 동안 저...   \n",
       "5   1 월 은 새로움 이 가득 XXX 입니다 . 올 한 해 더 많이 행복 한 한 해 되...   \n",
       "6              행복 한 주말 보내 세요 XXX 용 현남 전담 직원 대리 XXX 올림   \n",
       "7   XXX 고객 님 안녕 하 세요 XXX 은행 무교 지점 XXX 과장 입니다 오늘 아침...   \n",
       "8   XXX 고객 님 지난 한 해 베풀 어 주 신 은혜 진심 으로 감사 드립니다 . 가슴...   \n",
       "9   설 연휴 가족 들 과 훈훈 한 정 나누 시 고 정겨운 추억 많이 만드세요 XXX 오...   \n",
       "10  ( 광고 ) XXXBaXXX 고객 님 들 뒤 엔 XXX 언제나 XXX 새로운 마음가...   \n",
       "\n",
       "                                                nouns  \\\n",
       "id                                                      \n",
       "0                                 [행성, 산, 팀장, 행복, 주말]   \n",
       "1                          [오늘, 하루, 시작, 은행, 진월동, 라운지]   \n",
       "2      [안녕, 고객, 은행, 금일, 납부, 금액, 원, 감사, 새해, 복, 은행, 옥포]   \n",
       "4   [고객, 안녕, 은행, 지점, 해, 동안, 저희, 지점, 성원, 감사, 시작, 년,...   \n",
       "5                                       [월, 한, 행복, 해]   \n",
       "6                            [행복, 주말, 현남, 전담, 직원, 대리]   \n",
       "7   [고객, 안녕, 은행, 무교, 지점, 과장, 아침, 눈, 눈, 세상, 적, 눈, 눈...   \n",
       "8   [고객, 한, 은혜, 진심, 감사, 가슴, 간직, 정유, 년, 새해, 가족, 행복,...   \n",
       "9                               [설, 연휴, 가족, 정, 추억, 오]   \n",
       "10  [광고, 고객, 뒤, 마음가짐, 준비, 당, 행상, 품, 자격, 기준, 심사, 기준...   \n",
       "\n",
       "                                            nouns_str  fea__EC  fea__EF  \\\n",
       "id                                                                        \n",
       "0                                       행성 산 팀장 행복 주말        0        2   \n",
       "1                                 오늘 하루 시작 은행 진월동 라운지        1        1   \n",
       "2                   안녕 고객 은행 금일 납부 금액 원 감사 새해 복 은행 옥포        2        5   \n",
       "4   고객 안녕 은행 지점 해 동안 저희 지점 성원 감사 시작 년 소망 일 고객 가정 건...        1        5   \n",
       "5                                            월 한 행복 해        2        1   \n",
       "6                                   행복 주말 현남 전담 직원 대리        1        1   \n",
       "7   고객 안녕 은행 무교 지점 과장 아침 눈 눈 세상 적 눈 눈 순간 출근 걱정 어른 ...       15        8   \n",
       "8   고객 한 은혜 진심 감사 가슴 간직 정유 년 새해 가족 행복 뜻 바 진심 소망 은행...        3        3   \n",
       "9                                      설 연휴 가족 정 추억 오        3        0   \n",
       "10  광고 고객 뒤 마음가짐 준비 당 행상 품 자격 기준 심사 기준 완화 상품 상품 정보...       29        7   \n",
       "\n",
       "    fea__EP  ...  fea__VA  fea__VCN  fea__VCP  fea__VV  fea__VX  fea__XPN  \\\n",
       "id           ...                                                            \n",
       "0         1  ...        0         0         1        0        0         0   \n",
       "1         2  ...        0         0         0        2        0         0   \n",
       "2         4  ...        0         0         2        2        1         0   \n",
       "4         5  ...        0         0         1        7        1         0   \n",
       "5         1  ...        1         0         1        1        0         0   \n",
       "6         1  ...        0         0         0        2        0         0   \n",
       "7        10  ...        5         0         2       22        2         0   \n",
       "8         4  ...        0         0         0        5        1         0   \n",
       "9         2  ...        1         0         0        3        0         0   \n",
       "10       15  ...       10         0         4       18        6         1   \n",
       "\n",
       "    fea__XR  fea__XSA  fea__XSN  fea__XSV  \n",
       "id                                         \n",
       "0         0         1         0         1  \n",
       "1         0         0         0         1  \n",
       "2         0         0         1         3  \n",
       "4         0         2         2         4  \n",
       "5         0         1         0         2  \n",
       "6         0         1         1         0  \n",
       "7         2         2         2         4  \n",
       "8         0         2         1         2  \n",
       "9         1         1         1         0  \n",
       "10        2         8        12         5  \n",
       "\n",
       "[10 rows x 51 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fea = pd.read_pickle('data/df_fea_1.pkl')\n",
    "df_fea.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T11:46:25.546673Z",
     "start_time": "2020-01-05T11:46:25.543318Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['year_month', 'text', 'smishing', 'morphs', 'morphs_str', 'nouns',\n",
       "       'nouns_str', 'fea__EC', 'fea__EF', 'fea__EP', 'fea__ETM', 'fea__ETN',\n",
       "       'fea__IC', 'fea__JC', 'fea__JKB', 'fea__JKC', 'fea__JKG', 'fea__JKO',\n",
       "       'fea__JKQ', 'fea__JKS', 'fea__JKV', 'fea__JX', 'fea__MAG', 'fea__MAJ',\n",
       "       'fea__MM', 'fea__NA', 'fea__NNB', 'fea__NNBC', 'fea__NNG', 'fea__NNP',\n",
       "       'fea__NP', 'fea__NR', 'fea__SC', 'fea__SF', 'fea__SL', 'fea__SN',\n",
       "       'fea__SSC', 'fea__SSO', 'fea__SY', 'fea__UNA', 'fea__UNKNOWN',\n",
       "       'fea__VA', 'fea__VCN', 'fea__VCP', 'fea__VV', 'fea__VX', 'fea__XPN',\n",
       "       'fea__XR', 'fea__XSA', 'fea__XSN', 'fea__XSV'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fea.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diff Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T11:48:39.141270Z",
     "start_time": "2020-01-05T11:48:34.630567Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47094"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_neg_set = set()\n",
    "_ = df_fea[df_fea['smishing'] == 0]['morphs'].apply(lambda x: [vocab_neg_set.add(c) for c in x])\n",
    "len(vocab_neg_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T11:48:40.194754Z",
     "start_time": "2020-01-05T11:48:39.142171Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6205"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_pos_set = set()\n",
    "_ = df_fea[df_fea['smishing'] == 1]['morphs'].apply(lambda x: [vocab_pos_set.add(c) for c in x])\n",
    "len(vocab_pos_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T11:48:40.278605Z",
     "start_time": "2020-01-05T11:48:40.195701Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7760"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_test_set = set()\n",
    "_ = df_fea[df_fea['smishing'] == -1]['morphs'].apply(lambda x: [vocab_test_set.add(c) for c in x])\n",
    "len(vocab_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T11:54:45.407205Z",
     "start_time": "2020-01-05T11:54:45.400554Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18703, 277242)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_size = (df_fea['smishing'] == 1).sum()\n",
    "neg_size = (df_fea['smishing'] == 0).sum()\n",
    "pos_size, neg_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T11:54:00.573170Z",
     "start_time": "2020-01-05T11:54:00.569261Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4498"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_neg_set & vocab_pos_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T12:04:44.343159Z",
     "start_time": "2020-01-05T12:04:44.331376Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T20:31:35.507536Z",
     "start_time": "2020-01-05T20:31:33.829870Z"
    }
   },
   "outputs": [],
   "source": [
    "total = []\n",
    "_ = df_fea[df_fea['smishing'] == 0]['morphs'].apply(lambda x: total.extend(x))\n",
    "\n",
    "df_neg = pd.DataFrame.from_dict(Counter(total),orient='index', columns=['neg_cnt'])\n",
    "df_neg['neg_rate'] = df_neg['neg_cnt'] / neg_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T20:31:36.211755Z",
     "start_time": "2020-01-05T20:31:35.508500Z"
    }
   },
   "outputs": [],
   "source": [
    "total = []\n",
    "_ = df_fea[df_fea['smishing'] == 1]['morphs'].apply(lambda x: total.extend(x))\n",
    "\n",
    "df_pos = pd.DataFrame.from_dict(Counter(total),orient='index', columns=['pos_cnt'])\n",
    "df_pos['pos_rate'] = df_pos['pos_cnt'] / pos_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T20:31:36.281637Z",
     "start_time": "2020-01-05T20:31:36.212844Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.merge(df_pos, df_neg, how='outer', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T20:31:36.285301Z",
     "start_time": "2020-01-05T20:31:36.282517Z"
    }
   },
   "outputs": [],
   "source": [
    "df['diff'] = (df['pos_rate'] - df['neg_rate']).abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T20:31:36.300793Z",
     "start_time": "2020-01-05T20:31:36.286085Z"
    }
   },
   "outputs": [],
   "source": [
    "df_res = df.sort_values('diff', ascending=False).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T20:34:05.911373Z",
     "start_time": "2020-01-05T20:34:05.888654Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['가능', '상품', '금리', '으로', '등급', '대출', 'XXX', '상담', '습니다', '신용', '한도',\n",
       "       '신청', '까지', '드립니다', '부채', '진행', '통합', '드리', '문자', '은행', '거부', '이상',\n",
       "       '직장', '추가', '전환', '사용', '방식', '합니다', '방법', '광고', '전화', '자격', '수신',\n",
       "       '이자', '에서', '대환', '자금', '거나', '금융'], dtype=object)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_res[(df_res['index'].str.len() > 1) & (df_res['diff'] > 1)]['index'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T12:02:27.546017Z",
     "start_time": "2020-01-05T12:02:26.870Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(#analyzer='char',\n",
    "                              vocabulary=vocab_neg_set & vocab_pos_set,\n",
    "                             stop_words=[], \n",
    "#                              max_df=1.0,\n",
    "#                              min_df=100\n",
    ")\n",
    "\n",
    "vectorizer = vectorizer.fit(df_fea['morphs_str'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T11:59:26.286934Z",
     "start_time": "2020-01-05T11:59:26.275485Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!',\n",
       " '!!',\n",
       " '!!!',\n",
       " '!!(',\n",
       " '!!-',\n",
       " '!(',\n",
       " '!)',\n",
       " '!-',\n",
       " '!.',\n",
       " '%',\n",
       " '%!',\n",
       " '%(',\n",
       " '%)',\n",
       " '%)(',\n",
       " '%)-',\n",
       " '%-',\n",
       " '(',\n",
       " ')',\n",
       " '-',\n",
       " '-!',\n",
       " '-(',\n",
       " '--',\n",
       " '---',\n",
       " '-----',\n",
       " '------------',\n",
       " '-------------',\n",
       " '--------------',\n",
       " '---------------',\n",
       " '---------------------',\n",
       " '-------------------------',\n",
       " '-?',\n",
       " '.',\n",
       " '.(',\n",
       " '..',\n",
       " '..(',\n",
       " '..-',\n",
       " '...',\n",
       " '....-',\n",
       " '0',\n",
       " '00',\n",
       " '000',\n",
       " '0000',\n",
       " '001',\n",
       " '0011',\n",
       " '0018',\n",
       " '002',\n",
       " '004',\n",
       " '005',\n",
       " '01',\n",
       " '010',\n",
       " '0102',\n",
       " '0103',\n",
       " '0104',\n",
       " '0105',\n",
       " '0109',\n",
       " '012',\n",
       " '013',\n",
       " '015',\n",
       " '017',\n",
       " '02',\n",
       " '023',\n",
       " '024',\n",
       " '03',\n",
       " '031',\n",
       " '04',\n",
       " '05',\n",
       " '055',\n",
       " '06',\n",
       " '07',\n",
       " '070',\n",
       " '0709',\n",
       " '072',\n",
       " '0727',\n",
       " '08',\n",
       " '080',\n",
       " '0808',\n",
       " '09',\n",
       " '0926',\n",
       " '097',\n",
       " '0976',\n",
       " '0988',\n",
       " '1',\n",
       " '10',\n",
       " '100',\n",
       " '1000',\n",
       " '10000',\n",
       " '1001',\n",
       " '1002',\n",
       " '1003',\n",
       " '1004',\n",
       " '1005',\n",
       " '1006',\n",
       " '101',\n",
       " '1013',\n",
       " '1015',\n",
       " '1016',\n",
       " '1019',\n",
       " '102',\n",
       " '1020',\n",
       " '104',\n",
       " '105',\n",
       " '10530',\n",
       " '1054',\n",
       " '106',\n",
       " '108',\n",
       " '1080',\n",
       " '11',\n",
       " '110',\n",
       " '1100',\n",
       " '11000',\n",
       " '111',\n",
       " '1111',\n",
       " '1112',\n",
       " '11128',\n",
       " '1114',\n",
       " '1115',\n",
       " '1119',\n",
       " '112',\n",
       " '1122',\n",
       " '1123',\n",
       " '1124',\n",
       " '1128',\n",
       " '1129',\n",
       " '114',\n",
       " '115',\n",
       " '1166',\n",
       " '1167',\n",
       " '1168',\n",
       " '1169',\n",
       " '117',\n",
       " '1170',\n",
       " '1171',\n",
       " '1172',\n",
       " '1173',\n",
       " '119',\n",
       " '11977',\n",
       " '12',\n",
       " '120',\n",
       " '1200',\n",
       " '12000',\n",
       " '1202',\n",
       " '1217',\n",
       " '1219',\n",
       " '1221',\n",
       " '1227',\n",
       " '1229',\n",
       " '123',\n",
       " '1234',\n",
       " '125',\n",
       " '1259',\n",
       " '126',\n",
       " '1260',\n",
       " '1262',\n",
       " '1272',\n",
       " '128',\n",
       " '129',\n",
       " '13',\n",
       " '130',\n",
       " '1300',\n",
       " '13000',\n",
       " '131',\n",
       " '1327',\n",
       " '133',\n",
       " '136',\n",
       " '1367',\n",
       " '1372',\n",
       " '1375',\n",
       " '1386',\n",
       " '1389',\n",
       " '14',\n",
       " '14000',\n",
       " '1460',\n",
       " '1461',\n",
       " '147',\n",
       " '1489',\n",
       " '149',\n",
       " '15',\n",
       " '150',\n",
       " '1500',\n",
       " '15000',\n",
       " '151',\n",
       " '152',\n",
       " '1520',\n",
       " '152018',\n",
       " '153600',\n",
       " '1544',\n",
       " '1546',\n",
       " '156',\n",
       " '1566',\n",
       " '157',\n",
       " '1577',\n",
       " '1588',\n",
       " '159',\n",
       " '1599',\n",
       " '16',\n",
       " '160',\n",
       " '1600',\n",
       " '1601',\n",
       " '163',\n",
       " '1644',\n",
       " '1648',\n",
       " '166',\n",
       " '1661',\n",
       " '1666',\n",
       " '167',\n",
       " '1670',\n",
       " '1688',\n",
       " '17',\n",
       " '1700',\n",
       " '1711',\n",
       " '1751',\n",
       " '18',\n",
       " '180',\n",
       " '1800',\n",
       " '1811',\n",
       " '182',\n",
       " '1833',\n",
       " '187',\n",
       " '1877',\n",
       " '1899',\n",
       " '19',\n",
       " '190',\n",
       " '19000',\n",
       " '191',\n",
       " '194',\n",
       " '1970',\n",
       " '199',\n",
       " '2',\n",
       " '20',\n",
       " '200',\n",
       " '2000',\n",
       " '20000',\n",
       " '200250',\n",
       " '2008',\n",
       " '2009',\n",
       " '201',\n",
       " '2010',\n",
       " '2011',\n",
       " '2012',\n",
       " '2013',\n",
       " '2014',\n",
       " '2015',\n",
       " '2016',\n",
       " '20162017',\n",
       " '2017',\n",
       " '2018',\n",
       " '2019',\n",
       " '202',\n",
       " '2030',\n",
       " '2036',\n",
       " '2040',\n",
       " '20650',\n",
       " '2068',\n",
       " '207',\n",
       " '209',\n",
       " '21',\n",
       " '211',\n",
       " '212018',\n",
       " '2137',\n",
       " '2138',\n",
       " '214',\n",
       " '2140',\n",
       " '2141',\n",
       " '215',\n",
       " '2156',\n",
       " '2157',\n",
       " '216',\n",
       " '217',\n",
       " '2190',\n",
       " '2195',\n",
       " '22',\n",
       " '220',\n",
       " '22000',\n",
       " '22018',\n",
       " '2209',\n",
       " '225',\n",
       " '2250',\n",
       " '2279',\n",
       " '2297',\n",
       " '23',\n",
       " '230',\n",
       " '2303',\n",
       " '234',\n",
       " '235',\n",
       " '2357',\n",
       " '24',\n",
       " '240',\n",
       " '2416',\n",
       " '2427',\n",
       " '2435',\n",
       " '2437',\n",
       " '2451',\n",
       " '246',\n",
       " '248',\n",
       " '25',\n",
       " '250',\n",
       " '2500',\n",
       " '255',\n",
       " '256',\n",
       " '2574',\n",
       " '258',\n",
       " '2597',\n",
       " '26',\n",
       " '260',\n",
       " '263',\n",
       " '2632',\n",
       " '2634',\n",
       " '2635',\n",
       " '2695',\n",
       " '27',\n",
       " '2706',\n",
       " '2708',\n",
       " '2710',\n",
       " '27306',\n",
       " '2768',\n",
       " '28',\n",
       " '280',\n",
       " '2813',\n",
       " '2834',\n",
       " '2837',\n",
       " '2846',\n",
       " '285',\n",
       " '2853',\n",
       " '2865',\n",
       " '2874',\n",
       " '2893',\n",
       " '29',\n",
       " '294',\n",
       " '2971',\n",
       " '29900',\n",
       " '3',\n",
       " '30',\n",
       " '300',\n",
       " '3000',\n",
       " '30000',\n",
       " '301',\n",
       " '3012',\n",
       " '3050',\n",
       " '3051',\n",
       " '3081',\n",
       " '31',\n",
       " '311',\n",
       " '314',\n",
       " '3155',\n",
       " '3172',\n",
       " '32',\n",
       " '32018',\n",
       " '3203',\n",
       " '321',\n",
       " '3222',\n",
       " '33',\n",
       " '330',\n",
       " '333',\n",
       " '3348',\n",
       " '3377',\n",
       " '3394',\n",
       " '34',\n",
       " '3400',\n",
       " '3422',\n",
       " '3461',\n",
       " '3468',\n",
       " '3469',\n",
       " '35',\n",
       " '350',\n",
       " '3500',\n",
       " '3505',\n",
       " '351',\n",
       " '3516',\n",
       " '3526',\n",
       " '3531',\n",
       " '3564',\n",
       " '36',\n",
       " '365',\n",
       " '366',\n",
       " '369',\n",
       " '37',\n",
       " '38',\n",
       " '382',\n",
       " '39',\n",
       " '4',\n",
       " '40',\n",
       " '400',\n",
       " '4000',\n",
       " '40000',\n",
       " '401',\n",
       " '402',\n",
       " '404',\n",
       " '4048',\n",
       " '4062',\n",
       " '41',\n",
       " '410',\n",
       " '4123',\n",
       " '4143',\n",
       " '42',\n",
       " '420',\n",
       " '42000',\n",
       " '4237',\n",
       " '424',\n",
       " '43',\n",
       " '439',\n",
       " '44',\n",
       " '4407',\n",
       " '45',\n",
       " '4500',\n",
       " '45000',\n",
       " '456',\n",
       " '4585',\n",
       " '459',\n",
       " '46',\n",
       " '4641',\n",
       " '4647',\n",
       " '466',\n",
       " '4689',\n",
       " '47',\n",
       " '470',\n",
       " '476',\n",
       " '48',\n",
       " '4800',\n",
       " '4833',\n",
       " '4896',\n",
       " '49',\n",
       " '4924',\n",
       " '4989',\n",
       " '5',\n",
       " '50',\n",
       " '500',\n",
       " '5000',\n",
       " '501',\n",
       " '51',\n",
       " '511',\n",
       " '5123',\n",
       " '5127',\n",
       " '513',\n",
       " '514',\n",
       " '515',\n",
       " '517',\n",
       " '518',\n",
       " '519',\n",
       " '52',\n",
       " '5201',\n",
       " '5237',\n",
       " '53',\n",
       " '5373',\n",
       " '5393',\n",
       " '54',\n",
       " '5408',\n",
       " '55',\n",
       " '5509',\n",
       " '552',\n",
       " '56',\n",
       " '5614',\n",
       " '5669',\n",
       " '57',\n",
       " '5778',\n",
       " '578',\n",
       " '5788',\n",
       " '58',\n",
       " '5856',\n",
       " '59',\n",
       " '590',\n",
       " '5906',\n",
       " '5925',\n",
       " '596',\n",
       " '6',\n",
       " '60',\n",
       " '600',\n",
       " '6000',\n",
       " '60000',\n",
       " '606',\n",
       " '608',\n",
       " '61',\n",
       " '610',\n",
       " '616',\n",
       " '62',\n",
       " '620',\n",
       " '6239',\n",
       " '625',\n",
       " '63',\n",
       " '6327',\n",
       " '633',\n",
       " '634',\n",
       " '6390',\n",
       " '64',\n",
       " '6400',\n",
       " '6412',\n",
       " '6432',\n",
       " '65',\n",
       " '6538',\n",
       " '658',\n",
       " '66',\n",
       " '665',\n",
       " '67',\n",
       " '671',\n",
       " '6740',\n",
       " '68',\n",
       " '680',\n",
       " '6800',\n",
       " '6821',\n",
       " '6845',\n",
       " '69',\n",
       " '695',\n",
       " '7',\n",
       " '70',\n",
       " '700',\n",
       " '7000000',\n",
       " '702',\n",
       " '7040',\n",
       " '7064',\n",
       " '7080',\n",
       " '71',\n",
       " '710',\n",
       " '711',\n",
       " '7115',\n",
       " '712',\n",
       " '713',\n",
       " '717',\n",
       " '719',\n",
       " '72',\n",
       " '720',\n",
       " '7218',\n",
       " '724',\n",
       " '7243',\n",
       " '725',\n",
       " '726',\n",
       " '7271',\n",
       " '728',\n",
       " '7289',\n",
       " '73',\n",
       " '7300',\n",
       " '731',\n",
       " '7348',\n",
       " '74',\n",
       " '741',\n",
       " '745',\n",
       " '748',\n",
       " '7481',\n",
       " '75',\n",
       " '7500',\n",
       " '7503',\n",
       " '752',\n",
       " '758',\n",
       " '76',\n",
       " '7682',\n",
       " '7692',\n",
       " '77',\n",
       " '776',\n",
       " '78',\n",
       " '780',\n",
       " '79',\n",
       " '790',\n",
       " '791',\n",
       " '7917',\n",
       " '8',\n",
       " '80',\n",
       " '800',\n",
       " '8000',\n",
       " '81',\n",
       " '810',\n",
       " '8103',\n",
       " '814',\n",
       " '815',\n",
       " '816',\n",
       " '817',\n",
       " '82',\n",
       " '83',\n",
       " '832',\n",
       " '834',\n",
       " '836',\n",
       " '84',\n",
       " '840',\n",
       " '85',\n",
       " '850',\n",
       " '86',\n",
       " '8600',\n",
       " '87',\n",
       " '870',\n",
       " '871',\n",
       " '875',\n",
       " '876',\n",
       " '877',\n",
       " '88',\n",
       " '8839',\n",
       " '885',\n",
       " '886',\n",
       " '888',\n",
       " '89',\n",
       " '891',\n",
       " '8924',\n",
       " '9',\n",
       " '90',\n",
       " '900',\n",
       " '9000',\n",
       " '91',\n",
       " '911',\n",
       " '9115',\n",
       " '914',\n",
       " '915',\n",
       " '917',\n",
       " '918',\n",
       " '919',\n",
       " '92',\n",
       " '921',\n",
       " '923',\n",
       " '924',\n",
       " '927',\n",
       " '929',\n",
       " '93',\n",
       " '930',\n",
       " '932',\n",
       " '94',\n",
       " '9411',\n",
       " '946',\n",
       " '95',\n",
       " '958',\n",
       " '96',\n",
       " '9683',\n",
       " '97',\n",
       " '970',\n",
       " '9715',\n",
       " '9742',\n",
       " '98',\n",
       " '9802',\n",
       " '9833',\n",
       " '99',\n",
       " '9900',\n",
       " '9948',\n",
       " '9955',\n",
       " ':',\n",
       " '?',\n",
       " '??',\n",
       " '??(',\n",
       " 'A',\n",
       " 'AB',\n",
       " 'ABC',\n",
       " 'AE',\n",
       " 'AM',\n",
       " 'APT',\n",
       " 'ARS',\n",
       " 'AT',\n",
       " 'B',\n",
       " 'BANK',\n",
       " 'BB',\n",
       " 'BC',\n",
       " 'BIFC',\n",
       " 'BL',\n",
       " 'BN',\n",
       " 'BOJ',\n",
       " 'BW',\n",
       " 'BXXX',\n",
       " 'Br',\n",
       " 'C',\n",
       " 'CB',\n",
       " 'CC',\n",
       " 'CCTV',\n",
       " 'CD',\n",
       " 'CE',\n",
       " 'CEO',\n",
       " 'CF',\n",
       " 'CMS',\n",
       " 'COFIX',\n",
       " 'CSS',\n",
       " 'CT',\n",
       " 'CU',\n",
       " 'D',\n",
       " 'DAX',\n",
       " 'DC',\n",
       " 'DSR',\n",
       " 'DTI',\n",
       " 'E',\n",
       " 'EB',\n",
       " 'ECB',\n",
       " 'EU',\n",
       " 'F',\n",
       " 'FD',\n",
       " 'FOMC',\n",
       " 'FW',\n",
       " 'Fed',\n",
       " 'Fr',\n",
       " 'G',\n",
       " 'GM',\n",
       " 'H',\n",
       " 'HA',\n",
       " 'HD',\n",
       " 'HF',\n",
       " 'HN',\n",
       " 'HP',\n",
       " 'HXXX',\n",
       " 'I',\n",
       " 'ID',\n",
       " 'II',\n",
       " 'IN',\n",
       " 'IT',\n",
       " 'IXXX',\n",
       " 'J',\n",
       " 'JB',\n",
       " 'JC',\n",
       " 'JT',\n",
       " 'K',\n",
       " 'KA',\n",
       " 'KI',\n",
       " 'KM',\n",
       " 'KTXXX',\n",
       " 'KW',\n",
       " 'KXXX',\n",
       " 'Korea',\n",
       " 'L',\n",
       " 'LC',\n",
       " 'LD',\n",
       " 'LG',\n",
       " 'LHXXX',\n",
       " 'LMS',\n",
       " 'LOAN',\n",
       " 'M',\n",
       " 'MH',\n",
       " 'MJ',\n",
       " 'MOR',\n",
       " 'MXXXI',\n",
       " 'Merry',\n",
       " 'N',\n",
       " 'NICE',\n",
       " 'NXXX',\n",
       " 'New',\n",
       " 'O',\n",
       " 'OD',\n",
       " 'OK',\n",
       " 'OO',\n",
       " 'P',\n",
       " 'PM',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'RD',\n",
       " 'RM',\n",
       " 'Report',\n",
       " 'S',\n",
       " 'SGI',\n",
       " 'SK',\n",
       " 'SM',\n",
       " 'SP',\n",
       " 'SSG',\n",
       " 'SXXX',\n",
       " 'Standard',\n",
       " 'T',\n",
       " 'TB',\n",
       " 'TEL',\n",
       " 'TH',\n",
       " 'TO',\n",
       " 'TOP',\n",
       " 'TV',\n",
       " 'TY',\n",
       " 'Tel',\n",
       " 'Tip',\n",
       " 'U',\n",
       " 'UP',\n",
       " 'URL',\n",
       " 'USB',\n",
       " 'USD',\n",
       " 'V',\n",
       " 'VAT',\n",
       " 'VIP',\n",
       " 'W',\n",
       " 'WO',\n",
       " 'WU',\n",
       " 'WX',\n",
       " 'We',\n",
       " 'Web',\n",
       " 'X',\n",
       " 'XX',\n",
       " 'XXX',\n",
       " 'XXXA',\n",
       " 'XXXBC',\n",
       " 'XXXBXXX',\n",
       " 'XXXFW',\n",
       " 'XXXFWXXX',\n",
       " 'XXXH',\n",
       " 'XXXHP',\n",
       " 'XXXK',\n",
       " 'XXXM',\n",
       " 'XXXTEL',\n",
       " 'XXXXX',\n",
       " 'XXXXXX',\n",
       " 'XXXXXXXXX',\n",
       " 'XXXbank',\n",
       " 'XXXd',\n",
       " 'XXXe',\n",
       " 'XXXk',\n",
       " 'XXXkB',\n",
       " 'XZ',\n",
       " 'Y',\n",
       " 'Z',\n",
       " 'ZH',\n",
       " 'a',\n",
       " 'aXX',\n",
       " 'aaa',\n",
       " 'abc',\n",
       " 'ace',\n",
       " 'and',\n",
       " 'ars',\n",
       " 'as',\n",
       " 'b',\n",
       " 'ba',\n",
       " 'bank',\n",
       " 'bb',\n",
       " 'c',\n",
       " 'cXXX',\n",
       " 'cc',\n",
       " 'citibank',\n",
       " 'cm',\n",
       " 'd',\n",
       " 'e',\n",
       " 'ex',\n",
       " 'f',\n",
       " 'for',\n",
       " 'g',\n",
       " 'go',\n",
       " 'good',\n",
       " 'h',\n",
       " 'happy',\n",
       " 'html',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'kakao',\n",
       " 'kg',\n",
       " 'kr',\n",
       " 'l',\n",
       " 'lXXX',\n",
       " 'm',\n",
       " 'ml',\n",
       " 'n',\n",
       " 'new',\n",
       " 'null',\n",
       " 'office',\n",
       " 'ok',\n",
       " 'ooo',\n",
       " 'open',\n",
       " 'or',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'talk',\n",
       " 'tel',\n",
       " 'th',\n",
       " 'tip',\n",
       " 'tv',\n",
       " 'u',\n",
       " 'un',\n",
       " 'v',\n",
       " 'w',\n",
       " 'ww',\n",
       " 'x',\n",
       " 'xa',\n",
       " 'year',\n",
       " 'z',\n",
       " 'ㄱ',\n",
       " 'ㄷ',\n",
       " 'ㄹ',\n",
       " 'ㅁ',\n",
       " 'ㅂ',\n",
       " 'ㅅ',\n",
       " 'ㅇ',\n",
       " 'ㅈ',\n",
       " 'ㅋ',\n",
       " 'ㅡ',\n",
       " 'ㅡㅡ',\n",
       " 'ㅣ',\n",
       " '가',\n",
       " '가계',\n",
       " '가까운',\n",
       " '가까이',\n",
       " '가깝',\n",
       " '가능',\n",
       " '가동',\n",
       " '가득',\n",
       " '가디건',\n",
       " '가려운',\n",
       " '가리',\n",
       " '가뭄',\n",
       " '가방',\n",
       " '가비',\n",
       " '가산',\n",
       " '가상',\n",
       " '가셔도',\n",
       " '가스',\n",
       " '가승',\n",
       " '가압류',\n",
       " '가양',\n",
       " '가업',\n",
       " '가요',\n",
       " '가운데',\n",
       " '가을',\n",
       " '가을맞이',\n",
       " '가이',\n",
       " '가이드',\n",
       " '가입',\n",
       " '가입자',\n",
       " '가장',\n",
       " '가전',\n",
       " '가전제품',\n",
       " '가정',\n",
       " '가정주부',\n",
       " '가져가',\n",
       " '가족',\n",
       " '가죽',\n",
       " '가즈',\n",
       " '가지',\n",
       " '가치',\n",
       " '각',\n",
       " '각종',\n",
       " '간',\n",
       " '간곡히',\n",
       " '간단',\n",
       " '간단히',\n",
       " '간략',\n",
       " '간략히',\n",
       " '간식',\n",
       " '간접',\n",
       " '간주',\n",
       " '간편',\n",
       " '간혹',\n",
       " '갈',\n",
       " '갈수록',\n",
       " '갈아타',\n",
       " '갉아먹',\n",
       " '감',\n",
       " '감기',\n",
       " '감당',\n",
       " '감면',\n",
       " '감사',\n",
       " '감소',\n",
       " '감시',\n",
       " '감시인',\n",
       " '감안',\n",
       " '감염',\n",
       " '감히',\n",
       " '갑',\n",
       " '갑니다',\n",
       " '값',\n",
       " '갔',\n",
       " '강',\n",
       " '강남구',\n",
       " '강력',\n",
       " '강사',\n",
       " '강서',\n",
       " '강서구',\n",
       " '강세',\n",
       " '강의',\n",
       " '강제',\n",
       " '강제집행',\n",
       " '강화',\n",
       " '갖',\n",
       " '갖추',\n",
       " '갖춰',\n",
       " '같',\n",
       " '같이',\n",
       " '갚',\n",
       " '개',\n",
       " '개개인',\n",
       " '개념',\n",
       " '개발',\n",
       " '개방',\n",
       " '개별',\n",
       " '개선',\n",
       " '개설',\n",
       " '개시',\n",
       " '개업',\n",
       " '개요',\n",
       " '개월',\n",
       " '개인',\n",
       " '개인사',\n",
       " '개정',\n",
       " '개편',\n",
       " '객',\n",
       " '객관적으로',\n",
       " '객님',\n",
       " '갤럭시',\n",
       " '거',\n",
       " '거나',\n",
       " '거래',\n",
       " '거래소',\n",
       " '거리',\n",
       " '거부',\n",
       " '거의',\n",
       " '거절',\n",
       " '거제',\n",
       " '거주',\n",
       " '거짓',\n",
       " '거치',\n",
       " '거친',\n",
       " '걱정',\n",
       " '건',\n",
       " '건가요',\n",
       " '건강',\n",
       " '건강식',\n",
       " '건강히',\n",
       " '건물',\n",
       " '건설',\n",
       " '건수',\n",
       " '건승',\n",
       " ...]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_vec = vectorizer.transform(df_fea['text'].values).toarray()\n",
    "\n",
    "cnt_dict = {'cnt_{0:04d}'.format(i):'cnt_{0:04}_{1}'.format(i, c) for i, c in enumerate(vectorizer.get_feature_names())}\n",
    "fea_dict.update(cnt_dict)\n",
    "cnt_cols = sorted(cnt_dict.keys())\n",
    "\n",
    "df_cnt_vec = pd.DataFrame(data=cnt_vec, index=df_fea.index, columns=cnt_cols, dtype=np.float16)\n",
    "df_cnt_vec = df_cnt_vec.loc[:, (df_cnt_vec != 0).any(axis=0)]\n",
    "dfs.append(df_cnt_vec)\n",
    "df_cnt_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T11:49:49.180849Z",
     "start_time": "2020-01-05T11:49:49.170209Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6582"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len((vocab_neg_set | vocab_pos_set) & vocab_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T11:50:16.465247Z",
     "start_time": "2020-01-05T11:50:16.460923Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2958"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_pos_set & vocab_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T11:51:07.595957Z",
     "start_time": "2020-01-05T11:51:07.573872Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'국무총리실',\n",
       " 'krzzknF',\n",
       " '1655',\n",
       " '대삼',\n",
       " '887520',\n",
       " '000940',\n",
       " 'kbg',\n",
       " '유오',\n",
       " '2056',\n",
       " '돋워',\n",
       " '총궐기',\n",
       " '후성',\n",
       " '클렌',\n",
       " 'google',\n",
       " '1435',\n",
       " '정판',\n",
       " '뉴트',\n",
       " 'siawasekbfg',\n",
       " '500473',\n",
       " '86737',\n",
       " '신불자',\n",
       " '26667',\n",
       " '장년층',\n",
       " 'EYPFWeIvNMpPYlZ',\n",
       " 'evtpage',\n",
       " '임일',\n",
       " 'JHfg',\n",
       " '최석태',\n",
       " '8588',\n",
       " 'KTX',\n",
       " 'bitly',\n",
       " '간선',\n",
       " '49752',\n",
       " '당산',\n",
       " '대조표',\n",
       " 'se',\n",
       " 'BNKXXX',\n",
       " '39760',\n",
       " '슬라이스',\n",
       " 'ㅇ경품',\n",
       " 'xmlnshttp',\n",
       " '초임',\n",
       " 'ParameterXXX',\n",
       " 'mbank',\n",
       " '27990',\n",
       " 'HPxdNuyQj',\n",
       " 'GrPHAa',\n",
       " '블랙박스',\n",
       " '파헤치',\n",
       " '미신고',\n",
       " '대야',\n",
       " '가브리엘',\n",
       " 'pF',\n",
       " '깐깐',\n",
       " '른주말모든짐벗어두고여유를즐기시길빕니다',\n",
       " 'krMWMO',\n",
       " 'XXXnbXXX',\n",
       " 'BGlIvW',\n",
       " '입질',\n",
       " '움보',\n",
       " '크래비스',\n",
       " 'productCdDP',\n",
       " '신용한',\n",
       " '050774',\n",
       " 'Credit',\n",
       " '회리바람꽃',\n",
       " 'BY',\n",
       " '욕실',\n",
       " '테크니컬',\n",
       " '묶이',\n",
       " '총파업',\n",
       " '위자',\n",
       " '601662',\n",
       " '첼시',\n",
       " '27879',\n",
       " '현물출자',\n",
       " '41860',\n",
       " '구청장',\n",
       " 'rxuQel',\n",
       " '사간',\n",
       " '2692',\n",
       " '석하',\n",
       " '019478',\n",
       " '0002',\n",
       " '경비대',\n",
       " '등하',\n",
       " 'BA',\n",
       " '051343',\n",
       " '192020',\n",
       " 'blog',\n",
       " '군포역',\n",
       " 'ㅇ전담창구운영',\n",
       " '선언문',\n",
       " '클렌징',\n",
       " '캡',\n",
       " '161929',\n",
       " 'BD',\n",
       " '후생',\n",
       " '용스',\n",
       " '2980',\n",
       " '말레이시아',\n",
       " '상임',\n",
       " '셰프',\n",
       " '0021',\n",
       " '명암',\n",
       " 'nonghyup',\n",
       " '피부과',\n",
       " '땅값',\n",
       " 'tTsDu',\n",
       " 'GFCA',\n",
       " 'ajl',\n",
       " '텔레',\n",
       " '알레그리',\n",
       " '김선곤',\n",
       " 'PsoKZR',\n",
       " 'reduced',\n",
       " '정운경',\n",
       " '체비',\n",
       " '미네랄',\n",
       " '차월',\n",
       " '20180906',\n",
       " '2618',\n",
       " 'Com',\n",
       " 'kresg',\n",
       " '하치',\n",
       " '053342',\n",
       " '핀트',\n",
       " '아츠',\n",
       " '13480',\n",
       " '갈마동',\n",
       " '분쉬',\n",
       " '신분당선',\n",
       " 'Interbank',\n",
       " '22090',\n",
       " '캐치',\n",
       " 'divd',\n",
       " '농지',\n",
       " 'QvwpyJQ',\n",
       " 'xml',\n",
       " 'branches',\n",
       " '533130',\n",
       " '특선',\n",
       " '상한제',\n",
       " 'Laboratories',\n",
       " '마을버스',\n",
       " 'AdZLMHnLCXjOIMxiOIo',\n",
       " '증산로',\n",
       " '권리관계',\n",
       " '뉴스와이드',\n",
       " '준비금',\n",
       " '60052',\n",
       " '판별',\n",
       " '오정석',\n",
       " 'LED',\n",
       " '20190222',\n",
       " '054086',\n",
       " 'encodingEUC',\n",
       " '9800000',\n",
       " '35410',\n",
       " '튜버',\n",
       " '116420',\n",
       " '개발제한구역',\n",
       " 'glgbtXE',\n",
       " '위클리',\n",
       " '낮춰야',\n",
       " 'krfhrT',\n",
       " '벌깨덩굴',\n",
       " 'vtKns',\n",
       " 'Line',\n",
       " '른주말모든짐벗어두고여유를즐기시길빕니니다',\n",
       " '우회전',\n",
       " '법관',\n",
       " '무기력감',\n",
       " '이기준',\n",
       " '2550255',\n",
       " '유성온천역',\n",
       " '구미시',\n",
       " '도지사',\n",
       " 'CDATM',\n",
       " 'Nhb',\n",
       " 'Zb',\n",
       " '태아',\n",
       " '쇼콜라',\n",
       " '상법',\n",
       " '3940',\n",
       " 'DGBXXX',\n",
       " '유세대',\n",
       " '봉명동',\n",
       " 'comkwonminho',\n",
       " '드카로',\n",
       " '25530',\n",
       " 'EC',\n",
       " '프레드',\n",
       " '트리뷴',\n",
       " '국거리',\n",
       " 'comXXXCardapp',\n",
       " '착륙',\n",
       " '평균치',\n",
       " '49800',\n",
       " 'order',\n",
       " 'SERVICE',\n",
       " 'same',\n",
       " '000428',\n",
       " 'GrHLi',\n",
       " 'standardchartered',\n",
       " '지자',\n",
       " '삼공사',\n",
       " '쿠팡',\n",
       " '미개',\n",
       " '지존',\n",
       " 'KGC',\n",
       " '인토',\n",
       " 'PCCrfT',\n",
       " '000938',\n",
       " '69120',\n",
       " '앳',\n",
       " '충수',\n",
       " 'hanacard',\n",
       " '냉동',\n",
       " 'nono',\n",
       " '사순',\n",
       " '퍼트리',\n",
       " '단양',\n",
       " '금률',\n",
       " '코미사',\n",
       " '180250',\n",
       " '능히',\n",
       " '수내동',\n",
       " '플레',\n",
       " 'funds',\n",
       " '평균값',\n",
       " '숙녀',\n",
       " '118680',\n",
       " 'can',\n",
       " '40002',\n",
       " 'kbXXX',\n",
       " '보행로',\n",
       " 'registration',\n",
       " 'XBTS',\n",
       " 'Paris',\n",
       " '8291935',\n",
       " '96420',\n",
       " '132019',\n",
       " '042020',\n",
       " '00255',\n",
       " '사우샘프턴',\n",
       " 'Biz',\n",
       " '노사',\n",
       " 'facebook',\n",
       " 'krCrdInfoXXX',\n",
       " '6065',\n",
       " '고품격',\n",
       " '율면',\n",
       " '스무디',\n",
       " '이상비',\n",
       " 'NFC',\n",
       " 'krZNVbb',\n",
       " 'nbspnbspnbsp',\n",
       " '거두절미',\n",
       " '명현',\n",
       " 'productCdLN',\n",
       " '12800',\n",
       " '국회의원',\n",
       " '12627',\n",
       " '동작대로',\n",
       " '279204',\n",
       " 'TA',\n",
       " '연세대',\n",
       " 'EI',\n",
       " 'krdigitalbank',\n",
       " '162019',\n",
       " '키조개',\n",
       " 'welcomesbdirect',\n",
       " '695366',\n",
       " '예식',\n",
       " 'PViHhM',\n",
       " '050802',\n",
       " '비트코인',\n",
       " '강호영',\n",
       " '부적격자',\n",
       " '타이겐',\n",
       " 'rybCBMIqRm',\n",
       " '선접',\n",
       " 'prcodeLN',\n",
       " '594809',\n",
       " 'comBzRAPIMyStarBridge',\n",
       " '26770',\n",
       " '찰보리',\n",
       " 'Thread',\n",
       " '보정역',\n",
       " '뱅상',\n",
       " '반월당',\n",
       " '사주팔자',\n",
       " 'ydqBOZmafZYEzDGm',\n",
       " 'peYalcy',\n",
       " 'DREAMo',\n",
       " '단해',\n",
       " '016613',\n",
       " 'krOCUyYou',\n",
       " 'page',\n",
       " '기발',\n",
       " '선봉장',\n",
       " 'PLAHA',\n",
       " '분배금',\n",
       " 'room',\n",
       " '153153153',\n",
       " 'facebookXXX',\n",
       " 'YQnbsp',\n",
       " '에드워',\n",
       " '용평',\n",
       " '콜버그',\n",
       " '아이젠',\n",
       " 'MasterCard',\n",
       " '범어역',\n",
       " 'rN',\n",
       " '교육비',\n",
       " '컨디',\n",
       " '0593',\n",
       " '6329',\n",
       " '장에',\n",
       " 'fintra',\n",
       " '성령',\n",
       " '아로마',\n",
       " 'MYANMAR',\n",
       " '공유자',\n",
       " '종부',\n",
       " '0515',\n",
       " '94635',\n",
       " '멋진데',\n",
       " '훨',\n",
       " 'JWSm',\n",
       " 'PLUS',\n",
       " '정성원',\n",
       " '비승',\n",
       " 'GNC',\n",
       " '24800',\n",
       " '집회',\n",
       " '169000',\n",
       " '월수입',\n",
       " '건너오',\n",
       " '너자',\n",
       " '061589',\n",
       " '바네사',\n",
       " '교통난',\n",
       " 'prcodeDP',\n",
       " '출마',\n",
       " '출제',\n",
       " '출선',\n",
       " '청약률',\n",
       " '멜버른',\n",
       " '0289',\n",
       " '152020',\n",
       " 'Face',\n",
       " '무이',\n",
       " '012020',\n",
       " '불명',\n",
       " '0582',\n",
       " '전성인',\n",
       " 'color',\n",
       " '조개관자',\n",
       " 'rekrxR',\n",
       " '당가',\n",
       " '총파',\n",
       " 'aspx',\n",
       " '대동건설',\n",
       " '한양수',\n",
       " 'FNgUWA',\n",
       " '232019',\n",
       " '3885',\n",
       " '20190315',\n",
       " '알코트',\n",
       " '이징',\n",
       " '상문',\n",
       " '버거',\n",
       " '효성동',\n",
       " '8461',\n",
       " '면내',\n",
       " '9021',\n",
       " 'cog',\n",
       " '웨스트',\n",
       " 'styleheadbody',\n",
       " 'comquics',\n",
       " '두런',\n",
       " '일렉트로닉',\n",
       " 'krigcE',\n",
       " '요구르트',\n",
       " '조정안',\n",
       " '선입',\n",
       " '자내',\n",
       " '노량진동',\n",
       " '자녀안',\n",
       " '진열',\n",
       " '20874',\n",
       " '그린랜드',\n",
       " 'kbpension',\n",
       " '16278',\n",
       " '일래',\n",
       " 'STARCMS',\n",
       " '041244',\n",
       " 'kbstar',\n",
       " '0215',\n",
       " '7777',\n",
       " '절증',\n",
       " '소백산',\n",
       " '퍼펙트',\n",
       " '6080',\n",
       " '60057',\n",
       " '차전',\n",
       " 'PLLAPDO',\n",
       " '국산',\n",
       " '더프',\n",
       " 'rZ',\n",
       " '냉장',\n",
       " 'hi',\n",
       " '061496',\n",
       " '홋스퍼',\n",
       " '20180405',\n",
       " '6489',\n",
       " '계피',\n",
       " '7838',\n",
       " 'JBPyleCGA',\n",
       " '국공유지',\n",
       " '일당',\n",
       " '점용',\n",
       " 'nbspXXX',\n",
       " '워머',\n",
       " '애써도',\n",
       " 'ㅠ조금은',\n",
       " 'krupgrade',\n",
       " '노려볼',\n",
       " '두둑이',\n",
       " '우민',\n",
       " '초지',\n",
       " '노량진로',\n",
       " '차입금',\n",
       " 'FWe',\n",
       " 'CXXXENTER',\n",
       " '노인정',\n",
       " 'commobileMCCF',\n",
       " '챔오리지널',\n",
       " '대럴',\n",
       " '나영수',\n",
       " '한낱',\n",
       " '0710',\n",
       " '자손',\n",
       " '비엠',\n",
       " '갈치',\n",
       " '스탠드',\n",
       " '성산구',\n",
       " 'hpageIOS',\n",
       " '든데',\n",
       " '배송희',\n",
       " '박길수',\n",
       " '89350',\n",
       " '거근',\n",
       " '상사법정이율',\n",
       " '271821',\n",
       " '이화여대',\n",
       " '망일',\n",
       " '륙도',\n",
       " 'BIvNMpPYlZ',\n",
       " '젤름',\n",
       " '보월',\n",
       " '061645',\n",
       " '골든벨',\n",
       " '실태',\n",
       " 'kbfg',\n",
       " '46800',\n",
       " '생후',\n",
       " '영풍',\n",
       " '경부선',\n",
       " '계셔도',\n",
       " '홍제역',\n",
       " 'mWmhOIvNMpPYlZ',\n",
       " '성수대교',\n",
       " '오뚜기',\n",
       " '받들',\n",
       " '01280',\n",
       " '25390',\n",
       " '전속력',\n",
       " 'KPS',\n",
       " '오리지날',\n",
       " '정기회',\n",
       " '27737',\n",
       " 'glEkbxdB',\n",
       " '급매',\n",
       " '212019',\n",
       " '체꽃',\n",
       " '타이거',\n",
       " '외연',\n",
       " 'WELCXXX',\n",
       " '귀마개',\n",
       " 'SERICEO',\n",
       " '시목',\n",
       " '6971',\n",
       " '산회',\n",
       " 'GxzWaT',\n",
       " '20788',\n",
       " '설적',\n",
       " '853352',\n",
       " '이닝',\n",
       " '56800',\n",
       " '탈출구',\n",
       " '광안역',\n",
       " '4444',\n",
       " '고숙',\n",
       " '396000',\n",
       " 'SERVICETransfer',\n",
       " '312019',\n",
       " '활로',\n",
       " '노포',\n",
       " '모싯대',\n",
       " 'Kb',\n",
       " 'nCVM',\n",
       " '드라마틱',\n",
       " '258100',\n",
       " '급식비',\n",
       " '경변',\n",
       " '왕숙',\n",
       " '중과',\n",
       " '055796',\n",
       " 'twitter',\n",
       " 'krNLLMS',\n",
       " 'CST',\n",
       " '기술력',\n",
       " 'shinhancard',\n",
       " '153244',\n",
       " '잔솔',\n",
       " '모공',\n",
       " '월차',\n",
       " '서초구',\n",
       " '45230',\n",
       " 'GRAND',\n",
       " '시군',\n",
       " '울려라',\n",
       " '20190212',\n",
       " '사설',\n",
       " '우선변제권',\n",
       " 'menu',\n",
       " 'gLG',\n",
       " '지하도',\n",
       " '27930',\n",
       " 'seoulgas',\n",
       " '5108698',\n",
       " '옥천군',\n",
       " '성동구',\n",
       " 'xt',\n",
       " '짬뽕',\n",
       " 'krqlvhs',\n",
       " '내단',\n",
       " 'register',\n",
       " '퇴진',\n",
       " '101103107312116911',\n",
       " 'Little',\n",
       " '헐거워',\n",
       " 'WCA',\n",
       " '0397',\n",
       " '아비오',\n",
       " 'mdirect',\n",
       " '학여울역',\n",
       " '금순',\n",
       " '홍희',\n",
       " 'RetailRM',\n",
       " '퓨처',\n",
       " 'occur',\n",
       " '0662',\n",
       " '배송지',\n",
       " '7960',\n",
       " '스파르타',\n",
       " '00128',\n",
       " 'Wn',\n",
       " '20180831',\n",
       " '원정산',\n",
       " '맞춰진',\n",
       " '스낵면',\n",
       " '8729',\n",
       " 'SALE',\n",
       " 'monaa',\n",
       " '위크',\n",
       " '유튜브',\n",
       " '시유지',\n",
       " '본드',\n",
       " '거제동',\n",
       " '040',\n",
       " '59800',\n",
       " '통일부',\n",
       " '8289734',\n",
       " 'ISAIRP',\n",
       " 'LJaDvjAhCY',\n",
       " '볼륨',\n",
       " '금공',\n",
       " '178200',\n",
       " '6718',\n",
       " '임현',\n",
       " '질퍽질퍽',\n",
       " 'comembedl',\n",
       " '천당',\n",
       " '갈마',\n",
       " 'CDPDSEQ',\n",
       " '방유',\n",
       " '금사',\n",
       " '기회비용',\n",
       " '길반',\n",
       " '필기구',\n",
       " 'QLED',\n",
       " '후원회',\n",
       " '6202',\n",
       " '1347',\n",
       " '컴플라이언스',\n",
       " '뿜어낸다',\n",
       " '소재학',\n",
       " '안산선',\n",
       " '진퇴양난',\n",
       " '54500',\n",
       " '용산역',\n",
       " '대란',\n",
       " '4580',\n",
       " '2731',\n",
       " '어울러',\n",
       " '시심',\n",
       " '상설',\n",
       " '정장',\n",
       " 'QViewPCNparamXX',\n",
       " '장보기',\n",
       " '취해야',\n",
       " 'urlparam',\n",
       " '나타낼지',\n",
       " '브이',\n",
       " '눠',\n",
       " '신반포',\n",
       " '위시',\n",
       " '일원동',\n",
       " '85870',\n",
       " 'DP',\n",
       " '스티',\n",
       " '가성',\n",
       " 'BCXXX',\n",
       " 'June',\n",
       " '달려갔',\n",
       " '김칫국',\n",
       " 'BANKTransfer',\n",
       " '222020',\n",
       " '게임방',\n",
       " '초시',\n",
       " '6973',\n",
       " 'beneficiary',\n",
       " '험로',\n",
       " '조리',\n",
       " '애국',\n",
       " '컸으며',\n",
       " '섬세히',\n",
       " '6097',\n",
       " '풀럼',\n",
       " 'inquire',\n",
       " 'MSutmmediumcpcutmcampaign',\n",
       " '딜라이트',\n",
       " '9920',\n",
       " '우파',\n",
       " 'LEMMS',\n",
       " 'krinsbnfSpcfctInq',\n",
       " '0374',\n",
       " '물려준다',\n",
       " '목심',\n",
       " '01027',\n",
       " '탁기',\n",
       " '061479',\n",
       " '선보',\n",
       " '조에',\n",
       " 'comlocalsiteview',\n",
       " 'OAG',\n",
       " '체장',\n",
       " '똘똘한',\n",
       " '11151',\n",
       " 'MSFT',\n",
       " '1522154',\n",
       " '이주성',\n",
       " '산림',\n",
       " 'glfKA',\n",
       " '감삼동',\n",
       " '주구',\n",
       " '이방',\n",
       " '스쿠트',\n",
       " '%)..',\n",
       " '인터내셔널',\n",
       " '용도지역',\n",
       " 'XXXSBXXXNBC',\n",
       " '그륀',\n",
       " 'QGMQ',\n",
       " '로자',\n",
       " 'OSK',\n",
       " 'idcom',\n",
       " '27300',\n",
       " 'chngas',\n",
       " '주섬주섬',\n",
       " '발송인',\n",
       " '192192192',\n",
       " '22320',\n",
       " 'kb',\n",
       " 'AFC',\n",
       " '횡',\n",
       " '000088',\n",
       " '수강료',\n",
       " '224812',\n",
       " '구소',\n",
       " '7249',\n",
       " '로버츠',\n",
       " '공직자',\n",
       " '72019',\n",
       " '흑당',\n",
       " '대부료',\n",
       " '중앙선',\n",
       " '요금제',\n",
       " '들어설',\n",
       " '305100',\n",
       " 'pageC',\n",
       " '050768',\n",
       " '성균관',\n",
       " '사치성',\n",
       " '덕암',\n",
       " 'commhk',\n",
       " 'glyhwqBNXXX',\n",
       " 'ClbyqY',\n",
       " '02550',\n",
       " '되돌아옴',\n",
       " '34860',\n",
       " '0211',\n",
       " '20190228',\n",
       " '아스',\n",
       " '클린',\n",
       " '콜라겐',\n",
       " '매콤',\n",
       " '미확보',\n",
       " '벽산',\n",
       " '우회로',\n",
       " '동상이몽',\n",
       " '0366',\n",
       " '11063',\n",
       " 'returnURLhttp',\n",
       " '33800',\n",
       " 'divdivcolor',\n",
       " '약제',\n",
       " '거주민',\n",
       " 'krgt',\n",
       " '20359',\n",
       " '3941',\n",
       " '리프팅',\n",
       " '유재',\n",
       " '동호대교',\n",
       " '피스',\n",
       " '통시',\n",
       " '사우스',\n",
       " '보궐',\n",
       " '9815',\n",
       " '디에',\n",
       " '아다가',\n",
       " 'solid',\n",
       " '하교',\n",
       " 'ACC',\n",
       " '폴더',\n",
       " '고속철',\n",
       " '3115790',\n",
       " '직대',\n",
       " '그으며',\n",
       " '277370',\n",
       " '스패츠',\n",
       " 'ch',\n",
       " 'GyfOoP',\n",
       " '자유인',\n",
       " 'apt',\n",
       " 'HCN',\n",
       " 'obank',\n",
       " '292020',\n",
       " '본머스',\n",
       " '력사',\n",
       " '채납',\n",
       " '헤지스',\n",
       " '4232627',\n",
       " 'filler',\n",
       " 'codeXX',\n",
       " '39103',\n",
       " 'ZMMS',\n",
       " '20190210',\n",
       " 'Nahman',\n",
       " 'SBXXXNBC',\n",
       " '0357',\n",
       " 'mlX',\n",
       " '드레서',\n",
       " 'CNCITY',\n",
       " '동일시',\n",
       " '곰달래로',\n",
       " '리버풀',\n",
       " 'krOCUyCompliance',\n",
       " '챔',\n",
       " '제보',\n",
       " '큰바늘꽃',\n",
       " '오영태',\n",
       " '연중무휴',\n",
       " '원교',\n",
       " '23400',\n",
       " '272019',\n",
       " 'xs',\n",
       " '26290',\n",
       " '법규',\n",
       " 'ㅠㅠㅠ응원해',\n",
       " 'kbbizmatch',\n",
       " '어묵',\n",
       " '민물',\n",
       " 'Abbott',\n",
       " '테니스코트',\n",
       " '생명표',\n",
       " '56854',\n",
       " '법제',\n",
       " '36975',\n",
       " 'Pen',\n",
       " '23995',\n",
       " '172200',\n",
       " '일간지',\n",
       " '사이언스',\n",
       " 'nIla',\n",
       " '모듬',\n",
       " '구겨지',\n",
       " '0741',\n",
       " '디펜스',\n",
       " '브라이트',\n",
       " '표대',\n",
       " '원년',\n",
       " 'comdirecthiXXX',\n",
       " 'XXXhemekbbankpageidC',\n",
       " 'comkwonmh',\n",
       " '1142387',\n",
       " '자본금',\n",
       " '휩',\n",
       " '통변',\n",
       " '3900',\n",
       " '자인',\n",
       " '티켓몬스터',\n",
       " '출갱',\n",
       " '371',\n",
       " '관담',\n",
       " '도산대로',\n",
       " '명리',\n",
       " '에센스',\n",
       " '계양구',\n",
       " 'ㅇ과정개요',\n",
       " '심령',\n",
       " '하생',\n",
       " '280226',\n",
       " '보타',\n",
       " '구현정',\n",
       " '정섭',\n",
       " '넥',\n",
       " '규문',\n",
       " '12900',\n",
       " '가공',\n",
       " '이자율',\n",
       " '033878',\n",
       " '2552550',\n",
       " 'org',\n",
       " '오슬러',\n",
       " '필러',\n",
       " 'comportalwseindex',\n",
       " '초소',\n",
       " '1286',\n",
       " '12101',\n",
       " 'Mail',\n",
       " '3336',\n",
       " '정부서울청사',\n",
       " 'LNS',\n",
       " '033898',\n",
       " 'version',\n",
       " '직위',\n",
       " '일념통천',\n",
       " 'starlink',\n",
       " '북카페',\n",
       " '작전역',\n",
       " '칭대',\n",
       " 'King',\n",
       " '노동조합',\n",
       " 'Existing',\n",
       " '코윈',\n",
       " '063119',\n",
       " '0285',\n",
       " 'AirPods',\n",
       " '8289335',\n",
       " '김희규',\n",
       " '리안드로',\n",
       " '강도용',\n",
       " 'Expiry',\n",
       " '가유',\n",
       " '알아냈',\n",
       " 'onland',\n",
       " '납고',\n",
       " '11768',\n",
       " '64991',\n",
       " 'BTSday',\n",
       " '따낸',\n",
       " 'pdf',\n",
       " '들릴테',\n",
       " '개운동',\n",
       " '27900',\n",
       " 'kbinsure',\n",
       " '실부',\n",
       " 'ㅇ대상고객',\n",
       " '뭄바이',\n",
       " '윤연',\n",
       " '0255255',\n",
       " '조한나',\n",
       " '감결',\n",
       " '물티슈',\n",
       " '다리미',\n",
       " '크레이지',\n",
       " '8900',\n",
       " 'INVITATION',\n",
       " 'kbliivon',\n",
       " '급열',\n",
       " '차로',\n",
       " '도로공사',\n",
       " 'xhtmlheadstyle',\n",
       " '카플란',\n",
       " '끊어져서',\n",
       " '세웠으며',\n",
       " '1258351',\n",
       " '소스',\n",
       " '질스튜어트',\n",
       " 'feelmotioncard',\n",
       " '어지러워',\n",
       " '수술비',\n",
       " 'KpGOZM',\n",
       " 'sWu',\n",
       " '1281280',\n",
       " 'TNGT',\n",
       " 'above',\n",
       " '성서동',\n",
       " 'krmovinterview',\n",
       " 'comljbds',\n",
       " 'MD',\n",
       " 'emart',\n",
       " '바위솔',\n",
       " '조광',\n",
       " '녹지',\n",
       " '국내선',\n",
       " '지원군',\n",
       " '152694',\n",
       " '9597480',\n",
       " '벌집',\n",
       " 'ㅇ상품명',\n",
       " '기업어음',\n",
       " '24000',\n",
       " '손영우',\n",
       " 'Oi',\n",
       " '휴지통',\n",
       " '끌어낼',\n",
       " '조알',\n",
       " '브루노',\n",
       " '에디션',\n",
       " 'usnafabz',\n",
       " 'interbank',\n",
       " '들썩',\n",
       " 'TXgr',\n",
       " 'Monitor',\n",
       " '격력',\n",
       " '일품',\n",
       " '70262',\n",
       " '000260',\n",
       " 'manual',\n",
       " '만년교',\n",
       " '당구장',\n",
       " 'GUIDE',\n",
       " 'QLEDTV',\n",
       " '요점',\n",
       " '여벌',\n",
       " '12060',\n",
       " '헬로',\n",
       " '다던',\n",
       " '청천',\n",
       " 'typetextcss',\n",
       " '경희대',\n",
       " '80110',\n",
       " '25830',\n",
       " '잎채소',\n",
       " '류청',\n",
       " '361',\n",
       " '나머진',\n",
       " '최호섭',\n",
       " '배랑',\n",
       " '달구벌',\n",
       " '3175220',\n",
       " '이학',\n",
       " '고속버스',\n",
       " '사액',\n",
       " '경색',\n",
       " '근종',\n",
       " '루이자',\n",
       " '월전',\n",
       " '스티비',\n",
       " 'DGB',\n",
       " 'band',\n",
       " '0548',\n",
       " 'instagram',\n",
       " '010501',\n",
       " ...}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_test_set - (vocab_neg_set | vocab_pos_set))\n",
    "vocab_test_set - (vocab_neg_set | vocab_pos_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq Fea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAX_COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T01:42:28.444268Z",
     "start_time": "2020-01-05T01:42:28.438571Z"
    }
   },
   "outputs": [],
   "source": [
    "def toidx(src_col, max_len, reverse=False):\n",
    "    idx_col_nm = '{}_{}_idx'.format(src_col, max_len)\n",
    "    if reverse:\n",
    "        idx_col_nm = idx_col_nm.replace('idx', 'ridx')\n",
    "\n",
    "    vocab_set = set()\n",
    "    _ = df_fea[src_col].apply(lambda x: [vocab_set.add(c) for c in x])\n",
    "    \n",
    "    vocab_dict = {v: i+1 for i, v in enumerate(vocab_set)}\n",
    "    vocab_dim = len(vocab_dict.keys()) + 1\n",
    "\n",
    "    def _toidx(x):\n",
    "        if reverse:\n",
    "            x = x[::-1]\n",
    "        return [int(vocab_dict[x[i]]) if i < len(x) else 0 for i in range(max_len)]\n",
    "\n",
    "#     print(vocab_dim, max_len)\n",
    "    \n",
    "    return idx_col_nm, df_fea[src_col].apply(_toidx), vocab_dim, max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T01:42:28.452033Z",
     "start_time": "2020-01-05T01:42:28.445907Z"
    }
   },
   "outputs": [],
   "source": [
    "src_col = 'morphs'\n",
    "# src_col = 'text'\n",
    "max_len = 128#512#256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T01:42:28.457643Z",
     "start_time": "2020-01-05T01:42:28.452980Z"
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# vectorizer = CountVectorizer(max_features=1000)\n",
    "# vectorizer = vectorizer.fit(df_fea['morphs_str'].values)\n",
    "\n",
    "# seq_vocab = set(vectorizer.get_feature_names())\n",
    "\n",
    "# org_max_len = df_fea[src_col].str.len().max()\n",
    "# df_fea[src_col] = df_fea[src_col].apply(lambda x: [m for m in x if m in seq_vocab])\n",
    "# max_len = df_fea[src_col].str.len().max()\n",
    "# print(org_max_len, 'to', max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T01:42:37.256745Z",
     "start_time": "2020-01-05T01:42:28.458449Z"
    }
   },
   "outputs": [],
   "source": [
    "c, d, vocab_dim, max_len = toidx(src_col, max_len)\n",
    "df_fea[c] = d\n",
    "c, vocab_dim, max_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T01:42:46.746771Z",
     "start_time": "2020-01-05T01:42:37.257559Z"
    }
   },
   "outputs": [],
   "source": [
    "c, d, vocab_dim, max_len = toidx(src_col, max_len, reverse=True)\n",
    "df_fea[c] = d\n",
    "c, vocab_dim, max_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Fea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T01:42:47.002421Z",
     "start_time": "2020-01-05T01:42:46.747589Z"
    }
   },
   "outputs": [],
   "source": [
    "df_fea['fea__text_len'] = df_fea['text'].str.len().fillna(0).astype(np.float16)\n",
    "df_fea['fea__morphs_cnt'] = df_fea['morphs'].apply(lambda x: len(x)).fillna(0).astype(np.float16)\n",
    "# df_fea['fea__noun_cnt'] = df_fea['nouns'].apply(lambda x: len(x)).fillna(0).astype(np.float16)\n",
    "\n",
    "for c in [c for df_fea.columns if 'fea__' in c]:\n",
    "    df_fea[f'rate_{c}_textlen'] = df_fea[c]  /  df_fea['fea__text_len']\n",
    "    df_fea[f'rate_{c}_morphcnt'] = df_fea[c]  /  df_fea['fea__morphs_cnt']\n",
    "    \n",
    "df_fea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T01:43:22.261042Z",
     "start_time": "2020-01-05T01:42:47.003191Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer='char',\n",
    "#                              vocabulary=vocab,\n",
    "                             stop_words=stop_words, \n",
    "                             max_df=1.0,\n",
    "                             min_df=100)\n",
    "\n",
    "# vectorizer = vectorizer.fit(df_fea[df_fea['smishing']==1]['nouns_str'].values)\n",
    "vectorizer = vectorizer.fit(df_fea['text'].values)\n",
    "cnt_vec = vectorizer.transform(df_fea['text'].values).toarray()\n",
    "\n",
    "cnt_dict = {'cnt_{0:04d}'.format(i):'cnt_{0:04}_{1}'.format(i, c) for i, c in enumerate(vectorizer.get_feature_names())}\n",
    "fea_dict.update(cnt_dict)\n",
    "cnt_cols = sorted(cnt_dict.keys())\n",
    "\n",
    "df_cnt_vec = pd.DataFrame(data=cnt_vec, index=df_fea.index, columns=cnt_cols, dtype=np.float16)\n",
    "df_cnt_vec = df_cnt_vec.loc[:, (df_cnt_vec != 0).any(axis=0)]\n",
    "dfs.append(df_cnt_vec)\n",
    "df_cnt_vec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T01:43:22.265850Z",
     "start_time": "2020-01-05T01:43:22.261739Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "l2Dgkdt6pZw_"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def tfidf(data, params, tfidf_tag):\n",
    "    vectorizer = TfidfVectorizer(**params)\n",
    "    vectorizer = vectorizer.fit(data)\n",
    "\n",
    "    d = {'{0}_{1:04d}'.format(tfidf_tag, v):'{0}_{1:04d}_{2}'.format(tfidf_tag, v, k) for k, v in sorted(vectorizer.vocabulary_.items(), key=lambda item: item[1])}\n",
    "    c = sorted(d.keys())\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "#         data=vectorizer.transform(df_fea['nouns_str'].values).toarray(),\n",
    "        data=vectorizer.transform(df_fea['morphs_str'].values).toarray(),\n",
    "        columns=c, \n",
    "        index=df_fea.index,\n",
    "        dtype=np.float16)\n",
    "    \n",
    "    # Remove all zeros column\n",
    "    df = df.loc[:, (df != 0).any(axis=0)]\n",
    "    d = {k:v for k, v in d.items() if k in df.columns}\n",
    "    \n",
    "    print(tfidf_tag, df.shape)\n",
    "    \n",
    "    return df, d, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T01:43:22.281808Z",
     "start_time": "2020-01-05T01:43:22.266534Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_src_col = 'morphs_str'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1, 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T01:44:01.788475Z",
     "start_time": "2020-01-05T01:43:22.282601Z"
    }
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'analyzer' : 'char',\n",
    "    'max_features':None, \n",
    "#     'vocabulary': vocab,\n",
    "    'stop_words':stop_words, \n",
    "#     'max_df':0.1, \n",
    "    'min_df':100, \n",
    "    'ngram_range':(1, 1), \n",
    "}\n",
    "df, d, v = tfidf(df_fea[tfidf_src_col].values, params, 'tfidf_char_11')\n",
    "dfs.append(df)\n",
    "fea_dict.update(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T01:44:32.314844Z",
     "start_time": "2020-01-05T01:44:01.789193Z"
    }
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'max_features': 3000,#2500,#2000, \n",
    "#     'vocabulary': vocab,\n",
    "    'stop_words':stop_words, \n",
    "    'max_df':1.0, \n",
    "    'min_df':200, \n",
    "    'ngram_range':(1, 1), \n",
    "}\n",
    "df, d, v = tfidf(df_fea[tfidf_src_col].values, params, 'tfidf_word_11')\n",
    "dfs.append(df)\n",
    "fea_dict.update(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T01:45:10.138801Z",
     "start_time": "2020-01-05T01:44:32.315619Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "l2Dgkdt6pZw_",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'max_features':3000,#2500,#2000, \n",
    "#     'vocabulary': vocab,\n",
    "    'stop_words':stop_words, \n",
    "    'max_df':1.0, \n",
    "    'min_df':500, \n",
    "    'ngram_range':(2, 2), \n",
    "}\n",
    "df, d, v = tfidf(df_fea[tfidf_src_col].values, params, 'tfidf_word_22')\n",
    "dfs.append(df)\n",
    "fea_dict.update(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T01:45:44.272991Z",
     "start_time": "2020-01-05T01:45:10.139532Z"
    }
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'max_features':1500, #1000,#500, \n",
    "#     'vocabulary': vocab,\n",
    "    'stop_words':stop_words, \n",
    "    'max_df':1.0, \n",
    "    'min_df':500, \n",
    "    'ngram_range':(3, 3), \n",
    "}\n",
    "df, d, v = tfidf(df_fea[tfidf_src_col].values, params, 'tfidf_word_33')\n",
    "dfs.append(df)\n",
    "fea_dict.update(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### smishing 1, 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-01T09:36:52.987190Z",
     "start_time": "2020-01-01T09:36:52.984965Z"
    }
   },
   "source": [
    "##### char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T01:46:06.624059Z",
     "start_time": "2020-01-05T01:45:44.273772Z"
    }
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'analyzer' : 'char',\n",
    "#     'max_features':500, \n",
    "#     'vocabulary': vocab,\n",
    "    'stop_words':stop_words, \n",
    "#     'max_df':1.0, \n",
    "#     'min_df':100, \n",
    "    'ngram_range':(1, 1), \n",
    "}\n",
    "df, d, v = tfidf(df_fea[df_fea['smishing']==1][tfidf_src_col].values, params, 'tfidf_pos_char_11')\n",
    "dfs.append(df)\n",
    "fea_dict.update(d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T01:46:21.450294Z",
     "start_time": "2020-01-05T01:46:06.624789Z"
    }
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'max_features':1000,#500, \n",
    "#     'vocabulary': vocab,\n",
    "    'stop_words':stop_words, \n",
    "    'max_df':1.0, \n",
    "    'min_df':100, \n",
    "    'ngram_range':(1, 1), \n",
    "}\n",
    "df, d, v = tfidf(df_fea[df_fea['smishing']==1][tfidf_src_col].values, params, 'tfidf_pos_word_11')\n",
    "dfs.append(df)\n",
    "fea_dict.update(d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### smishing 2, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T01:46:39.868211Z",
     "start_time": "2020-01-05T01:46:21.451188Z"
    }
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'max_features':1000,#,500, \n",
    "#     'vocabulary': vocab,\n",
    "    'stop_words':stop_words, \n",
    "    'max_df':1.0, \n",
    "    'min_df':100, \n",
    "    'ngram_range':(2, 2), \n",
    "}\n",
    "df, d, v = tfidf(df_fea[df_fea['smishing']==1][tfidf_src_col].values, params, 'tfidf_pos_word_22')\n",
    "dfs.append(df)\n",
    "fea_dict.update(d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T01:46:39.871154Z",
     "start_time": "2020-01-05T01:46:39.869703Z"
    }
   },
   "outputs": [],
   "source": [
    "# ts = '20191214T055747'\n",
    "# file_name = 'model/gensim_{}'.format(ts)\n",
    "\n",
    "# w2v_model = joblib.load(os.path.join(base_path, '{}.pkl'.format(file_name)))\n",
    "# w2v_size = w2v_model.wv.vectors.shape[1]\n",
    "\n",
    "# def mean_w2v(row):\n",
    "#     nouns = row['nouns']\n",
    "#     w2v = np.zeros(w2v_size)\n",
    "\n",
    "    \n",
    "#     for n in nouns:\n",
    "#         if n in w2v_model.wv.vocab.keys():\n",
    "#             w2v = np.add(w2v, w2v_model.wv[n])\n",
    "            \n",
    "#     return w2v if len(nouns) == 0 else np.true_divide(w2v, len(nouns))\n",
    "\n",
    "# w2v_cols = ['w2v_{}'.format(i) for i in range(w2v_size)]\n",
    "\n",
    "# df_fea[w2v_cols] = df_fea.apply(mean_w2v, axis=1, result_type='expand')\n",
    "\n",
    "# for c in w2v_cols:\n",
    "#     df_fea[c] = df_fea[c].astype(np.float16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T01:47:20.103890Z",
     "start_time": "2020-01-05T01:46:39.871873Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 185503,
     "status": "ok",
     "timestamp": 1576287090605,
     "user": {
      "displayName": "주이클",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCWVh4nvn9788qUddhoWTl5qQoYt0bzVMDlJWUumTg=s64",
      "userId": "02708070532256873610"
     },
     "user_tz": -540
    },
    "id": "4f9oWotspZxF",
    "outputId": "6e0f2830-c6ef-4e00-9b0b-19ea6751dad0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df_fea.shape)\n",
    "for df in dfs:\n",
    "    print(df.shape)\n",
    "\n",
    "df_merged = pd.concat([df_fea] + dfs, axis=1)\n",
    "print('df_merged', df_merged.shape)\n",
    "print(df_merged.info())\n",
    "# df_merged.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T01:47:20.163715Z",
     "start_time": "2020-01-05T01:47:20.117233Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 185838,
     "status": "ok",
     "timestamp": 1576287091046,
     "user": {
      "displayName": "주이클",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCWVh4nvn9788qUddhoWTl5qQoYt0bzVMDlJWUumTg=s64",
      "userId": "02708070532256873610"
     },
     "user_tz": -540
    },
    "id": "QOi9dYzqpZxV",
    "outputId": "9ceb7e20-059e-4946-934e-ab85c98dca56",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idx_cols = ['id', 'year_month', 'text', 'smishing', 'nouns', 'nouns_str', 'morphs', 'morphs_str']\n",
    "\n",
    "cat_cols = []\n",
    "fea_cols = [c for c in df_merged.columns if c not in idx_cols]\n",
    "\n",
    "for c in fea_cols:\n",
    "    if c not in fea_dict.keys():\n",
    "        fea_dict[c] = c\n",
    "\n",
    "len(fea_cols), len(fea_dict.keys()), vocab_dim, max_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T01:47:39.480808Z",
     "start_time": "2020-01-05T01:47:20.170942Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "dtoNNPnG75fT"
   },
   "outputs": [],
   "source": [
    "del df_fea, df_train, df_test\n",
    "for df in dfs:\n",
    "    del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T01:47:39.494794Z",
     "start_time": "2020-01-05T01:47:39.484144Z"
    }
   },
   "outputs": [],
   "source": [
    "# base_dict = (df_merged[fea_cols].max() - df_merged[fea_cols].min()).to_dict()\n",
    "\n",
    "# for c in fea_cols:\n",
    "#     df_merged[c] = df_merged[c] / base_dict[c]\n",
    "\n",
    "# print(df_merged[fea_cols].info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T01:48:06.269746Z",
     "start_time": "2020-01-05T01:47:39.498026Z"
    }
   },
   "outputs": [],
   "source": [
    "# merged_ts = datetime.now().strftime('%Y%m%dT%H%M%S') + '_' + str(len(fea_cols))\n",
    "merged_ts = '{}_{}_{}_{}'.format(datetime.now().strftime('%Y%m%dT%H%M%S'), \n",
    "                                 str(len(fea_cols)), \n",
    "                                 str(max_len), \n",
    "                                 str(vocab_dim))\n",
    "print(merged_ts)\n",
    "\n",
    "for c in df_merged:\n",
    "    if c not in fea_cols + ['smishing']:\n",
    "        df_merged.drop(c, axis=1, inplace=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T01:48:06.751913Z",
     "start_time": "2020-01-05T01:48:06.271130Z"
    }
   },
   "outputs": [],
   "source": [
    "df_merged.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T01:48:06.842562Z",
     "start_time": "2020-01-05T01:48:06.752967Z"
    }
   },
   "outputs": [],
   "source": [
    "joblib.dump(fea_dict, 'data/df_merged_{}_fea_dict.pkl'.format(merged_ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T01:48:29.145900Z",
     "start_time": "2020-01-05T01:48:06.843474Z"
    }
   },
   "outputs": [],
   "source": [
    "joblib.dump(df_merged.loc[df_merged['smishing'] != -1,:], 'data/df_merged_{}_train.pkl'.format(merged_ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T01:48:30.446526Z",
     "start_time": "2020-01-05T01:48:29.149173Z"
    }
   },
   "outputs": [],
   "source": [
    "joblib.dump(df_merged.loc[df_merged['smishing'] == 1,:], 'data/df_merged_{}_train_pos.pkl'.format(merged_ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T01:48:31.399195Z",
     "start_time": "2020-01-05T01:48:30.650925Z"
    }
   },
   "outputs": [],
   "source": [
    "joblib.dump(df_merged.loc[df_merged['smishing'] == -1,:], 'data/df_merged_{}_test.pkl'.format(merged_ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T01:48:31.413923Z",
     "start_time": "2020-01-05T01:48:31.407230Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "dtoNNPnG75fT"
   },
   "outputs": [],
   "source": [
    "# del df_merged, df_fea, df_train, df_test\n",
    "# for df in dfs:\n",
    "#     del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T01:48:33.807140Z",
     "start_time": "2020-01-05T01:48:31.414597Z"
    }
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T01:48:33.818764Z",
     "start_time": "2020-01-05T01:48:33.809130Z"
    }
   },
   "outputs": [],
   "source": [
    "exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "mecab_tfidf.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
