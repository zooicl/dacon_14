{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T08:25:16.412145Z",
     "start_time": "2019-12-31T08:25:14.844164Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1590,
     "status": "ok",
     "timestamp": 1576286906380,
     "user": {
      "displayName": "주이클",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCWVh4nvn9788qUddhoWTl5qQoYt0bzVMDlJWUumTg=s64",
      "userId": "02708070532256873610"
     },
     "user_tz": -540
    },
    "id": "U3xmRNtgpZwi",
    "outputId": "f6e95e03-5913-4a53-9ff3-1b6cde1b5a82"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiden/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.2\n",
      "1.3.1\n",
      "GeForce RTX 2070 SUPER\n",
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.externals import joblib \n",
    "import os\n",
    "from konlpy.tag import Mecab\n",
    "import lightgbm as lgb\n",
    "print(lgb.__version__)\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.externals import joblib \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import gc\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T08:25:16.415458Z",
     "start_time": "2019-12-31T08:25:16.413264Z"
    }
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torchtext\n",
    "# from torchtext.datasets import text_classification\n",
    "# NGRAMS = 2\n",
    "# import os\n",
    "# if not os.path.isdir('./.data'):\n",
    "#     os.mkdir('./.data')\n",
    "# train_dataset, test_dataset = text_classification.DATASETS['AG_NEWS'](\n",
    "#     root='./.data', ngrams=NGRAMS, vocab=None)\n",
    "# BATCH_SIZE = 16\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# train_dataset.\n",
    "\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class TextSentiment(nn.Module):\n",
    "#     def __init__(self, vocab_size, embed_dim, num_class):\n",
    "#         super().__init__()\n",
    "#         self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "#         self.fc = nn.Linear(embed_dim, num_class)\n",
    "#         self.init_weights()\n",
    "\n",
    "#     def init_weights(self):\n",
    "#         initrange = 0.5\n",
    "#         self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "#         self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "#         self.fc.bias.data.zero_()\n",
    "\n",
    "#     def forward(self, text, offsets):\n",
    "#         embedded = self.embedding(text, offsets)\n",
    "#         return self.fc(embedded)\n",
    "\n",
    "# VOCAB_SIZE = len(train_dataset.get_vocab())\n",
    "# EMBED_DIM = 32\n",
    "# NUN_CLASS = len(train_dataset.get_labels())\n",
    "# model = TextSentiment(VOCAB_SIZE, EMBED_DIM, NUN_CLASS).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T08:25:16.426585Z",
     "start_time": "2019-12-31T08:25:16.416416Z"
    }
   },
   "outputs": [],
   "source": [
    "class LRModel(torch.nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(LRModel,self).__init__()\n",
    "\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_size, 2), \n",
    "#             torch.nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):        \n",
    "        return self.model(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T08:25:16.435793Z",
     "start_time": "2019-12-31T08:25:16.427929Z"
    }
   },
   "outputs": [],
   "source": [
    "class NNModel(torch.nn.Module):\n",
    "    def __init__(self, input_size, dropout_probability=0.3):\n",
    "        super(NNModel,self).__init__()\n",
    "        relu = torch.nn.ReLU()\n",
    "        dropout = torch.nn.Dropout(p=dropout_probability)\n",
    "\n",
    "        self.model = torch.nn.Sequential(\n",
    "#             torch.nn.Linear(input_size, 4096), relu, torch.nn.BatchNorm1d(4096), dropout,\n",
    "#             torch.nn.Linear(4096, 2048), relu, torch.nn.BatchNorm1d(2048), dropout,\n",
    "            \n",
    "            torch.nn.Linear(input_size, 2048), relu, torch.nn.BatchNorm1d(2048), dropout,\n",
    "            torch.nn.Linear(2048, 1024), relu, torch.nn.BatchNorm1d(1024), dropout,\n",
    "            \n",
    "#             torch.nn.Linear(input_size, 1024), relu, torch.nn.BatchNorm1d(1024), dropout, \n",
    "\n",
    "            torch.nn.Linear(1024, 512), relu, torch.nn.BatchNorm1d(512), dropout,\n",
    "            torch.nn.Linear(512, 512), relu, torch.nn.BatchNorm1d(512), dropout,\n",
    "            torch.nn.Linear(512, 256), relu, torch.nn.BatchNorm1d(256), dropout,\n",
    "            torch.nn.Linear(256, 128), relu, torch.nn.BatchNorm1d(128), dropout,\n",
    "            torch.nn.Linear(128, 2), \n",
    "#             torch.nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):        \n",
    "        return self.model(x) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T08:25:16.452026Z",
     "start_time": "2019-12-31T08:25:16.436590Z"
    }
   },
   "outputs": [],
   "source": [
    "class KBDataset(Dataset):\n",
    "    def __init__(self, df, y_col):\n",
    "#         print(df.shape)\n",
    "#         print(df.info())\n",
    "        \n",
    "        self.columns = [c for c in df.columns if c not in [y_col]]\n",
    "        self.X = df[self.columns].values\n",
    "        self.y = pd.get_dummies(df[y_col].astype(int), prefix=y_col).values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        return self.columns\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx].astype(np.float32), self.y[idx].astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### eval_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T08:25:16.458356Z",
     "start_time": "2019-12-31T08:25:16.452977Z"
    }
   },
   "outputs": [],
   "source": [
    "def eval_summary(y_true, y_score, cut_off=0.5):\n",
    "    if len(y_true) == 0 or len(y_score) == 0:\n",
    "        return 'zero length'\n",
    "    if len(y_true) != len(y_score):\n",
    "        return 'diff length'\n",
    "    \n",
    "    y_pred = y_score.copy()\n",
    "    y_pred[y_pred > cut_off] = 1\n",
    "    y_pred[y_pred <= cut_off] = 0\n",
    "\n",
    "    eval_dict = {}\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_true, y_score, pos_label=1)\n",
    "    \n",
    "    eval_dict['auc'] = metrics.auc(fpr, tpr)\n",
    "    eval_dict['confusion_matrix'] = metrics.confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    pre, rec, _, _ = metrics.precision_recall_fscore_support(y_true, y_pred, pos_label=1)\n",
    "    eval_dict['precision'] = pre[1]\n",
    "    eval_dict['recall'] = rec[1]\n",
    "    \n",
    "    return eval_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train/test_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T08:25:16.471709Z",
     "start_time": "2019-12-31T08:25:16.459411Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "step = 4\n",
    "num_workers = 6\n",
    "\n",
    "def train_torch(dataset):\n",
    "    model.train()\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    data_loader = DataLoader(dataset=dataset,\n",
    "#                           batch_size=100000,\n",
    "#                         batch_size=int(train_size * 0.7),\n",
    "                          batch_size=len(dataset) // step,\n",
    "#                           batch_size=10000,\n",
    "                          shuffle=True,\n",
    "                          num_workers=num_workers,\n",
    "                         drop_last=True\n",
    "                         )\n",
    "    for i, data in enumerate(data_loader):\n",
    "#     for i, data in tqdm_notebook(enumerate(train_loader), total=len(train_loader), desc = 'epoch{}_batch'.format(e)):\n",
    "#         print(e, i)\n",
    "        X_batch, y_batch = data\n",
    "        \n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        \n",
    "        y_pred = model(X_batch)\n",
    "        \n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        \n",
    "        loss += loss.item()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        acc += (y_pred.argmax(1) == y_batch.argmax(1)).sum().item()\n",
    "        \n",
    "        del X_batch, y_batch, y_pred\n",
    "        gc.collect()\n",
    "\n",
    "    return loss / len(dataset), acc / len(dataset)\n",
    "\n",
    "\n",
    "def test_torch(dataset):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    \n",
    "    data_loader = DataLoader(dataset=dataset,\n",
    "                          batch_size=len(dataset) // step,\n",
    "                          shuffle=False,\n",
    "                          num_workers=num_workers,\n",
    "                          drop_last=True\n",
    "                         ) \n",
    "    \n",
    "    for i, data in enumerate(data_loader):\n",
    "        X_batch, y_batch = data\n",
    "        \n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            loss += loss.item()\n",
    "            acc += (y_pred.argmax(1) == y_batch.argmax(1)).sum().item()\n",
    "            \n",
    "            y_pred = torch.sigmoid(y_pred)\n",
    "            \n",
    "            del X_batch, y_batch, y_pred\n",
    "            \n",
    "    return loss / len(dataset), acc / len(dataset)\n",
    "\n",
    "\n",
    "def pred_torch(dataset):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    \n",
    "    y_true_list = []\n",
    "    y_score_list = []\n",
    "\n",
    "    data_loader = DataLoader(dataset=dataset,\n",
    "                          batch_size=len(dataset) // step,\n",
    "                          shuffle=False,\n",
    "                          num_workers=1,\n",
    "                          drop_last=True\n",
    "                         ) \n",
    "    \n",
    "    for i, data in enumerate(data_loader):\n",
    "        X_batch, y_batch = data\n",
    "        \n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        \n",
    "        y_true = y_batch\n",
    "        y_true_list.append(y_true[:, 1].cpu().detach().numpy())\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            loss += loss.item()\n",
    "            acc += (y_pred.argmax(1) == y_batch.argmax(1)).sum().item()\n",
    "            \n",
    "            y_pred = torch.sigmoid(y_pred)\n",
    "            y_score_list.append(y_pred[:, 1].cpu().detach().numpy())\n",
    "            \n",
    "            del X_batch, y_batch, y_true, y_pred\n",
    "            \n",
    "    return loss / len(dataset), acc / len(dataset), np.concatenate(y_true_list, axis=0), np.concatenate(y_score_list, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T08:25:20.476021Z",
     "start_time": "2019-12-31T08:25:16.472863Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_set\n",
      " 0    277242\n",
      "1     18703\n",
      "Name: smishing, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# merged_ts = '20191230T014439_8180'\n",
    "# merged_ts = '20191229T155539'\n",
    "# merged_ts = '20191231T113708_5499'\n",
    "\n",
    "merged_ts = '20191231T165424_6099'\n",
    "\n",
    "\n",
    "train_path = 'data/df_merged_{}_train.pkl'.format(merged_ts)\n",
    "test_path = 'data/df_merged_{}_test.pkl'.format(merged_ts)\n",
    "\n",
    "df_model = joblib.load(train_path)  \n",
    "df_model = df_model.reset_index()\n",
    "print('model_set\\n', df_model['smishing'].value_counts())\n",
    "df_test = joblib.load(test_path) \n",
    "\n",
    "idx_cols = ['smishing', 'id', 'index']\n",
    "fea_cols = [c for c in df_model.columns if c not in idx_cols]#[:500]\n",
    "input_size = len(fea_cols)\n",
    "\n",
    "x_test = torch.Tensor(df_test[fea_cols].values).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T08:25:21.495603Z",
     "start_time": "2019-12-31T08:25:20.477079Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fea__text_len</th>\n",
       "      <th>fea__noun_cnt</th>\n",
       "      <th>cnt_0000</th>\n",
       "      <th>cnt_0001</th>\n",
       "      <th>cnt_0002</th>\n",
       "      <th>cnt_0003</th>\n",
       "      <th>cnt_0004</th>\n",
       "      <th>cnt_0005</th>\n",
       "      <th>cnt_0006</th>\n",
       "      <th>cnt_0007</th>\n",
       "      <th>...</th>\n",
       "      <th>tfidf_pos_word_22_0490</th>\n",
       "      <th>tfidf_pos_word_22_0491</th>\n",
       "      <th>tfidf_pos_word_22_0492</th>\n",
       "      <th>tfidf_pos_word_22_0493</th>\n",
       "      <th>tfidf_pos_word_22_0494</th>\n",
       "      <th>tfidf_pos_word_22_0495</th>\n",
       "      <th>tfidf_pos_word_22_0496</th>\n",
       "      <th>tfidf_pos_word_22_0497</th>\n",
       "      <th>tfidf_pos_word_22_0498</th>\n",
       "      <th>tfidf_pos_word_22_0499</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>37.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>81.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>174.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>40.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>295940</td>\n",
       "      <td>168.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>295941</td>\n",
       "      <td>33.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>295942</td>\n",
       "      <td>97.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>295943</td>\n",
       "      <td>66.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>295944</td>\n",
       "      <td>642.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>295945 rows × 6099 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        fea__text_len  fea__noun_cnt  cnt_0000  cnt_0001  cnt_0002  cnt_0003  \\\n",
       "0                24.0            5.0       0.0       0.0       0.0       0.0   \n",
       "1                37.0            6.0       2.0       0.0       0.0       0.0   \n",
       "2                81.0           12.0      11.0       0.0       0.0       0.0   \n",
       "3               174.0           29.0      34.0       0.0       0.0       0.0   \n",
       "4                40.0            4.0       6.0       0.0       0.0       0.0   \n",
       "...               ...            ...       ...       ...       ...       ...   \n",
       "295940          168.0           18.0      32.0       1.0       0.0       0.0   \n",
       "295941           33.0            9.0       2.0       0.0       0.0       0.0   \n",
       "295942           97.0           15.0      18.0       0.0       0.0       0.0   \n",
       "295943           66.0           15.0       9.0       0.0       0.0       1.0   \n",
       "295944          642.0          130.0      99.0       1.0       0.0       5.0   \n",
       "\n",
       "        cnt_0004  cnt_0005  cnt_0006  cnt_0007  ...  tfidf_pos_word_22_0490  \\\n",
       "0            0.0       0.0       1.0       0.0  ...                     0.0   \n",
       "1            0.0       0.0       0.0       0.0  ...                     0.0   \n",
       "2            0.0       0.0       5.0       2.0  ...                     0.0   \n",
       "3            0.0       0.0       2.0       1.0  ...                     0.0   \n",
       "4            0.0       0.0       1.0       0.0  ...                     0.0   \n",
       "...          ...       ...       ...       ...  ...                     ...   \n",
       "295940       0.0       0.0       4.0       2.0  ...                     0.0   \n",
       "295941       0.0       0.0       1.0       0.0  ...                     0.0   \n",
       "295942       0.0       0.0       0.0       1.0  ...                     0.0   \n",
       "295943       1.0       0.0       2.0       0.0  ...                     0.0   \n",
       "295944       5.0      14.0      19.0      12.0  ...                     0.0   \n",
       "\n",
       "        tfidf_pos_word_22_0491  tfidf_pos_word_22_0492  \\\n",
       "0                          0.0                     0.0   \n",
       "1                          0.0                     0.0   \n",
       "2                          0.0                     0.0   \n",
       "3                          0.0                     0.0   \n",
       "4                          0.0                     0.0   \n",
       "...                        ...                     ...   \n",
       "295940                     0.0                     0.0   \n",
       "295941                     0.0                     0.0   \n",
       "295942                     0.0                     0.0   \n",
       "295943                     0.0                     0.0   \n",
       "295944                     0.0                     0.0   \n",
       "\n",
       "        tfidf_pos_word_22_0493  tfidf_pos_word_22_0494  \\\n",
       "0                          0.0                     0.0   \n",
       "1                          0.0                     0.0   \n",
       "2                          0.0                     0.0   \n",
       "3                          0.0                     0.0   \n",
       "4                          0.0                     0.0   \n",
       "...                        ...                     ...   \n",
       "295940                     0.0                     0.0   \n",
       "295941                     0.0                     0.0   \n",
       "295942                     0.0                     0.0   \n",
       "295943                     0.0                     0.0   \n",
       "295944                     0.0                     0.0   \n",
       "\n",
       "        tfidf_pos_word_22_0495  tfidf_pos_word_22_0496  \\\n",
       "0                          0.0                     0.0   \n",
       "1                          0.0                     0.0   \n",
       "2                          0.0                     0.0   \n",
       "3                          0.0                     0.0   \n",
       "4                          0.0                     0.0   \n",
       "...                        ...                     ...   \n",
       "295940                     0.0                     0.0   \n",
       "295941                     0.0                     0.0   \n",
       "295942                     0.0                     0.0   \n",
       "295943                     0.0                     0.0   \n",
       "295944                     0.0                     0.0   \n",
       "\n",
       "        tfidf_pos_word_22_0497  tfidf_pos_word_22_0498  tfidf_pos_word_22_0499  \n",
       "0                          0.0                     0.0                     0.0  \n",
       "1                          0.0                     0.0                     0.0  \n",
       "2                          0.0                     0.0                     0.0  \n",
       "3                          0.0                     0.0                     0.0  \n",
       "4                          0.0                     0.0                     0.0  \n",
       "...                        ...                     ...                     ...  \n",
       "295940                     0.0                     0.0                     0.0  \n",
       "295941                     0.0                     0.0                     0.0  \n",
       "295942                     0.0                     0.0                     0.0  \n",
       "295943                     0.0                     0.0                     0.0  \n",
       "295944                     0.0                     0.0                     0.0  \n",
       "\n",
       "[295945 rows x 6099 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model[fea_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T08:25:21.497741Z",
     "start_time": "2019-12-31T08:25:21.496429Z"
    }
   },
   "outputs": [],
   "source": [
    "# model = NNModel(input_size=input_size, dropout_probability=0.7).to(device)\n",
    "# epoch = 1\n",
    "# print(summary(model, (input_size, )))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T08:25:21.510798Z",
     "start_time": "2019-12-31T08:25:21.498459Z"
    }
   },
   "outputs": [],
   "source": [
    "# import EarlyStopping\n",
    "from torchtools import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-12-31T08:25:14.891Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236755 59190\n",
      "\n",
      "CV 0\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                 [-1, 2048]      12,492,800\n",
      "              ReLU-2                 [-1, 2048]               0\n",
      "       BatchNorm1d-3                 [-1, 2048]           4,096\n",
      "           Dropout-4                 [-1, 2048]               0\n",
      "            Linear-5                 [-1, 1024]       2,098,176\n",
      "              ReLU-6                 [-1, 1024]               0\n",
      "       BatchNorm1d-7                 [-1, 1024]           2,048\n",
      "           Dropout-8                 [-1, 1024]               0\n",
      "            Linear-9                  [-1, 512]         524,800\n",
      "             ReLU-10                  [-1, 512]               0\n",
      "      BatchNorm1d-11                  [-1, 512]           1,024\n",
      "          Dropout-12                  [-1, 512]               0\n",
      "           Linear-13                  [-1, 512]         262,656\n",
      "             ReLU-14                  [-1, 512]               0\n",
      "      BatchNorm1d-15                  [-1, 512]           1,024\n",
      "          Dropout-16                  [-1, 512]               0\n",
      "           Linear-17                  [-1, 256]         131,328\n",
      "             ReLU-18                  [-1, 256]               0\n",
      "      BatchNorm1d-19                  [-1, 256]             512\n",
      "          Dropout-20                  [-1, 256]               0\n",
      "           Linear-21                  [-1, 128]          32,896\n",
      "             ReLU-22                  [-1, 128]               0\n",
      "      BatchNorm1d-23                  [-1, 128]             256\n",
      "          Dropout-24                  [-1, 128]               0\n",
      "           Linear-25                    [-1, 2]             258\n",
      "================================================================\n",
      "Total params: 15,551,874\n",
      "Trainable params: 15,551,874\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 0.14\n",
      "Params size (MB): 59.33\n",
      "Estimated Total Size (MB): 59.49\n",
      "----------------------------------------------------------------\n",
      "None\n",
      "20191231T172522\n",
      "Epoch: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3387ed4989447e4a2b62788d994106a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='CV 0 Epoch', style=ProgressStyle(description_width='initial')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV 0 Epoch 1\n",
      "\tTrain loss: 0.7742999196052551\tValid loss: 0.6042466163635254\t1.2814302444458008\n",
      "Validation loss decreased (inf --> 0.604247).  Saving model ...\n",
      "\t {'auc': 0.9931941801899445, 'confusion_matrix': array([[47568,  7881],\n",
      "       [    6,  3733]]), 'precision': 0.3214224212157741, 'recall': 0.9983952928590533}\n",
      "CV 0 Epoch 2\n",
      "\tTrain loss: 0.6421478390693665\tValid loss: 0.5045629143714905\t1.2726813554763794\n",
      "Validation loss decreased (0.604247 --> 0.504563).  Saving model ...\n",
      "CV 0 Epoch 3\n",
      "\tTrain loss: 0.527597188949585\tValid loss: 0.32794803380966187\t1.6087828874588013\n",
      "Validation loss decreased (0.504563 --> 0.327948).  Saving model ...\n",
      "CV 0 Epoch 4\n",
      "\tTrain loss: 0.4310115575790405\tValid loss: 0.28785645961761475\t1.497314214706421\n",
      "Validation loss decreased (0.327948 --> 0.287856).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "[df_test.drop(c, axis=1, inplace=True) for c in df_test.columns if 'smishing_' in c]\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, random_state=8405)\n",
    "\n",
    "for cv, index in enumerate(skf.split(df_model[fea_cols], df_model['smishing'])):\n",
    "    train_index, valid_index = index\n",
    "    \n",
    "    print(len(train_index), len(valid_index))\n",
    "    print('\\nCV', cv)\n",
    "    model = NNModel(input_size=input_size, dropout_probability=0.7).to(device)\n",
    "#     model =  LRModel(input_size=input_size).to(device)\n",
    "\n",
    "    # initialize the early_stopping object\n",
    "    early_stopping = EarlyStopping(patience=15, verbose=True)\n",
    "\n",
    "    epoch = 1\n",
    "    if cv == 0:\n",
    "        print(summary(model, (input_size, )))\n",
    "    \n",
    "    pos_weight = torch.Tensor([1., 10.,])\n",
    "#     pos_weight = torch.Tensor([1., 1.,])\n",
    "    criterion = torch.nn.BCEWithLogitsLoss(reduction='sum', pos_weight=pos_weight).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "    model_ts = datetime.now().strftime('%Y%m%dT%H%M%S')\n",
    "    print(model_ts)\n",
    "    print('Epoch:', epoch)\n",
    "\n",
    "    N_EPOCHS = 100\n",
    "    is_summary = True\n",
    "    for e in tqdm_notebook(range(epoch, epoch + N_EPOCHS), total=N_EPOCHS, desc = 'CV {} Epoch'.format(cv)):\n",
    "        start_time = time.time()\n",
    "        train_set = KBDataset(df_model.loc[train_index, fea_cols + ['smishing']], 'smishing')\n",
    "        valid_set = KBDataset(df_model.loc[valid_index, fea_cols + ['smishing']], 'smishing')\n",
    "\n",
    "        train_loss, train_acc = train_torch(train_set)\n",
    "        valid_loss, valid_acc = test_torch(valid_set)\n",
    "        print('CV {} Epoch {}\\n\\tTrain loss: {}\\tValid loss: {}\\t{}'.format(cv, e, train_loss, valid_loss, train_loss / valid_loss))\n",
    "        \n",
    "        early_stopping(valid_loss, model)\n",
    "        \n",
    "        \n",
    "        if early_stopping.counter == 0:\n",
    "            if is_summary:\n",
    "                _, _, y_true, y_score = pred_torch(valid_set)            \n",
    "                print('\\t', eval_summary(y_true, y_score, cut_off=0.5))\n",
    "                is_summary = False\n",
    "        else:\n",
    "            is_summary = True\n",
    "            \n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"\\tEarly stopping epoch {}, valid loss {}\".format(e, valid_loss))\n",
    "            break\n",
    "    \n",
    "        del train_set, valid_set\n",
    "        gc.collect()\n",
    "        \n",
    "        epoch = e + 1\n",
    "    \n",
    "    # load the last checkpoint with the best model\n",
    "    model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "    \n",
    "    valid_set = KBDataset(df_model.loc[valid_index, fea_cols + ['smishing']], 'smishing')\n",
    "    _, _, y_true, y_score = pred_torch(valid_set)            \n",
    "    print('\\t', eval_summary(y_true, y_score, cut_off=0.5))\n",
    "\n",
    "    train_set = KBDataset(df_model.loc[train_index, fea_cols + ['smishing']], 'smishing')\n",
    "    _, _, y_true, y_score = pred_torch(train_set)            \n",
    "    print('END CV {} eval summary (train)\\n'.format(cv), eval_summary(y_true, y_score, cut_off=0.5))\n",
    "\n",
    "    torch.save(model.state_dict(), 'model/{}_{}_{}.model'.format(model_ts, cv, epoch-1))\n",
    "    \n",
    "    model.eval()\n",
    "    pred_col = 'smishing_{}'.format(cv)\n",
    "    df_test[pred_col] = torch.sigmoid(model(x_test))[:, 1].cpu().detach().numpy()\n",
    "    df_test[[pred_col]].to_csv('submit/{}_{}_nn.csv'.format(model_ts, pred_col), index=True)\n",
    "    \n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-12-31T08:25:14.894Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.Series(y_score)\n",
    "df.hist(bins=100, figsize=(20, 5))\n",
    "(df * 10).astype(int).value_counts(sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-12-31T08:25:14.896Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_model[(y_score <= 0.5) & (y_true == 1)]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-12-31T08:25:14.897Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_model[(y_score > 0.5) & (y_true == 0)]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-12-31T08:25:14.899Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_cols = [c for c in df_test.columns if 'smishing_' in c]\n",
    "print(len(pred_cols))\n",
    "df_test['pred_max'] = df_test[pred_cols].max(axis=1)\n",
    "df_test['pred_min'] = df_test[pred_cols].min(axis=1)\n",
    "df_test['pred_mean'] = df_test[pred_cols].mean(axis=1)\n",
    "df_test['pred_std'] = df_test[pred_cols].std(axis=1)\n",
    "\n",
    "print(df_test['pred_std'].max(), df_test['pred_std'].min(), df_test['pred_std'].mean())\n",
    "\n",
    "df_test['smishing'] = df_test['pred_mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-12-31T08:25:14.901Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test['smishing'].hist(bins=100, figsize=(20, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-12-31T08:25:14.903Z"
    }
   },
   "outputs": [],
   "source": [
    "for c in pred_cols:\n",
    "    print(c)\n",
    "    display((df_test[c] * 10).astype(int).value_counts(sort=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-12-31T08:25:14.904Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 0     1504\n",
    "# 1       11\n",
    "# 2        6\n",
    "# 3        6\n",
    "# 4        2\n",
    "# 5        3\n",
    "# 6        2\n",
    "# 9       39\n",
    "# 10      53\n",
    "(df_test['smishing'] * 10).astype(int).value_counts(sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-12-31T08:25:14.905Z"
    }
   },
   "outputs": [],
   "source": [
    "model_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-12-31T08:25:14.907Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test[['smishing']].to_csv('submit/{}_nn.csv'.format(model_ts), index=True)\n",
    "# df_test[['id', 'smishing', 'text']].sort_values('smishing', ascending=False).to_csv('{}_text.csv'.format(model_ts), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dtoNNPnG75fT"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "mecab_tfidf.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
