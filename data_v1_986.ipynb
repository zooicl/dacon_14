{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-01T16:43:29.623148Z",
     "start_time": "2020-01-01T16:43:28.668504Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1590,
     "status": "ok",
     "timestamp": 1576286906380,
     "user": {
      "displayName": "주이클",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCWVh4nvn9788qUddhoWTl5qQoYt0bzVMDlJWUumTg=s64",
      "userId": "02708070532256873610"
     },
     "user_tz": -540
    },
    "id": "U3xmRNtgpZwi",
    "outputId": "f6e95e03-5913-4a53-9ff3-1b6cde1b5a82"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.externals import joblib \n",
    "import os\n",
    "from konlpy.tag import Mecab\n",
    "import lightgbm as lgb\n",
    "print(lgb.__version__)\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.externals import joblib \n",
    "\n",
    "import gc\n",
    "from tqdm import tqdm_notebook\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-01T16:43:30.828458Z",
     "start_time": "2020-01-01T16:43:29.624161Z"
    }
   },
   "outputs": [],
   "source": [
    "print(os.getcwd())\n",
    "\n",
    "base_path = '.'\n",
    "\n",
    "df_train = pd.read_csv(os.path.join(base_path , 'input/train.csv'), index_col=0)\n",
    "df_test = pd.read_csv(os.path.join(base_path , 'input/public_test.csv'), index_col=0)\n",
    "df_test['smishing'] = -1\n",
    "\n",
    "df_fea = pd.concat([df_train, df_test])\n",
    "df_fea.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mecab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-01T16:47:58.205429Z",
     "start_time": "2020-01-01T16:43:30.829836Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "uquxguHUpZwt"
   },
   "outputs": [],
   "source": [
    "# mecab = Mecab()\n",
    "# df_fea['morphs'] = df_fea['text'].apply(lambda x: mecab.morphs(x))\n",
    "# df_fea['morphs_str'] = df_fea['morphs'].apply(lambda x: ' '.join(x))\n",
    "# df_fea['nouns'] = df_fea['text'].apply(lambda x: mecab.nouns(x))\n",
    "# df_fea['nouns_str'] = df_fea['nouns'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# def pos(row):\n",
    "#     x = row['text']\n",
    "#     pos_dict = {c:0 for c in pos_cols}\n",
    "    \n",
    "#     for _, p in mecab.pos(x):\n",
    "#         for v in p.split('+'):\n",
    "#             pos_dict[v] += 1\n",
    "        \n",
    "#     return [pos_dict[k] for k in sorted(pos_dict.keys())]\n",
    "\n",
    "# pos_cols = [\n",
    "#     'EC',\n",
    "#     'EF',\n",
    "#     'EP',\n",
    "#     'ETM',\n",
    "#     'ETN',\n",
    "#     'IC',\n",
    "#     'JC',\n",
    "#     'JKB',\n",
    "#     'JKC',\n",
    "#     'JKG',\n",
    "#     'JKO',\n",
    "#     'JKQ',\n",
    "#     'JKS',\n",
    "#     'JKV',\n",
    "#     'JX',\n",
    "#     'MAG',\n",
    "#     'MAJ',\n",
    "#     'MM',\n",
    "#     'NA',\n",
    "#     'NNB',\n",
    "#     'NNBC',\n",
    "#     'NNG',\n",
    "#     'NNP',\n",
    "#     'NP',\n",
    "#     'NR',\n",
    "#     'SC',\n",
    "#     'SF',\n",
    "#     'SL',\n",
    "#     'SN',\n",
    "#     'SSC',\n",
    "#     'SSO',\n",
    "#     'SY',\n",
    "#     'UNA',\n",
    "#     'UNKNOWN',\n",
    "#     'VA',\n",
    "#     'VCN',\n",
    "#     'VCP',\n",
    "#     'VV',\n",
    "#     'VX',\n",
    "#     'XPN',\n",
    "#     'XR',\n",
    "#     'XSA',\n",
    "#     'XSN',\n",
    "#     'XSV',\n",
    "# ]\n",
    "# df_fea[pos_cols] = df_fea.apply(pos, axis=1, result_type='expand')\n",
    "# df_fea[pos_cols] = df_fea[pos_cols].astype(np.int16)\n",
    "# df_fea.to_pickle('data/df_fea_1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-01T16:47:58.240446Z",
     "start_time": "2020-01-01T16:47:58.207172Z"
    }
   },
   "outputs": [],
   "source": [
    "df_fea = pd.read_pickle('data/df_fea_1.pkl')\n",
    "df_fea.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-01T16:47:58.243433Z",
     "start_time": "2020-01-01T16:47:58.241182Z"
    }
   },
   "outputs": [],
   "source": [
    "df_fea.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-01T16:47:58.251816Z",
     "start_time": "2020-01-01T16:47:58.244709Z"
    }
   },
   "outputs": [],
   "source": [
    "dfs = []\n",
    "fea_dict = {}\n",
    "\n",
    "# vocab = None\n",
    "# stop_words = joblib.load('model/vocab_20191231T111937.pkl')\n",
    "\n",
    "vocab = joblib.load('model/vocab_20191231T111937.pkl')\n",
    "stop_words = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq Fea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-01T16:47:58.258702Z",
     "start_time": "2020-01-01T16:47:58.252904Z"
    }
   },
   "outputs": [],
   "source": [
    "def _toidx(x):\n",
    "    return [int(vocab_dict[x[i]]) if i < len(x) else 0 for i in range(max_len)]\n",
    "\n",
    "def toidx(src_col, max_len):\n",
    "    idx_col_nm = '{}_{}_idx'.format(src_col, max_len)\n",
    "\n",
    "    vocab_set = set()\n",
    "    _ = df_fea[src_col].apply(lambda x: [vocab_set.add(c) for c in x])\n",
    "\n",
    "    vocab_dict = {v: i+1 for i, v in enumerate(vocab_set)}\n",
    "    vocab_dim = len(vocab_dict.keys()) + 1\n",
    "\n",
    "    def toidx(x):\n",
    "        return [int(vocab_dict[x[i]]) if i < len(x) else 0 for i in range(max_len)]\n",
    "\n",
    "#     print(vocab_dim, max_len)\n",
    "    \n",
    "    return idx_col_nm, df_fea[src_col].apply(toidx), vocab_dim, max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-01T16:48:08.000276Z",
     "start_time": "2020-01-01T16:47:58.260476Z"
    }
   },
   "outputs": [],
   "source": [
    "src_col = 'morphs'\n",
    "max_len = 128\n",
    "\n",
    "c, d, vocab_dim, max_len = toidx(src_col, max_len)\n",
    "df_fea[c] = d\n",
    "c, vocab_dim, max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-01T16:48:08.002866Z",
     "start_time": "2020-01-01T16:48:08.001335Z"
    }
   },
   "outputs": [],
   "source": [
    "# src_col = 'nouns'\n",
    "# max_len = 100\n",
    "\n",
    "# c, d, vocab_dim, max_len = toidx(src_col, max_len)\n",
    "# df_fea[c] = d\n",
    "# c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Fea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-01T16:48:08.371100Z",
     "start_time": "2020-01-01T16:48:08.003548Z"
    }
   },
   "outputs": [],
   "source": [
    "df_fea['fea__text_len'] = df_fea['text'].str.len().fillna(0).astype(np.float16)\n",
    "df_fea['fea__morphs_cnt'] = df_fea['morphs'].apply(lambda x: len(x)).fillna(0).astype(np.float16)\n",
    "df_fea['fea__noun_cnt'] = df_fea['nouns'].apply(lambda x: len(x)).fillna(0).astype(np.float16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-01T16:48:43.902162Z",
     "start_time": "2020-01-01T16:48:08.371894Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer='char',\n",
    "#                              vocabulary=vocab,\n",
    "                             stop_words=stop_words, \n",
    "                             max_df=1.0,\n",
    "                             min_df=100)\n",
    "\n",
    "# vectorizer = vectorizer.fit(df_fea[df_fea['smishing']==1]['nouns_str'].values)\n",
    "vectorizer = vectorizer.fit(df_fea['text'].values)\n",
    "cnt_vec = vectorizer.transform(df_fea['text'].values).toarray()\n",
    "\n",
    "cnt_dict = {'cnt_{0:04d}'.format(i):'cnt_{0:04}_{1}'.format(i, c) for i, c in enumerate(vectorizer.get_feature_names())}\n",
    "fea_dict.update(cnt_dict)\n",
    "cnt_cols = sorted(cnt_dict.keys())\n",
    "\n",
    "df_cnt_vec = pd.DataFrame(data=cnt_vec, index=df_fea.index, columns=cnt_cols, dtype=np.float16)\n",
    "df_cnt_vec = df_cnt_vec.loc[:, (df_cnt_vec != 0).any(axis=0)]\n",
    "dfs.append(df_cnt_vec)\n",
    "df_cnt_vec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-01T16:48:43.907242Z",
     "start_time": "2020-01-01T16:48:43.902981Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "l2Dgkdt6pZw_"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def tfidf(data, params, tfidf_tag):\n",
    "    vectorizer = TfidfVectorizer(**params)\n",
    "    vectorizer = vectorizer.fit(data)\n",
    "\n",
    "    d = {'{0}_{1:04d}'.format(tfidf_tag, v):'{0}_{1:04d}_{2}'.format(tfidf_tag, v, k) for k, v in sorted(vectorizer.vocabulary_.items(), key=lambda item: item[1])}\n",
    "    c = sorted(d.keys())\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "#         data=vectorizer.transform(df_fea['nouns_str'].values).toarray(),\n",
    "        data=vectorizer.transform(df_fea['morphs_str'].values).toarray(),\n",
    "        columns=c, \n",
    "        index=df_fea.index,\n",
    "        dtype=np.float16)\n",
    "    \n",
    "    # Remove all zeros column\n",
    "    df = df.loc[:, (df != 0).any(axis=0)]\n",
    "    d = {k:v for k, v in d.items() if k in df.columns}\n",
    "    \n",
    "    print(tfidf_tag, df.shape)\n",
    "    \n",
    "    return df, d, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-01T16:48:43.913218Z",
     "start_time": "2020-01-01T16:48:43.908051Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_src_col = 'morphs_str'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1, 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-01T16:49:23.455611Z",
     "start_time": "2020-01-01T16:48:43.914229Z"
    }
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'analyzer' : 'char',\n",
    "    'max_features':None, \n",
    "#     'vocabulary': vocab,\n",
    "    'stop_words':stop_words, \n",
    "#     'max_df':0.1, \n",
    "    'min_df':100, \n",
    "    'ngram_range':(1, 1), \n",
    "}\n",
    "df, d, v = tfidf(df_fea[tfidf_src_col].values, params, 'tfidf_char_11')\n",
    "dfs.append(df)\n",
    "fea_dict.update(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-01T16:49:48.983046Z",
     "start_time": "2020-01-01T16:49:23.456389Z"
    }
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'max_features':2000, \n",
    "#     'vocabulary': vocab,\n",
    "    'stop_words':stop_words, \n",
    "    'max_df':1.0, \n",
    "    'min_df':200, \n",
    "    'ngram_range':(1, 1), \n",
    "}\n",
    "df, d, v = tfidf(df_fea[tfidf_src_col].values, params, 'tfidf_word_11')\n",
    "dfs.append(df)\n",
    "fea_dict.update(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-01T16:50:22.881751Z",
     "start_time": "2020-01-01T16:49:48.983758Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "l2Dgkdt6pZw_",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'max_features':2000, \n",
    "#     'vocabulary': vocab,\n",
    "    'stop_words':stop_words, \n",
    "    'max_df':1.0, \n",
    "    'min_df':500, \n",
    "    'ngram_range':(2, 2), \n",
    "}\n",
    "df, d, v = tfidf(df_fea[tfidf_src_col].values, params, 'tfidf_word_22')\n",
    "dfs.append(df)\n",
    "fea_dict.update(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-01T16:50:53.064268Z",
     "start_time": "2020-01-01T16:50:22.882809Z"
    }
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'max_features':500, \n",
    "#     'vocabulary': vocab,\n",
    "    'stop_words':stop_words, \n",
    "    'max_df':1.0, \n",
    "    'min_df':500, \n",
    "    'ngram_range':(3, 3), \n",
    "}\n",
    "df, d, v = tfidf(df_fea[tfidf_src_col].values, params, 'tfidf_word_33')\n",
    "dfs.append(df)\n",
    "fea_dict.update(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### smishing 1, 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-01T09:36:52.987190Z",
     "start_time": "2020-01-01T09:36:52.984965Z"
    }
   },
   "source": [
    "##### char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-01T16:51:15.344911Z",
     "start_time": "2020-01-01T16:50:53.065028Z"
    }
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'analyzer' : 'char',\n",
    "#     'max_features':500, \n",
    "#     'vocabulary': vocab,\n",
    "    'stop_words':stop_words, \n",
    "#     'max_df':1.0, \n",
    "#     'min_df':100, \n",
    "    'ngram_range':(1, 1), \n",
    "}\n",
    "df, d, v = tfidf(df_fea[df_fea['smishing']==1][tfidf_src_col].values, params, 'tfidf_pos_char_11')\n",
    "dfs.append(df)\n",
    "fea_dict.update(d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-01T16:51:28.070988Z",
     "start_time": "2020-01-01T16:51:15.345857Z"
    }
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'max_features':500, \n",
    "#     'vocabulary': vocab,\n",
    "    'stop_words':stop_words, \n",
    "    'max_df':1.0, \n",
    "    'min_df':100, \n",
    "    'ngram_range':(1, 1), \n",
    "}\n",
    "df, d, v = tfidf(df_fea[df_fea['smishing']==1][tfidf_src_col].values, params, 'tfidf_pos_word_11')\n",
    "dfs.append(df)\n",
    "fea_dict.update(d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### smishing 2, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-01T16:51:44.207134Z",
     "start_time": "2020-01-01T16:51:28.072454Z"
    }
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'max_features':500, \n",
    "#     'vocabulary': vocab,\n",
    "    'stop_words':stop_words, \n",
    "    'max_df':1.0, \n",
    "    'min_df':100, \n",
    "    'ngram_range':(2, 2), \n",
    "}\n",
    "df, d, v = tfidf(df_fea[df_fea['smishing']==1][tfidf_src_col].values, params, 'tfidf_pos_word_22')\n",
    "dfs.append(df)\n",
    "fea_dict.update(d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-01T16:51:44.210456Z",
     "start_time": "2020-01-01T16:51:44.208314Z"
    }
   },
   "outputs": [],
   "source": [
    "# ts = '20191214T055747'\n",
    "# file_name = 'model/gensim_{}'.format(ts)\n",
    "\n",
    "# w2v_model = joblib.load(os.path.join(base_path, '{}.pkl'.format(file_name)))\n",
    "# w2v_size = w2v_model.wv.vectors.shape[1]\n",
    "\n",
    "# def mean_w2v(row):\n",
    "#     nouns = row['nouns']\n",
    "#     w2v = np.zeros(w2v_size)\n",
    "\n",
    "    \n",
    "#     for n in nouns:\n",
    "#         if n in w2v_model.wv.vocab.keys():\n",
    "#             w2v = np.add(w2v, w2v_model.wv[n])\n",
    "            \n",
    "#     return w2v if len(nouns) == 0 else np.true_divide(w2v, len(nouns))\n",
    "\n",
    "# w2v_cols = ['w2v_{}'.format(i) for i in range(w2v_size)]\n",
    "\n",
    "# df_fea[w2v_cols] = df_fea.apply(mean_w2v, axis=1, result_type='expand')\n",
    "\n",
    "# for c in w2v_cols:\n",
    "#     df_fea[c] = df_fea[c].astype(np.float16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-01T16:51:54.921913Z",
     "start_time": "2020-01-01T16:51:44.211583Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 185503,
     "status": "ok",
     "timestamp": 1576287090605,
     "user": {
      "displayName": "주이클",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCWVh4nvn9788qUddhoWTl5qQoYt0bzVMDlJWUumTg=s64",
      "userId": "02708070532256873610"
     },
     "user_tz": -540
    },
    "id": "4f9oWotspZxF",
    "outputId": "6e0f2830-c6ef-4e00-9b0b-19ea6751dad0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df_fea.shape)\n",
    "for df in dfs:\n",
    "    print(df.shape)\n",
    "\n",
    "df_merged = pd.concat([df_fea] + dfs, axis=1)\n",
    "print('df_merged', df_merged.shape)\n",
    "print(df_merged.info())\n",
    "# df_merged.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-01T16:51:54.929415Z",
     "start_time": "2020-01-01T16:51:54.923580Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 185838,
     "status": "ok",
     "timestamp": 1576287091046,
     "user": {
      "displayName": "주이클",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCWVh4nvn9788qUddhoWTl5qQoYt0bzVMDlJWUumTg=s64",
      "userId": "02708070532256873610"
     },
     "user_tz": -540
    },
    "id": "QOi9dYzqpZxV",
    "outputId": "9ceb7e20-059e-4946-934e-ab85c98dca56",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idx_cols = ['id', 'year_month', 'text', 'smishing', 'nouns', 'nouns_str', 'morphs', 'morphs_str']\n",
    "\n",
    "cat_cols = []\n",
    "fea_cols = [c for c in df_merged.columns if c not in idx_cols]\n",
    "\n",
    "for c in fea_cols:\n",
    "    if c not in fea_dict.keys():\n",
    "        fea_dict[c] = c\n",
    "\n",
    "len(fea_cols), len(fea_dict.keys()), vocab_dim, max_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-01T16:51:55.945010Z",
     "start_time": "2020-01-01T16:51:54.930293Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "dtoNNPnG75fT"
   },
   "outputs": [],
   "source": [
    "del df_fea, df_train, df_test\n",
    "for df in dfs:\n",
    "    del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-01T16:51:55.957650Z",
     "start_time": "2020-01-01T16:51:55.946342Z"
    }
   },
   "outputs": [],
   "source": [
    "# base_dict = (df_merged[fea_cols].max() - df_merged[fea_cols].min()).to_dict()\n",
    "\n",
    "# for c in fea_cols:\n",
    "#     df_merged[c] = df_merged[c] / base_dict[c]\n",
    "\n",
    "# print(df_merged[fea_cols].info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-01T16:52:05.012455Z",
     "start_time": "2020-01-01T16:51:55.958721Z"
    }
   },
   "outputs": [],
   "source": [
    "# merged_ts = datetime.now().strftime('%Y%m%dT%H%M%S') + '_' + str(len(fea_cols))\n",
    "merged_ts = '{}_{}_{}_{}'.format(datetime.now().strftime('%Y%m%dT%H%M%S'), \n",
    "                                 str(len(fea_cols)), \n",
    "                                 str(max_len), \n",
    "                                 str(vocab_dim))\n",
    "print(merged_ts)\n",
    "for c in df_merged:\n",
    "    if c not in fea_cols + ['smishing']:\n",
    "        df_merged.drop(c, axis=1, inplace=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-01T16:52:05.295457Z",
     "start_time": "2020-01-01T16:52:05.013219Z"
    }
   },
   "outputs": [],
   "source": [
    "df_merged.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-01T16:52:05.343960Z",
     "start_time": "2020-01-01T16:52:05.296194Z"
    }
   },
   "outputs": [],
   "source": [
    "joblib.dump(fea_dict, 'data/df_merged_{}_fea_dict.pkl'.format(merged_ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-01T16:52:13.956073Z",
     "start_time": "2020-01-01T16:52:05.344880Z"
    }
   },
   "outputs": [],
   "source": [
    "joblib.dump(df_merged.loc[df_merged['smishing'] != -1,:], 'data/df_merged_{}_train.pkl'.format(merged_ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-01T16:52:14.678760Z",
     "start_time": "2020-01-01T16:52:13.956707Z"
    }
   },
   "outputs": [],
   "source": [
    "joblib.dump(df_merged.loc[df_merged['smishing'] == 1,:], 'data/df_merged_{}_train_pos.pkl'.format(merged_ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-01T16:52:14.737738Z",
     "start_time": "2020-01-01T16:52:14.679495Z"
    }
   },
   "outputs": [],
   "source": [
    "joblib.dump(df_merged.loc[df_merged['smishing'] == -1,:], 'data/df_merged_{}_test.pkl'.format(merged_ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-01T16:52:14.739613Z",
     "start_time": "2020-01-01T16:52:14.738399Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "dtoNNPnG75fT"
   },
   "outputs": [],
   "source": [
    "# del df_merged, df_fea, df_train, df_test\n",
    "# for df in dfs:\n",
    "#     del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-01T16:52:15.704015Z",
     "start_time": "2020-01-01T16:52:14.740238Z"
    }
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-01T16:52:15.707752Z",
     "start_time": "2020-01-01T16:52:15.705555Z"
    }
   },
   "outputs": [],
   "source": [
    "exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "mecab_tfidf.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
